{
  "_autogen_note": "This file is automatically generated by the RAID leaderboard submission script. Do not edit this file manually or include it in new submissions' PRs.",
  "_submission_hash": "9fc4bb82392cbb4d0603b8b35ae38df51919b504e0ed070c0380ce86114f4b21",
  "_results_hash": "87f81687591c6136e6ba8d565c0ccbf89719aecf6a7a956667d484a1b8a80c07",
  "date_released": "2025-11-10",
  "detector_name": "debarta-text-classifier-v1",
  "contact_info": "kishan.sutex.kp@gmail.com",
  "website": null,
  "paper_link": null,
  "huggingface_link": "https://huggingface.co/kishankachhadiya/debarta-text-classifier-v1",
  "github_link": null,
  "additional_metadata": null,
  "score_agg": {
    "all": {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 619760,
          "fn": 33040,
          "accuracy": 0.9493872549019607
        },
        "0.01": null
      },
      "auroc": 0.9835996787875306
    },
    "no_adversarial": {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 53300,
          "fn": 1100,
          "accuracy": 0.9797794117647058
        },
        "0.01": null
      },
      "auroc": 0.9884165431602328
    }
  },
  "scores": [
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991614583333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991640625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991640625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991653645833334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.99899375
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9988333333333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9989135416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.933028125
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.99799375
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9655109374999999
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        }
      },
      "auroc": 0.9660109375000001
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9984135416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 764,
          "fn": 36,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 733,
          "fn": 67,
          "accuracy": 0.91625
        }
      },
      "auroc": 0.9822122395833333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990843749999999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991255208333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990635416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991151041666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990739583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991203125
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991614583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991640625
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9907479166666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.998090625
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9944192708333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9949546875
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9986286458333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 793,
          "fn": 7,
          "accuracy": 0.99125
        },
        "0.01": {
          "tp": 777,
          "fn": 23,
          "accuracy": 0.97125
        }
      },
      "auroc": 0.9967916666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.95799375
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9902416666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        }
      },
      "auroc": 0.9741177083333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9785802083333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9947041666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 770,
          "fn": 30,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 755,
          "fn": 45,
          "accuracy": 0.94375
        }
      },
      "auroc": 0.9866421875
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991458333333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9907604166666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.994953125
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991562500000001
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9949635416666668
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 795,
          "fn": 5,
          "accuracy": 0.99375
        },
        "0.01": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9970598958333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9981916666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9981916666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9965791666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9965791666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9973854166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9973854166666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9327156249999999
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9327156249999999
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9066885416666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9066885416666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 267,
          "fn": 133,
          "accuracy": 0.6675
        }
      },
      "auroc": 0.9197020833333334
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 267,
          "fn": 133,
          "accuracy": 0.6675
        }
      },
      "auroc": 0.9197020833333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991520833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991520833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999159375
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999159375
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.992421875
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.992421875
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9800552083333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9800552083333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9862385416666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9862385416666667
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2155,
          "fn": 45,
          "accuracy": 0.9795454545454545
        },
        "0.01": {
          "tp": 2129,
          "fn": 71,
          "accuracy": 0.9677272727272728
        }
      },
      "auroc": 0.9924076704545455
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        }
      },
      "auroc": 0.9990973958333333
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3355,
          "fn": 45,
          "accuracy": 0.986764705882353
        },
        "0.01": {
          "tp": 3327,
          "fn": 73,
          "accuracy": 0.9785294117647059
        }
      },
      "auroc": 0.99476875
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2072,
          "fn": 128,
          "accuracy": 0.9418181818181818
        },
        "0.01": {
          "tp": 1989,
          "fn": 211,
          "accuracy": 0.9040909090909091
        }
      },
      "auroc": 0.978262784090909
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1186,
          "fn": 14,
          "accuracy": 0.9883333333333333
        },
        "0.01": {
          "tp": 1168,
          "fn": 32,
          "accuracy": 0.9733333333333334
        }
      },
      "auroc": 0.9958852430555556
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3258,
          "fn": 142,
          "accuracy": 0.9582352941176471
        },
        "0.01": {
          "tp": 3157,
          "fn": 243,
          "accuracy": 0.9285294117647059
        }
      },
      "auroc": 0.9844824754901961
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 4227,
          "fn": 173,
          "accuracy": 0.9606818181818182
        },
        "0.01": {
          "tp": 4118,
          "fn": 282,
          "accuracy": 0.9359090909090909
        }
      },
      "auroc": 0.9853352272727274
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2386,
          "fn": 14,
          "accuracy": 0.9941666666666666
        },
        "0.01": {
          "tp": 2366,
          "fn": 34,
          "accuracy": 0.9858333333333333
        }
      },
      "auroc": 0.9974913194444444
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 6613,
          "fn": 187,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 6484,
          "fn": 316,
          "accuracy": 0.9535294117647058
        }
      },
      "auroc": 0.989625612745098
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991614583333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991640625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991640625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991653645833334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.99899375
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9988333333333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9989135416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.933028125
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.99799375
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9655109374999999
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        }
      },
      "auroc": 0.9660109375000001
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9984135416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 764,
          "fn": 36,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 733,
          "fn": 67,
          "accuracy": 0.91625
        }
      },
      "auroc": 0.9822122395833333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990843749999999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991255208333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990635416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991151041666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990739583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991203125
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991614583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991640625
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9907552083333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.998090625
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9944229166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9949583333333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9986286458333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 793,
          "fn": 7,
          "accuracy": 0.99125
        },
        "0.01": {
          "tp": 777,
          "fn": 23,
          "accuracy": 0.97125
        }
      },
      "auroc": 0.9967934895833334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9579875
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9902416666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        }
      },
      "auroc": 0.9741145833333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9785770833333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9947041666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 770,
          "fn": 30,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 755,
          "fn": 45,
          "accuracy": 0.94375
        }
      },
      "auroc": 0.9866406249999999
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991458333333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9907604166666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.994953125
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991562500000001
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9949635416666668
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 795,
          "fn": 5,
          "accuracy": 0.99375
        },
        "0.01": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9970598958333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9981916666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9981916666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.99658125
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.99658125
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9973864583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9973864583333334
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9327156249999999
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9327156249999999
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9066885416666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9066885416666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 267,
          "fn": 133,
          "accuracy": 0.6675
        }
      },
      "auroc": 0.9197020833333334
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 267,
          "fn": 133,
          "accuracy": 0.6675
        }
      },
      "auroc": 0.9197020833333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991520833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991520833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999159375
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999159375
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.992421875
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.992421875
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9800552083333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9800552083333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9862385416666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9862385416666667
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2155,
          "fn": 45,
          "accuracy": 0.9795454545454545
        },
        "0.01": {
          "tp": 2129,
          "fn": 71,
          "accuracy": 0.9677272727272728
        }
      },
      "auroc": 0.9924076704545455
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        }
      },
      "auroc": 0.9990973958333333
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3355,
          "fn": 45,
          "accuracy": 0.986764705882353
        },
        "0.01": {
          "tp": 3327,
          "fn": 73,
          "accuracy": 0.9785294117647059
        }
      },
      "auroc": 0.99476875
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2072,
          "fn": 128,
          "accuracy": 0.9418181818181818
        },
        "0.01": {
          "tp": 1989,
          "fn": 211,
          "accuracy": 0.9040909090909091
        }
      },
      "auroc": 0.9782630681818182
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1186,
          "fn": 14,
          "accuracy": 0.9883333333333333
        },
        "0.01": {
          "tp": 1168,
          "fn": 32,
          "accuracy": 0.9733333333333334
        }
      },
      "auroc": 0.9958852430555556
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3258,
          "fn": 142,
          "accuracy": 0.9582352941176471
        },
        "0.01": {
          "tp": 3157,
          "fn": 243,
          "accuracy": 0.9285294117647059
        }
      },
      "auroc": 0.9844826593137256
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 4227,
          "fn": 173,
          "accuracy": 0.9606818181818182
        },
        "0.01": {
          "tp": 4118,
          "fn": 282,
          "accuracy": 0.9359090909090909
        }
      },
      "auroc": 0.9853353693181819
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2386,
          "fn": 14,
          "accuracy": 0.9941666666666666
        },
        "0.01": {
          "tp": 2366,
          "fn": 34,
          "accuracy": 0.9858333333333333
        }
      },
      "auroc": 0.9974913194444444
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 6613,
          "fn": 187,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 6484,
          "fn": 316,
          "accuracy": 0.9535294117647058
        }
      },
      "auroc": 0.9896257046568627
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991614583333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991640625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991640625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991653645833334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9989885416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9987958333333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9988921875
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9399552083333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9979427083333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9689489583333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        }
      },
      "auroc": 0.969471875
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9983692708333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 766,
          "fn": 34,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 750,
          "fn": 50,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9839205729166667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9990875
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9991270833333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990354166666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991010416666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9990614583333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9991140625
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991291666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991479166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.994146875
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9988385416666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9964927083333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9966380208333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9990026041666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 786,
          "fn": 14,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9978203125
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.96336875
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9933020833333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        }
      },
      "auroc": 0.9783354166666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9812677083333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.996234375
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 777,
          "fn": 23,
          "accuracy": 0.97125
        },
        "0.01": {
          "tp": 759,
          "fn": 41,
          "accuracy": 0.94875
        }
      },
      "auroc": 0.9887510416666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990875
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.992253125
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9956703125
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991270833333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9957098958333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 795,
          "fn": 5,
          "accuracy": 0.99375
        },
        "0.01": {
          "tp": 792,
          "fn": 8,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9974184895833333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9983291666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9983291666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9969052083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9969052083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9976171875
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9976171875
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.9375781250000002
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.9375781250000002
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9177177083333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9177177083333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 267,
          "fn": 133,
          "accuracy": 0.6675
        }
      },
      "auroc": 0.9276479166666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 267,
          "fn": 133,
          "accuracy": 0.6675
        }
      },
      "auroc": 0.9276479166666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991479166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991479166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991572916666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991572916666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9934052083333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9934052083333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9810124999999998
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9810124999999998
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9872088541666666
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9872088541666666
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2160,
          "fn": 40,
          "accuracy": 0.9818181818181818
        },
        "0.01": {
          "tp": 2131,
          "fn": 69,
          "accuracy": 0.9686363636363636
        }
      },
      "auroc": 0.9929482007575756
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        }
      },
      "auroc": 0.9990916666666667
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3360,
          "fn": 40,
          "accuracy": 0.9882352941176471
        },
        "0.01": {
          "tp": 3329,
          "fn": 71,
          "accuracy": 0.9791176470588235
        }
      },
      "auroc": 0.9951164828431373
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2091,
          "fn": 109,
          "accuracy": 0.9504545454545454
        },
        "0.01": {
          "tp": 2019,
          "fn": 181,
          "accuracy": 0.9177272727272727
        }
      },
      "auroc": 0.9808037878787879
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1191,
          "fn": 9,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 1173,
          "fn": 27,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9967555555555556
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3282,
          "fn": 118,
          "accuracy": 0.9652941176470589
        },
        "0.01": {
          "tp": 3192,
          "fn": 208,
          "accuracy": 0.9388235294117647
        }
      },
      "auroc": 0.9864338235294119
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 4251,
          "fn": 149,
          "accuracy": 0.9661363636363637
        },
        "0.01": {
          "tp": 4150,
          "fn": 250,
          "accuracy": 0.9431818181818182
        }
      },
      "auroc": 0.9868759943181817
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2391,
          "fn": 9,
          "accuracy": 0.99625
        },
        "0.01": {
          "tp": 2371,
          "fn": 29,
          "accuracy": 0.9879166666666667
        }
      },
      "auroc": 0.9979236111111112
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 6642,
          "fn": 158,
          "accuracy": 0.976764705882353
        },
        "0.01": {
          "tp": 6521,
          "fn": 279,
          "accuracy": 0.9589705882352941
        }
      },
      "auroc": 0.9907751531862744
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991145833333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9986833333333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9988989583333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991614583333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9982302083333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9986958333333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991380208333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9984567708333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9987973958333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9985166666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9988854166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9987010416666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        }
      },
      "auroc": 0.92695625
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9976541666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9623052083333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9627364583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9982697916666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 760,
          "fn": 40,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 739,
          "fn": 61,
          "accuracy": 0.92375
        }
      },
      "auroc": 0.9805031249999999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991614583333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9980187500000001
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9985901041666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9989166666666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9990416666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991640625
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9984677083333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9988158854166667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991572916666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9990875
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9991223958333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9885322916666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9979572916666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9932447916666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9938447916666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9985223958333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 792,
          "fn": 8,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 773,
          "fn": 27,
          "accuracy": 0.96625
        }
      },
      "auroc": 0.9961835937500001
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9990479166666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9991072916666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.959765625
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.992115625
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.975940625
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9794661458333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9955817708333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 777,
          "fn": 23,
          "accuracy": 0.97125
        },
        "0.01": {
          "tp": 759,
          "fn": 41,
          "accuracy": 0.94875
        }
      },
      "auroc": 0.9875239583333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991385416666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991239583333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99913125
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.99786875
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.97870625
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9882875
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9985036458333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9889151041666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 788,
          "fn": 12,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 780,
          "fn": 20,
          "accuracy": 0.975
        }
      },
      "auroc": 0.993709375
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9951614583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9951614583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.99209375
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.99209375
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9936276041666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9936276041666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        }
      },
      "auroc": 0.8944614583333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        }
      },
      "auroc": 0.8944614583333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        }
      },
      "auroc": 0.8508958333333334
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        }
      },
      "auroc": 0.8508958333333334
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.8726786458333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.8726786458333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9950822916666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9950822916666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9971244791666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9971244791666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9860729166666666
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9860729166666666
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9658447916666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9658447916666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        }
      },
      "auroc": 0.9759588541666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        }
      },
      "auroc": 0.9759588541666667
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2133,
          "fn": 67,
          "accuracy": 0.9695454545454546
        },
        "0.01": {
          "tp": 2083,
          "fn": 117,
          "accuracy": 0.9468181818181818
        }
      },
      "auroc": 0.9880258522727273
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 1,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 1192,
          "fn": 8,
          "accuracy": 0.9933333333333333
        }
      },
      "auroc": 0.9988078125000001
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3332,
          "fn": 68,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 3275,
          "fn": 125,
          "accuracy": 0.9632352941176471
        }
      },
      "auroc": 0.99183125
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2033,
          "fn": 167,
          "accuracy": 0.9240909090909091
        },
        "0.01": {
          "tp": 1932,
          "fn": 268,
          "accuracy": 0.8781818181818182
        }
      },
      "auroc": 0.9704122159090909
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1182,
          "fn": 18,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 1161,
          "fn": 39,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9939300347222222
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3215,
          "fn": 185,
          "accuracy": 0.9455882352941176
        },
        "0.01": {
          "tp": 3093,
          "fn": 307,
          "accuracy": 0.9097058823529411
        }
      },
      "auroc": 0.9787126225490197
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 4166,
          "fn": 234,
          "accuracy": 0.9468181818181818
        },
        "0.01": {
          "tp": 4015,
          "fn": 385,
          "accuracy": 0.9125
        }
      },
      "auroc": 0.979219034090909
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2381,
          "fn": 19,
          "accuracy": 0.9920833333333333
        },
        "0.01": {
          "tp": 2353,
          "fn": 47,
          "accuracy": 0.9804166666666667
        }
      },
      "auroc": 0.9963689236111111
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 6547,
          "fn": 253,
          "accuracy": 0.9627941176470588
        },
        "0.01": {
          "tp": 6368,
          "fn": 432,
          "accuracy": 0.9364705882352942
        }
      },
      "auroc": 0.9852719362745098
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991614583333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991640625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991640625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991653645833334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9989885416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9988
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9988942708333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9264864583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9979979166666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9622421875
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9627375000000001
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9983989583333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 760,
          "fn": 40,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 731,
          "fn": 69,
          "accuracy": 0.91375
        }
      },
      "auroc": 0.9805682291666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9990822916666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9991244791666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990343749999999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991005208333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9990583333333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9991125
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991614583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991640625
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9898343749999999
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9977510416666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9937927083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9944979166666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9984588541666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 792,
          "fn": 8,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 774,
          "fn": 26,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9964783854166668
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9565114583333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9896291666666668
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9730703125
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9778390625
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9943979166666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 771,
          "fn": 29,
          "accuracy": 0.96375
        },
        "0.01": {
          "tp": 754,
          "fn": 46,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9861184895833333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991270833333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.99004375
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9945854166666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999146875
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9946052083333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 795,
          "fn": 5,
          "accuracy": 0.99375
        },
        "0.01": {
          "tp": 789,
          "fn": 11,
          "accuracy": 0.98625
        }
      },
      "auroc": 0.9968760416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9981822916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9981822916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9965437500000001
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9965437500000001
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9973630208333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9973630208333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9313020833333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9313020833333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9037135416666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9037135416666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        }
      },
      "auroc": 0.9175078125
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        }
      },
      "auroc": 0.9175078125
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9990520833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9990520833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.999109375
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.999109375
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9917093749999999
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9917093749999999
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9787239583333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9787239583333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9852166666666666
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9852166666666666
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2155,
          "fn": 45,
          "accuracy": 0.9795454545454545
        },
        "0.01": {
          "tp": 2126,
          "fn": 74,
          "accuracy": 0.9663636363636363
        }
      },
      "auroc": 0.9922130681818182
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1197,
          "fn": 3,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9990914930555556
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3355,
          "fn": 45,
          "accuracy": 0.986764705882353
        },
        "0.01": {
          "tp": 3323,
          "fn": 77,
          "accuracy": 0.9773529411764705
        }
      },
      "auroc": 0.9946407475490195
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2065,
          "fn": 135,
          "accuracy": 0.9386363636363636
        },
        "0.01": {
          "tp": 1983,
          "fn": 217,
          "accuracy": 0.9013636363636364
        }
      },
      "auroc": 0.9770447916666667
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1187,
          "fn": 13,
          "accuracy": 0.9891666666666666
        },
        "0.01": {
          "tp": 1164,
          "fn": 36,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9956029513888889
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3252,
          "fn": 148,
          "accuracy": 0.9564705882352941
        },
        "0.01": {
          "tp": 3147,
          "fn": 253,
          "accuracy": 0.9255882352941176
        }
      },
      "auroc": 0.9835947303921568
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 4220,
          "fn": 180,
          "accuracy": 0.9590909090909091
        },
        "0.01": {
          "tp": 4109,
          "fn": 291,
          "accuracy": 0.9338636363636363
        }
      },
      "auroc": 0.9846289299242423
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2387,
          "fn": 13,
          "accuracy": 0.9945833333333334
        },
        "0.01": {
          "tp": 2361,
          "fn": 39,
          "accuracy": 0.98375
        }
      },
      "auroc": 0.9973472222222222
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 6607,
          "fn": 193,
          "accuracy": 0.9716176470588235
        },
        "0.01": {
          "tp": 6470,
          "fn": 330,
          "accuracy": 0.9514705882352941
        }
      },
      "auroc": 0.9891177389705881
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9989427083333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9930635416666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.996003125
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9983989583333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.993390625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9958947916666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9986708333333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9932270833333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 792,
          "fn": 8,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 777,
          "fn": 23,
          "accuracy": 0.97125
        }
      },
      "auroc": 0.9959489583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.993
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9684989583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9807494791666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.9136395833333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9759093750000001
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9447744791666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9533197916666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9722041666666668
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 713,
          "fn": 87,
          "accuracy": 0.89125
        },
        "0.01": {
          "tp": 584,
          "fn": 216,
          "accuracy": 0.73
        }
      },
      "auroc": 0.9627619791666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999125
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.994328125
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9967265625
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9989864583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.96113125
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        }
      },
      "auroc": 0.9800588541666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9990557291666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9777296875
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 777,
          "fn": 23,
          "accuracy": 0.97125
        },
        "0.01": {
          "tp": 749,
          "fn": 51,
          "accuracy": 0.93625
        }
      },
      "auroc": 0.9883927083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9984322916666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9977572916666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9980947916666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9681468750000001
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.960171875
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        }
      },
      "auroc": 0.9641593750000002
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9832895833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9789645833333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 757,
          "fn": 43,
          "accuracy": 0.94625
        },
        "0.01": {
          "tp": 672,
          "fn": 128,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9811270833333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.996721875
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.9590354166666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        }
      },
      "auroc": 0.9778786458333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.888878125
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        }
      },
      "auroc": 0.8669489583333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        }
      },
      "auroc": 0.8779135416666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        }
      },
      "auroc": 0.9428000000000001
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        }
      },
      "auroc": 0.9129921875
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 642,
          "fn": 158,
          "accuracy": 0.8025
        },
        "0.01": {
          "tp": 527,
          "fn": 273,
          "accuracy": 0.65875
        }
      },
      "auroc": 0.92789609375
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990979166666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9946760416666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9968869791666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9963208333333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9697416666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        }
      },
      "auroc": 0.98303125
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9977093750000001
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9822088541666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 778,
          "fn": 22,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 756,
          "fn": 44,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9899591145833333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.996203125
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.996203125
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9927281250000001
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9927281250000001
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.994465625
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.994465625
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        }
      },
      "auroc": 0.923640625
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        }
      },
      "auroc": 0.923640625
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8881885416666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8881885416666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        }
      },
      "auroc": 0.9059145833333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        }
      },
      "auroc": 0.9059145833333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9988708333333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9988708333333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9989854166666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9989854166666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99906875
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99906875
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9879625000000001
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9879625000000001
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.993515625
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.993515625
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9864593749999999
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9864593749999999
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9679645833333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9679645833333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9772119791666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9772119791666667
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2139,
          "fn": 61,
          "accuracy": 0.9722727272727273
        },
        "0.01": {
          "tp": 2069,
          "fn": 131,
          "accuracy": 0.9404545454545454
        }
      },
      "auroc": 0.9899810606060606
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1145,
          "fn": 55,
          "accuracy": 0.9541666666666667
        },
        "0.01": {
          "tp": 1059,
          "fn": 141,
          "accuracy": 0.8825
        }
      },
      "auroc": 0.9845598958333334
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3284,
          "fn": 116,
          "accuracy": 0.9658823529411765
        },
        "0.01": {
          "tp": 3128,
          "fn": 272,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9880677083333333
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1980,
          "fn": 220,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 1773,
          "fn": 427,
          "accuracy": 0.8059090909090909
        }
      },
      "auroc": 0.9636441287878789
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1046,
          "fn": 154,
          "accuracy": 0.8716666666666667
        },
        "0.01": {
          "tp": 897,
          "fn": 303,
          "accuracy": 0.7475
        }
      },
      "auroc": 0.9545489583333333
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3026,
          "fn": 374,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 2670,
          "fn": 730,
          "accuracy": 0.7852941176470588
        }
      },
      "auroc": 0.9604340686274511
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 4119,
          "fn": 281,
          "accuracy": 0.9361363636363637
        },
        "0.01": {
          "tp": 3842,
          "fn": 558,
          "accuracy": 0.8731818181818182
        }
      },
      "auroc": 0.9768125946969698
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2191,
          "fn": 209,
          "accuracy": 0.9129166666666667
        },
        "0.01": {
          "tp": 1956,
          "fn": 444,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9695544270833334
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 6310,
          "fn": 490,
          "accuracy": 0.9279411764705883
        },
        "0.01": {
          "tp": 5798,
          "fn": 1002,
          "accuracy": 0.8526470588235294
        }
      },
      "auroc": 0.9742508884803922
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991614583333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991640625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991640625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991653645833334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9989614583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9988395833333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9989005208333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9339104166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9975541666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        }
      },
      "auroc": 0.9657322916666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        }
      },
      "auroc": 0.9664359375
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.998196875
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 761,
          "fn": 39,
          "accuracy": 0.95125
        },
        "0.01": {
          "tp": 732,
          "fn": 68,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9823164062499999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990843749999999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991255208333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990635416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991151041666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990739583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991203125
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991614583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991640625
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9909708333333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9980854166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.994528125
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9950661458333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9986260416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 777,
          "fn": 23,
          "accuracy": 0.97125
        }
      },
      "auroc": 0.99684609375
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.958596875
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.99015625
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9743765625
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9788817708333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9946614583333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 771,
          "fn": 29,
          "accuracy": 0.96375
        },
        "0.01": {
          "tp": 754,
          "fn": 46,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9867716145833333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991458333333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9907854166666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.994965625
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991562500000001
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9949760416666668
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 795,
          "fn": 5,
          "accuracy": 0.99375
        },
        "0.01": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9970661458333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9983479166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9983479166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9965927083333334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9965927083333334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9974703125000001
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9974703125000001
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9340385416666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9340385416666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9075510416666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9075510416666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        }
      },
      "auroc": 0.9207947916666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        }
      },
      "auroc": 0.9207947916666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991520833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991520833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999159375
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999159375
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.992053125
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.992053125
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.98083125
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.98083125
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9864421875
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9864421875
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2155,
          "fn": 45,
          "accuracy": 0.9795454545454545
        },
        "0.01": {
          "tp": 2128,
          "fn": 72,
          "accuracy": 0.9672727272727273
        }
      },
      "auroc": 0.9925056818181818
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        }
      },
      "auroc": 0.9990984375
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3355,
          "fn": 45,
          "accuracy": 0.986764705882353
        },
        "0.01": {
          "tp": 3326,
          "fn": 74,
          "accuracy": 0.9782352941176471
        }
      },
      "auroc": 0.994832536764706
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2070,
          "fn": 130,
          "accuracy": 0.9409090909090909
        },
        "0.01": {
          "tp": 1989,
          "fn": 211,
          "accuracy": 0.9040909090909091
        }
      },
      "auroc": 0.9785682765151514
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1187,
          "fn": 13,
          "accuracy": 0.9891666666666666
        },
        "0.01": {
          "tp": 1166,
          "fn": 34,
          "accuracy": 0.9716666666666667
        }
      },
      "auroc": 0.9958010416666666
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3257,
          "fn": 143,
          "accuracy": 0.9579411764705882
        },
        "0.01": {
          "tp": 3155,
          "fn": 245,
          "accuracy": 0.9279411764705883
        }
      },
      "auroc": 0.9846504289215685
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 4225,
          "fn": 175,
          "accuracy": 0.9602272727272727
        },
        "0.01": {
          "tp": 4117,
          "fn": 283,
          "accuracy": 0.9356818181818182
        }
      },
      "auroc": 0.9855369791666667
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2387,
          "fn": 13,
          "accuracy": 0.9945833333333334
        },
        "0.01": {
          "tp": 2364,
          "fn": 36,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9974497395833333
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 6612,
          "fn": 188,
          "accuracy": 0.9723529411764706
        },
        "0.01": {
          "tp": 6481,
          "fn": 319,
          "accuracy": 0.9530882352941177
        }
      },
      "auroc": 0.9897414828431372
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991614583333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991640625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991640625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991653645833334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.99899375
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9988333333333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9989135416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.933028125
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9979760416666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9655020833333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        }
      },
      "auroc": 0.9660109375000001
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9984046875
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 764,
          "fn": 36,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 733,
          "fn": 67,
          "accuracy": 0.91625
        }
      },
      "auroc": 0.9822078125
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990843749999999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991255208333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990635416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991151041666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990739583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991203125
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991614583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991640625
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9907552083333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.998090625
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9944229166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9949583333333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9986286458333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 793,
          "fn": 7,
          "accuracy": 0.99125
        },
        "0.01": {
          "tp": 777,
          "fn": 23,
          "accuracy": 0.97125
        }
      },
      "auroc": 0.9967934895833334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9579875
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9902729166666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        }
      },
      "auroc": 0.9741302083333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9785770833333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9947197916666668
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 770,
          "fn": 30,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 755,
          "fn": 45,
          "accuracy": 0.94375
        }
      },
      "auroc": 0.9866484375
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991458333333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9907604166666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.994953125
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991562500000001
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9949635416666668
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 795,
          "fn": 5,
          "accuracy": 0.99375
        },
        "0.01": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9970598958333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9981916666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9981916666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.99658125
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.99658125
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9973864583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9973864583333334
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9327156249999999
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9327156249999999
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9066885416666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9066885416666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 267,
          "fn": 133,
          "accuracy": 0.6675
        }
      },
      "auroc": 0.9197020833333334
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 267,
          "fn": 133,
          "accuracy": 0.6675
        }
      },
      "auroc": 0.9197020833333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991520833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991520833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999159375
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999159375
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.992421875
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.992421875
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.980059375
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.980059375
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9862406250000001
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9862406250000001
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2155,
          "fn": 45,
          "accuracy": 0.9795454545454545
        },
        "0.01": {
          "tp": 2129,
          "fn": 71,
          "accuracy": 0.9677272727272728
        }
      },
      "auroc": 0.9924076704545455
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        }
      },
      "auroc": 0.9990973958333333
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3355,
          "fn": 45,
          "accuracy": 0.986764705882353
        },
        "0.01": {
          "tp": 3327,
          "fn": 73,
          "accuracy": 0.9785294117647059
        }
      },
      "auroc": 0.99476875
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2072,
          "fn": 128,
          "accuracy": 0.9418181818181818
        },
        "0.01": {
          "tp": 1989,
          "fn": 211,
          "accuracy": 0.9040909090909091
        }
      },
      "auroc": 0.9782634469696969
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1186,
          "fn": 14,
          "accuracy": 0.9883333333333333
        },
        "0.01": {
          "tp": 1168,
          "fn": 32,
          "accuracy": 0.9733333333333334
        }
      },
      "auroc": 0.9958875
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3258,
          "fn": 142,
          "accuracy": 0.9582352941176471
        },
        "0.01": {
          "tp": 3157,
          "fn": 243,
          "accuracy": 0.9285294117647059
        }
      },
      "auroc": 0.9844837009803922
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 4227,
          "fn": 173,
          "accuracy": 0.9606818181818182
        },
        "0.01": {
          "tp": 4118,
          "fn": 282,
          "accuracy": 0.9359090909090909
        }
      },
      "auroc": 0.9853355587121213
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2386,
          "fn": 14,
          "accuracy": 0.9941666666666666
        },
        "0.01": {
          "tp": 2366,
          "fn": 34,
          "accuracy": 0.9858333333333333
        }
      },
      "auroc": 0.9974924479166667
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 6613,
          "fn": 187,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 6484,
          "fn": 316,
          "accuracy": 0.9535294117647058
        }
      },
      "auroc": 0.9896262254901961
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9940791666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.99384375
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.9939614583333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9928989583333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9854479166666668
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        }
      },
      "auroc": 0.9891734375
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9934890625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9896458333333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 783,
          "fn": 17,
          "accuracy": 0.97875
        },
        "0.01": {
          "tp": 710,
          "fn": 90,
          "accuracy": 0.8875
        }
      },
      "auroc": 0.9915674479166666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9842020833333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.9656708333333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 280,
          "fn": 120,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9749364583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.9136625
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.966021875
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        }
      },
      "auroc": 0.9398421875
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        },
        "0.01": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        }
      },
      "auroc": 0.9489322916666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        }
      },
      "auroc": 0.9658463541666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 678,
          "fn": 122,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 475,
          "fn": 325,
          "accuracy": 0.59375
        }
      },
      "auroc": 0.9573893229166667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9977729166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.9754333333333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        }
      },
      "auroc": 0.9866031249999999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9964552083333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        }
      },
      "auroc": 0.96399375
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        }
      },
      "auroc": 0.9802244791666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9971140625
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        }
      },
      "auroc": 0.9697135416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 761,
          "fn": 39,
          "accuracy": 0.95125
        },
        "0.01": {
          "tp": 623,
          "fn": 177,
          "accuracy": 0.77875
        }
      },
      "auroc": 0.9834138020833334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9971125000000001
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9974697916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9972911458333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        }
      },
      "auroc": 0.9505177083333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.9606239583333332
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        },
        "0.01": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        }
      },
      "auroc": 0.9555708333333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        }
      },
      "auroc": 0.9738151041666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 295,
          "fn": 105,
          "accuracy": 0.7375
        }
      },
      "auroc": 0.9790468750000001
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 728,
          "fn": 72,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 597,
          "fn": 203,
          "accuracy": 0.74625
        }
      },
      "auroc": 0.9764309895833334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9981375
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9986520833333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.9630020833333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.9691041666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 278,
          "fn": 122,
          "accuracy": 0.695
        }
      },
      "auroc": 0.966053125
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        }
      },
      "auroc": 0.981084375
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        }
      },
      "auroc": 0.9836208333333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 745,
          "fn": 55,
          "accuracy": 0.93125
        },
        "0.01": {
          "tp": 675,
          "fn": 125,
          "accuracy": 0.84375
        }
      },
      "auroc": 0.9823526041666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9948489583333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9820822916666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        }
      },
      "auroc": 0.988465625
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9877572916666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.9734395833333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        }
      },
      "auroc": 0.9805984375
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9913031250000001
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 271,
          "fn": 129,
          "accuracy": 0.6775
        }
      },
      "auroc": 0.9777609375
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 770,
          "fn": 30,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 623,
          "fn": 177,
          "accuracy": 0.77875
        }
      },
      "auroc": 0.98453203125
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9829291666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9829291666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        }
      },
      "auroc": 0.9660604166666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        }
      },
      "auroc": 0.9660604166666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 274,
          "fn": 126,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9744947916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 274,
          "fn": 126,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9744947916666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        }
      },
      "auroc": 0.8994125
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        }
      },
      "auroc": 0.8994125
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.9014572916666668
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.9014572916666668
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 236,
          "fn": 164,
          "accuracy": 0.59
        },
        "0.01": {
          "tp": 79,
          "fn": 321,
          "accuracy": 0.1975
        }
      },
      "auroc": 0.9004348958333335
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 236,
          "fn": 164,
          "accuracy": 0.59
        },
        "0.01": {
          "tp": 79,
          "fn": 321,
          "accuracy": 0.1975
        }
      },
      "auroc": 0.9004348958333335
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9972447916666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9972447916666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9934854166666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9934854166666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9953651041666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9953651041666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9980458333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9980458333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        }
      },
      "auroc": 0.9765302083333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        }
      },
      "auroc": 0.9765302083333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9872880208333332
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9872880208333332
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9795281250000001
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9795281250000001
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.9697
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.9697
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        }
      },
      "auroc": 0.9746140624999999
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        }
      },
      "auroc": 0.9746140624999999
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2081,
          "fn": 119,
          "accuracy": 0.9459090909090909
        },
        "0.01": {
          "tp": 1861,
          "fn": 339,
          "accuracy": 0.8459090909090909
        }
      },
      "auroc": 0.9840311553030303
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1151,
          "fn": 49,
          "accuracy": 0.9591666666666666
        },
        "0.01": {
          "tp": 979,
          "fn": 221,
          "accuracy": 0.8158333333333333
        }
      },
      "auroc": 0.9854395833333334
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3232,
          "fn": 168,
          "accuracy": 0.9505882352941176
        },
        "0.01": {
          "tp": 2840,
          "fn": 560,
          "accuracy": 0.8352941176470589
        }
      },
      "auroc": 0.9845282475490196
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1900,
          "fn": 300,
          "accuracy": 0.8636363636363636
        },
        "0.01": {
          "tp": 1468,
          "fn": 732,
          "accuracy": 0.6672727272727272
        }
      },
      "auroc": 0.9646842803030303
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1083,
          "fn": 117,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 751,
          "fn": 449,
          "accuracy": 0.6258333333333334
        }
      },
      "auroc": 0.969771875
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2983,
          "fn": 417,
          "accuracy": 0.8773529411764706
        },
        "0.01": {
          "tp": 2219,
          "fn": 1181,
          "accuracy": 0.6526470588235294
        }
      },
      "auroc": 0.9664799019607843
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3981,
          "fn": 419,
          "accuracy": 0.9047727272727273
        },
        "0.01": {
          "tp": 3329,
          "fn": 1071,
          "accuracy": 0.7565909090909091
        }
      },
      "auroc": 0.9743577178030304
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2234,
          "fn": 166,
          "accuracy": 0.9308333333333333
        },
        "0.01": {
          "tp": 1730,
          "fn": 670,
          "accuracy": 0.7208333333333333
        }
      },
      "auroc": 0.9776057291666667
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 6215,
          "fn": 585,
          "accuracy": 0.9139705882352941
        },
        "0.01": {
          "tp": 5059,
          "fn": 1741,
          "accuracy": 0.7439705882352942
        }
      },
      "auroc": 0.975504074754902
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9990729166666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9991197916666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9991197916666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9991432291666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9988520833333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9988333333333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9988427083333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        }
      },
      "auroc": 0.940584375
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9978781250000001
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        }
      },
      "auroc": 0.96923125
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        }
      },
      "auroc": 0.9697182291666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9983557291666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 765,
          "fn": 35,
          "accuracy": 0.95625
        },
        "0.01": {
          "tp": 747,
          "fn": 53,
          "accuracy": 0.93375
        }
      },
      "auroc": 0.9840369791666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990843749999999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991255208333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990708333333332
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991187499999999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990776041666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991221354166667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991614583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991640625
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9907875
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.998703125
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9947453125000001
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9949744791666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9989348958333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 783,
          "fn": 17,
          "accuracy": 0.97875
        }
      },
      "auroc": 0.9969546875
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.959521875
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9927489583333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        }
      },
      "auroc": 0.9761354166666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        }
      },
      "auroc": 0.9793442708333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9959578125
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 776,
          "fn": 24,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 759,
          "fn": 41,
          "accuracy": 0.94875
        }
      },
      "auroc": 0.9876510416666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9990104166666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9923364583333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9956734375
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9990885416666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9957515625000001
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 795,
          "fn": 5,
          "accuracy": 0.99375
        },
        "0.01": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9974200520833333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9977510416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9977510416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.994575
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.994575
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9961630208333334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9961630208333334
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9245125000000001
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9245125000000001
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.89858125
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.89858125
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        }
      },
      "auroc": 0.911546875
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        }
      },
      "auroc": 0.911546875
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991291666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991291666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991479166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991479166666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9908427083333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9908427083333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9754572916666666
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9754572916666666
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        }
      },
      "auroc": 0.98315
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        }
      },
      "auroc": 0.98315
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2152,
          "fn": 48,
          "accuracy": 0.9781818181818182
        },
        "0.01": {
          "tp": 2126,
          "fn": 74,
          "accuracy": 0.9663636363636363
        }
      },
      "auroc": 0.9914654356060606
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        }
      },
      "auroc": 0.9990973958333333
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3352,
          "fn": 48,
          "accuracy": 0.9858823529411764
        },
        "0.01": {
          "tp": 3324,
          "fn": 76,
          "accuracy": 0.9776470588235294
        }
      },
      "auroc": 0.994159068627451
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2069,
          "fn": 131,
          "accuracy": 0.9404545454545454
        },
        "0.01": {
          "tp": 2002,
          "fn": 198,
          "accuracy": 0.91
        }
      },
      "auroc": 0.977740625
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1191,
          "fn": 9,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 1173,
          "fn": 27,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9966350694444445
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3260,
          "fn": 140,
          "accuracy": 0.9588235294117647
        },
        "0.01": {
          "tp": 3175,
          "fn": 225,
          "accuracy": 0.9338235294117647
        }
      },
      "auroc": 0.9844092524509803
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 4221,
          "fn": 179,
          "accuracy": 0.9593181818181818
        },
        "0.01": {
          "tp": 4128,
          "fn": 272,
          "accuracy": 0.9381818181818182
        }
      },
      "auroc": 0.9846030303030304
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2391,
          "fn": 9,
          "accuracy": 0.99625
        },
        "0.01": {
          "tp": 2371,
          "fn": 29,
          "accuracy": 0.9879166666666667
        }
      },
      "auroc": 0.997866232638889
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 6612,
          "fn": 188,
          "accuracy": 0.9723529411764706
        },
        "0.01": {
          "tp": 6499,
          "fn": 301,
          "accuracy": 0.955735294117647
        }
      },
      "auroc": 0.9892841605392156
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991614583333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991640625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991640625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991653645833334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9989322916666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.998771875
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9988520833333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.9308864583333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9980041666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        }
      },
      "auroc": 0.9644453125000001
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9649093750000001
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9983880208333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 762,
          "fn": 38,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 732,
          "fn": 68,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9816486979166668
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991125
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991395833333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990697916666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991182291666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990911458333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99912890625
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991614583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991640625
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9909520833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9978760416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9944140625
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9950567708333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9985213541666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 778,
          "fn": 22,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9967890625
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9557916666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9888020833333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.9722968750000001
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9774791666666668
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.993984375
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 769,
          "fn": 31,
          "accuracy": 0.96125
        },
        "0.01": {
          "tp": 751,
          "fn": 49,
          "accuracy": 0.93875
        }
      },
      "auroc": 0.9857317708333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9990520833333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9886333333333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9938427083333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.999109375
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9939
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 789,
          "fn": 11,
          "accuracy": 0.98625
        }
      },
      "auroc": 0.9965046875
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9981
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9981
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.996503125
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.996503125
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9973015625
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9973015625
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.9299520833333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.9299520833333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9036541666666668
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9036541666666668
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        }
      },
      "auroc": 0.916803125
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        }
      },
      "auroc": 0.916803125
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991458333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991458333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991562500000001
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991562500000001
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9918645833333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9918645833333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9789645833333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9789645833333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9854145833333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9854145833333333
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2153,
          "fn": 47,
          "accuracy": 0.9786363636363636
        },
        "0.01": {
          "tp": 2126,
          "fn": 74,
          "accuracy": 0.9663636363636363
        }
      },
      "auroc": 0.992091856060606
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        }
      },
      "auroc": 0.9990918402777778
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3353,
          "fn": 47,
          "accuracy": 0.9861764705882353
        },
        "0.01": {
          "tp": 3324,
          "fn": 76,
          "accuracy": 0.9776470588235294
        }
      },
      "auroc": 0.9945624387254902
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2067,
          "fn": 133,
          "accuracy": 0.9395454545454546
        },
        "0.01": {
          "tp": 1988,
          "fn": 212,
          "accuracy": 0.9036363636363637
        }
      },
      "auroc": 0.9774954545454546
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1185,
          "fn": 15,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 1165,
          "fn": 35,
          "accuracy": 0.9708333333333333
        }
      },
      "auroc": 0.9952578125
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3252,
          "fn": 148,
          "accuracy": 0.9564705882352941
        },
        "0.01": {
          "tp": 3153,
          "fn": 247,
          "accuracy": 0.9273529411764706
        }
      },
      "auroc": 0.9837645220588235
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 4220,
          "fn": 180,
          "accuracy": 0.9590909090909091
        },
        "0.01": {
          "tp": 4114,
          "fn": 286,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9847936553030303
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2385,
          "fn": 15,
          "accuracy": 0.99375
        },
        "0.01": {
          "tp": 2363,
          "fn": 37,
          "accuracy": 0.9845833333333334
        }
      },
      "auroc": 0.9971748263888889
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 6605,
          "fn": 195,
          "accuracy": 0.9713235294117647
        },
        "0.01": {
          "tp": 6477,
          "fn": 323,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9891634803921568
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        }
      },
      "auroc": 0.9801177083333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.9780885416666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        }
      },
      "auroc": 0.979103125
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.9797239583333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        }
      },
      "auroc": 0.9733322916666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        }
      },
      "auroc": 0.976528125
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        }
      },
      "auroc": 0.9799208333333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        }
      },
      "auroc": 0.9757104166666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 786,
          "fn": 14,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 423,
          "fn": 377,
          "accuracy": 0.52875
        }
      },
      "auroc": 0.9778156249999999
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        }
      },
      "auroc": 0.9723781249999999
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        }
      },
      "auroc": 0.9487552083333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 167,
          "fn": 233,
          "accuracy": 0.4175
        }
      },
      "auroc": 0.9605666666666668
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        }
      },
      "auroc": 0.9328020833333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        }
      },
      "auroc": 0.9497166666666668
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": {
          "tp": 136,
          "fn": 264,
          "accuracy": 0.34
        }
      },
      "auroc": 0.941259375
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": {
          "tp": 178,
          "fn": 222,
          "accuracy": 0.445
        }
      },
      "auroc": 0.9525901041666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 125,
          "fn": 275,
          "accuracy": 0.3125
        }
      },
      "auroc": 0.9492359375
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 670,
          "fn": 130,
          "accuracy": 0.8375
        },
        "0.01": {
          "tp": 303,
          "fn": 497,
          "accuracy": 0.37875
        }
      },
      "auroc": 0.9509130208333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        }
      },
      "auroc": 0.9886583333333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        }
      },
      "auroc": 0.9668072916666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.9777328125
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        }
      },
      "auroc": 0.9861864583333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.9589281249999999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        }
      },
      "auroc": 0.9725572916666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9874223958333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        }
      },
      "auroc": 0.9628677083333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 774,
          "fn": 26,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 398,
          "fn": 402,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.9751450520833334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9943156249999999
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9913125
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        }
      },
      "auroc": 0.9928140625
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        }
      },
      "auroc": 0.9537322916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.9608614583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 167,
          "fn": 233,
          "accuracy": 0.4175
        }
      },
      "auroc": 0.957296875
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        }
      },
      "auroc": 0.9740239583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        }
      },
      "auroc": 0.9760869791666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 743,
          "fn": 57,
          "accuracy": 0.92875
        },
        "0.01": {
          "tp": 512,
          "fn": 288,
          "accuracy": 0.64
        }
      },
      "auroc": 0.97505546875
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9989604166666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9984208333333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9986906249999999
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.9684677083333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.969225
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        }
      },
      "auroc": 0.9688463541666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9837140625
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        }
      },
      "auroc": 0.9838229166666668
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 748,
          "fn": 52,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 661,
          "fn": 139,
          "accuracy": 0.82625
        }
      },
      "auroc": 0.9837684895833333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        }
      },
      "auroc": 0.9788822916666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        }
      },
      "auroc": 0.9595166666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 157,
          "fn": 243,
          "accuracy": 0.3925
        }
      },
      "auroc": 0.9691994791666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        }
      },
      "auroc": 0.9685197916666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.9548322916666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 146,
          "fn": 254,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9616760416666668
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 209,
          "fn": 191,
          "accuracy": 0.5225
        }
      },
      "auroc": 0.9737010416666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 94,
          "fn": 306,
          "accuracy": 0.235
        }
      },
      "auroc": 0.9571744791666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 761,
          "fn": 39,
          "accuracy": 0.95125
        },
        "0.01": {
          "tp": 303,
          "fn": 497,
          "accuracy": 0.37875
        }
      },
      "auroc": 0.9654377604166666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        }
      },
      "auroc": 0.96624375
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        }
      },
      "auroc": 0.96624375
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.9493114583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.9493114583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 145,
          "fn": 255,
          "accuracy": 0.3625
        }
      },
      "auroc": 0.9577776041666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 145,
          "fn": 255,
          "accuracy": 0.3625
        }
      },
      "auroc": 0.9577776041666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.9097343750000002
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.9097343750000002
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        }
      },
      "auroc": 0.9125093750000001
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        }
      },
      "auroc": 0.9125093750000001
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 35,
          "fn": 365,
          "accuracy": 0.0875
        }
      },
      "auroc": 0.911121875
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 35,
          "fn": 365,
          "accuracy": 0.0875
        }
      },
      "auroc": 0.911121875
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.9840010416666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.9840010416666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        }
      },
      "auroc": 0.97598125
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        }
      },
      "auroc": 0.97598125
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        }
      },
      "auroc": 0.9799911458333332
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        }
      },
      "auroc": 0.9799911458333332
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9928645833333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9928645833333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        }
      },
      "auroc": 0.969009375
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        }
      },
      "auroc": 0.969009375
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        }
      },
      "auroc": 0.9809369791666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        }
      },
      "auroc": 0.9809369791666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.9710072916666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.9710072916666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.9680177083333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.9680177083333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        }
      },
      "auroc": 0.9695125
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        }
      },
      "auroc": 0.9695125
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2058,
          "fn": 142,
          "accuracy": 0.9354545454545454
        },
        "0.01": {
          "tp": 1412,
          "fn": 788,
          "accuracy": 0.6418181818181818
        }
      },
      "auroc": 0.9761057765151515
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1156,
          "fn": 44,
          "accuracy": 0.9633333333333334
        },
        "0.01": {
          "tp": 621,
          "fn": 579,
          "accuracy": 0.5175
        }
      },
      "auroc": 0.9738168402777778
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3214,
          "fn": 186,
          "accuracy": 0.9452941176470588
        },
        "0.01": {
          "tp": 2033,
          "fn": 1367,
          "accuracy": 0.5979411764705882
        }
      },
      "auroc": 0.9752979166666668
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1915,
          "fn": 285,
          "accuracy": 0.8704545454545455
        },
        "0.01": {
          "tp": 1007,
          "fn": 1193,
          "accuracy": 0.4577272727272727
        }
      },
      "auroc": 0.9603874053030304
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1094,
          "fn": 106,
          "accuracy": 0.9116666666666666
        },
        "0.01": {
          "tp": 467,
          "fn": 733,
          "accuracy": 0.38916666666666666
        }
      },
      "auroc": 0.9611493055555556
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3009,
          "fn": 391,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 1474,
          "fn": 1926,
          "accuracy": 0.4335294117647059
        }
      },
      "auroc": 0.9606563112745098
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3973,
          "fn": 427,
          "accuracy": 0.9029545454545455
        },
        "0.01": {
          "tp": 2419,
          "fn": 1981,
          "accuracy": 0.5497727272727273
        }
      },
      "auroc": 0.9682465909090909
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2250,
          "fn": 150,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 1088,
          "fn": 1312,
          "accuracy": 0.4533333333333333
        }
      },
      "auroc": 0.9674830729166666
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 6223,
          "fn": 577,
          "accuracy": 0.9151470588235294
        },
        "0.01": {
          "tp": 3507,
          "fn": 3293,
          "accuracy": 0.5157352941176471
        }
      },
      "auroc": 0.9679771139705882
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2394,
          "fn": 6,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 2300,
          "fn": 100,
          "accuracy": 0.9583333333333334
        }
      },
      "auroc": 0.9971322916666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2390,
          "fn": 10,
          "accuracy": 0.9958333333333333
        },
        "0.01": {
          "tp": 2278,
          "fn": 122,
          "accuracy": 0.9491666666666667
        }
      },
      "auroc": 0.9964177083333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4784,
          "fn": 16,
          "accuracy": 0.9966666666666667
        },
        "0.01": {
          "tp": 4578,
          "fn": 222,
          "accuracy": 0.95375
        }
      },
      "auroc": 0.9967750000000001
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2396,
          "fn": 4,
          "accuracy": 0.9983333333333333
        },
        "0.01": {
          "tp": 2289,
          "fn": 111,
          "accuracy": 0.95375
        }
      },
      "auroc": 0.9969597222222222
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2381,
          "fn": 19,
          "accuracy": 0.9920833333333333
        },
        "0.01": {
          "tp": 2236,
          "fn": 164,
          "accuracy": 0.9316666666666666
        }
      },
      "auroc": 0.9953003472222222
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4777,
          "fn": 23,
          "accuracy": 0.9952083333333334
        },
        "0.01": {
          "tp": 4525,
          "fn": 275,
          "accuracy": 0.9427083333333334
        }
      },
      "auroc": 0.9961300347222222
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4790,
          "fn": 10,
          "accuracy": 0.9979166666666667
        },
        "0.01": {
          "tp": 4589,
          "fn": 211,
          "accuracy": 0.9560416666666667
        }
      },
      "auroc": 0.9970460069444445
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4771,
          "fn": 29,
          "accuracy": 0.9939583333333334
        },
        "0.01": {
          "tp": 4514,
          "fn": 286,
          "accuracy": 0.9404166666666667
        }
      },
      "auroc": 0.9958590277777778
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9561,
          "fn": 39,
          "accuracy": 0.9959375
        },
        "0.01": {
          "tp": 9103,
          "fn": 497,
          "accuracy": 0.9482291666666667
        }
      },
      "auroc": 0.996452517361111
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2378,
          "fn": 22,
          "accuracy": 0.9908333333333333
        },
        "0.01": {
          "tp": 2241,
          "fn": 159,
          "accuracy": 0.93375
        }
      },
      "auroc": 0.9949834201388889
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2334,
          "fn": 66,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 2108,
          "fn": 292,
          "accuracy": 0.8783333333333333
        }
      },
      "auroc": 0.9893625868055556
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4712,
          "fn": 88,
          "accuracy": 0.9816666666666667
        },
        "0.01": {
          "tp": 4349,
          "fn": 451,
          "accuracy": 0.9060416666666666
        }
      },
      "auroc": 0.9921730034722223
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1883,
          "fn": 517,
          "accuracy": 0.7845833333333333
        },
        "0.01": {
          "tp": 1535,
          "fn": 865,
          "accuracy": 0.6395833333333333
        }
      },
      "auroc": 0.929830642361111
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2332,
          "fn": 68,
          "accuracy": 0.9716666666666667
        },
        "0.01": {
          "tp": 2108,
          "fn": 292,
          "accuracy": 0.8783333333333333
        }
      },
      "auroc": 0.9893868923611111
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4215,
          "fn": 585,
          "accuracy": 0.878125
        },
        "0.01": {
          "tp": 3643,
          "fn": 1157,
          "accuracy": 0.7589583333333333
        }
      },
      "auroc": 0.9596087673611111
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4261,
          "fn": 539,
          "accuracy": 0.8877083333333333
        },
        "0.01": {
          "tp": 3776,
          "fn": 1024,
          "accuracy": 0.7866666666666666
        }
      },
      "auroc": 0.96240703125
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4666,
          "fn": 134,
          "accuracy": 0.9720833333333333
        },
        "0.01": {
          "tp": 4216,
          "fn": 584,
          "accuracy": 0.8783333333333333
        }
      },
      "auroc": 0.9893747395833333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8927,
          "fn": 673,
          "accuracy": 0.9298958333333334
        },
        "0.01": {
          "tp": 7992,
          "fn": 1608,
          "accuracy": 0.8325
        }
      },
      "auroc": 0.9758908854166667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2343,
          "fn": 57,
          "accuracy": 0.97625
        }
      },
      "auroc": 0.998170920138889
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2372,
          "fn": 28,
          "accuracy": 0.9883333333333333
        },
        "0.01": {
          "tp": 2183,
          "fn": 217,
          "accuracy": 0.9095833333333333
        }
      },
      "auroc": 0.9939409722222222
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4772,
          "fn": 28,
          "accuracy": 0.9941666666666666
        },
        "0.01": {
          "tp": 4526,
          "fn": 274,
          "accuracy": 0.9429166666666666
        }
      },
      "auroc": 0.9960559461805556
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2399,
          "fn": 1,
          "accuracy": 0.9995833333333334
        },
        "0.01": {
          "tp": 2325,
          "fn": 75,
          "accuracy": 0.96875
        }
      },
      "auroc": 0.9978440104166667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2340,
          "fn": 60,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 2113,
          "fn": 287,
          "accuracy": 0.8804166666666666
        }
      },
      "auroc": 0.98961953125
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4739,
          "fn": 61,
          "accuracy": 0.9872916666666667
        },
        "0.01": {
          "tp": 4438,
          "fn": 362,
          "accuracy": 0.9245833333333333
        }
      },
      "auroc": 0.9937317708333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4799,
          "fn": 1,
          "accuracy": 0.9997916666666666
        },
        "0.01": {
          "tp": 4668,
          "fn": 132,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9980074652777777
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4712,
          "fn": 88,
          "accuracy": 0.9816666666666667
        },
        "0.01": {
          "tp": 4296,
          "fn": 504,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9917802517361112
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9511,
          "fn": 89,
          "accuracy": 0.9907291666666667
        },
        "0.01": {
          "tp": 8964,
          "fn": 636,
          "accuracy": 0.93375
        }
      },
      "auroc": 0.9948938585069445
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2398,
          "fn": 2,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 2376,
          "fn": 24,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9985230902777777
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2399,
          "fn": 1,
          "accuracy": 0.9995833333333334
        },
        "0.01": {
          "tp": 2349,
          "fn": 51,
          "accuracy": 0.97875
        }
      },
      "auroc": 0.9982467013888889
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4797,
          "fn": 3,
          "accuracy": 0.999375
        },
        "0.01": {
          "tp": 4725,
          "fn": 75,
          "accuracy": 0.984375
        }
      },
      "auroc": 0.9983848958333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2253,
          "fn": 147,
          "accuracy": 0.93875
        },
        "0.01": {
          "tp": 1963,
          "fn": 437,
          "accuracy": 0.8179166666666666
        }
      },
      "auroc": 0.9824899305555554
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2319,
          "fn": 81,
          "accuracy": 0.96625
        },
        "0.01": {
          "tp": 2095,
          "fn": 305,
          "accuracy": 0.8729166666666667
        }
      },
      "auroc": 0.98876171875
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4572,
          "fn": 228,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 4058,
          "fn": 742,
          "accuracy": 0.8454166666666667
        }
      },
      "auroc": 0.9856258246527777
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4651,
          "fn": 149,
          "accuracy": 0.9689583333333334
        },
        "0.01": {
          "tp": 4339,
          "fn": 461,
          "accuracy": 0.9039583333333333
        }
      },
      "auroc": 0.9905065104166666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4718,
          "fn": 82,
          "accuracy": 0.9829166666666667
        },
        "0.01": {
          "tp": 4444,
          "fn": 356,
          "accuracy": 0.9258333333333333
        }
      },
      "auroc": 0.9935042100694444
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9369,
          "fn": 231,
          "accuracy": 0.9759375
        },
        "0.01": {
          "tp": 8783,
          "fn": 817,
          "accuracy": 0.9148958333333334
        }
      },
      "auroc": 0.9920053602430556
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2399,
          "fn": 1,
          "accuracy": 0.9995833333333334
        },
        "0.01": {
          "tp": 2391,
          "fn": 9,
          "accuracy": 0.99625
        }
      },
      "auroc": 0.9989457465277778
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2376,
          "fn": 24,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 2334,
          "fn": 66,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9956645833333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4775,
          "fn": 25,
          "accuracy": 0.9947916666666666
        },
        "0.01": {
          "tp": 4725,
          "fn": 75,
          "accuracy": 0.984375
        }
      },
      "auroc": 0.9973051649305555
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2080,
          "fn": 320,
          "accuracy": 0.8666666666666667
        },
        "0.01": {
          "tp": 1917,
          "fn": 483,
          "accuracy": 0.79875
        }
      },
      "auroc": 0.9539894097222222
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2231,
          "fn": 169,
          "accuracy": 0.9295833333333333
        },
        "0.01": {
          "tp": 2022,
          "fn": 378,
          "accuracy": 0.8425
        }
      },
      "auroc": 0.9768990451388888
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4311,
          "fn": 489,
          "accuracy": 0.898125
        },
        "0.01": {
          "tp": 3939,
          "fn": 861,
          "accuracy": 0.820625
        }
      },
      "auroc": 0.9654442274305556
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4479,
          "fn": 321,
          "accuracy": 0.933125
        },
        "0.01": {
          "tp": 4308,
          "fn": 492,
          "accuracy": 0.8975
        }
      },
      "auroc": 0.9764675781250001
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4607,
          "fn": 193,
          "accuracy": 0.9597916666666667
        },
        "0.01": {
          "tp": 4356,
          "fn": 444,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.986281814236111
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9086,
          "fn": 514,
          "accuracy": 0.9464583333333333
        },
        "0.01": {
          "tp": 8664,
          "fn": 936,
          "accuracy": 0.9025
        }
      },
      "auroc": 0.9813746961805555
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2394,
          "fn": 6,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 2302,
          "fn": 98,
          "accuracy": 0.9591666666666666
        }
      },
      "auroc": 0.9971084201388888
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2380,
          "fn": 20,
          "accuracy": 0.9916666666666667
        },
        "0.01": {
          "tp": 2182,
          "fn": 218,
          "accuracy": 0.9091666666666667
        }
      },
      "auroc": 0.9940610243055557
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4774,
          "fn": 26,
          "accuracy": 0.9945833333333334
        },
        "0.01": {
          "tp": 4484,
          "fn": 316,
          "accuracy": 0.9341666666666667
        }
      },
      "auroc": 0.9955847222222222
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2381,
          "fn": 19,
          "accuracy": 0.9920833333333333
        },
        "0.01": {
          "tp": 2251,
          "fn": 149,
          "accuracy": 0.9379166666666666
        }
      },
      "auroc": 0.9952772569444445
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2301,
          "fn": 99,
          "accuracy": 0.95875
        },
        "0.01": {
          "tp": 2047,
          "fn": 353,
          "accuracy": 0.8529166666666667
        }
      },
      "auroc": 0.9835877604166667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4682,
          "fn": 118,
          "accuracy": 0.9754166666666667
        },
        "0.01": {
          "tp": 4298,
          "fn": 502,
          "accuracy": 0.8954166666666666
        }
      },
      "auroc": 0.9894325086805555
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4775,
          "fn": 25,
          "accuracy": 0.9947916666666666
        },
        "0.01": {
          "tp": 4553,
          "fn": 247,
          "accuracy": 0.9485416666666666
        }
      },
      "auroc": 0.9961928385416667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4681,
          "fn": 119,
          "accuracy": 0.9752083333333333
        },
        "0.01": {
          "tp": 4229,
          "fn": 571,
          "accuracy": 0.8810416666666666
        }
      },
      "auroc": 0.9888243923611111
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9456,
          "fn": 144,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 8782,
          "fn": 818,
          "accuracy": 0.9147916666666667
        }
      },
      "auroc": 0.9925086154513889
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2357,
          "fn": 43,
          "accuracy": 0.9820833333333333
        },
        "0.01": {
          "tp": 2224,
          "fn": 176,
          "accuracy": 0.9266666666666666
        }
      },
      "auroc": 0.9938185763888889
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2357,
          "fn": 43,
          "accuracy": 0.9820833333333333
        },
        "0.01": {
          "tp": 2224,
          "fn": 176,
          "accuracy": 0.9266666666666666
        }
      },
      "auroc": 0.9938185763888889
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2323,
          "fn": 77,
          "accuracy": 0.9679166666666666
        },
        "0.01": {
          "tp": 2093,
          "fn": 307,
          "accuracy": 0.8720833333333333
        }
      },
      "auroc": 0.9892546006944445
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2323,
          "fn": 77,
          "accuracy": 0.9679166666666666
        },
        "0.01": {
          "tp": 2093,
          "fn": 307,
          "accuracy": 0.8720833333333333
        }
      },
      "auroc": 0.9892546006944445
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4680,
          "fn": 120,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 4317,
          "fn": 483,
          "accuracy": 0.899375
        }
      },
      "auroc": 0.9915365885416666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4680,
          "fn": 120,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 4317,
          "fn": 483,
          "accuracy": 0.899375
        }
      },
      "auroc": 0.9915365885416666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1811,
          "fn": 589,
          "accuracy": 0.7545833333333334
        },
        "0.01": {
          "tp": 1421,
          "fn": 979,
          "accuracy": 0.5920833333333333
        }
      },
      "auroc": 0.9235649305555556
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1811,
          "fn": 589,
          "accuracy": 0.7545833333333334
        },
        "0.01": {
          "tp": 1421,
          "fn": 979,
          "accuracy": 0.5920833333333333
        }
      },
      "auroc": 0.9235649305555556
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1700,
          "fn": 700,
          "accuracy": 0.7083333333333334
        },
        "0.01": {
          "tp": 1242,
          "fn": 1158,
          "accuracy": 0.5175
        }
      },
      "auroc": 0.9003611979166667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1700,
          "fn": 700,
          "accuracy": 0.7083333333333334
        },
        "0.01": {
          "tp": 1242,
          "fn": 1158,
          "accuracy": 0.5175
        }
      },
      "auroc": 0.9003611979166667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3511,
          "fn": 1289,
          "accuracy": 0.7314583333333333
        },
        "0.01": {
          "tp": 2663,
          "fn": 2137,
          "accuracy": 0.5547916666666667
        }
      },
      "auroc": 0.9119630642361112
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3511,
          "fn": 1289,
          "accuracy": 0.7314583333333333
        },
        "0.01": {
          "tp": 2663,
          "fn": 2137,
          "accuracy": 0.5547916666666667
        }
      },
      "auroc": 0.9119630642361112
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2396,
          "fn": 4,
          "accuracy": 0.9983333333333333
        },
        "0.01": {
          "tp": 2335,
          "fn": 65,
          "accuracy": 0.9729166666666667
        }
      },
      "auroc": 0.9977371527777777
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2396,
          "fn": 4,
          "accuracy": 0.9983333333333333
        },
        "0.01": {
          "tp": 2335,
          "fn": 65,
          "accuracy": 0.9729166666666667
        }
      },
      "auroc": 0.9977371527777777
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2390,
          "fn": 10,
          "accuracy": 0.9958333333333333
        },
        "0.01": {
          "tp": 2289,
          "fn": 111,
          "accuracy": 0.95375
        }
      },
      "auroc": 0.9967364583333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2390,
          "fn": 10,
          "accuracy": 0.9958333333333333
        },
        "0.01": {
          "tp": 2289,
          "fn": 111,
          "accuracy": 0.95375
        }
      },
      "auroc": 0.9967364583333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4786,
          "fn": 14,
          "accuracy": 0.9970833333333333
        },
        "0.01": {
          "tp": 4624,
          "fn": 176,
          "accuracy": 0.9633333333333334
        }
      },
      "auroc": 0.9972368055555556
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4786,
          "fn": 14,
          "accuracy": 0.9970833333333333
        },
        "0.01": {
          "tp": 4624,
          "fn": 176,
          "accuracy": 0.9633333333333334
        }
      },
      "auroc": 0.9972368055555556
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2399,
          "fn": 1,
          "accuracy": 0.9995833333333334
        },
        "0.01": {
          "tp": 2373,
          "fn": 27,
          "accuracy": 0.98875
        }
      },
      "auroc": 0.9985399305555556
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2399,
          "fn": 1,
          "accuracy": 0.9995833333333334
        },
        "0.01": {
          "tp": 2373,
          "fn": 27,
          "accuracy": 0.98875
        }
      },
      "auroc": 0.9985399305555556
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2370,
          "fn": 30,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 2201,
          "fn": 199,
          "accuracy": 0.9170833333333334
        }
      },
      "auroc": 0.9934723090277777
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2370,
          "fn": 30,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 2201,
          "fn": 199,
          "accuracy": 0.9170833333333334
        }
      },
      "auroc": 0.9934723090277777
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4769,
          "fn": 31,
          "accuracy": 0.9935416666666667
        },
        "0.01": {
          "tp": 4574,
          "fn": 226,
          "accuracy": 0.9529166666666666
        }
      },
      "auroc": 0.9960061197916665
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4769,
          "fn": 31,
          "accuracy": 0.9935416666666667
        },
        "0.01": {
          "tp": 4574,
          "fn": 226,
          "accuracy": 0.9529166666666666
        }
      },
      "auroc": 0.9960061197916665
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2325,
          "fn": 75,
          "accuracy": 0.96875
        },
        "0.01": {
          "tp": 2143,
          "fn": 257,
          "accuracy": 0.8929166666666667
        }
      },
      "auroc": 0.9883506944444445
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2325,
          "fn": 75,
          "accuracy": 0.96875
        },
        "0.01": {
          "tp": 2143,
          "fn": 257,
          "accuracy": 0.8929166666666667
        }
      },
      "auroc": 0.9883506944444445
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2231,
          "fn": 169,
          "accuracy": 0.9295833333333333
        },
        "0.01": {
          "tp": 2023,
          "fn": 377,
          "accuracy": 0.8429166666666666
        }
      },
      "auroc": 0.9755572048611112
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2231,
          "fn": 169,
          "accuracy": 0.9295833333333333
        },
        "0.01": {
          "tp": 2023,
          "fn": 377,
          "accuracy": 0.8429166666666666
        }
      },
      "auroc": 0.9755572048611112
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4556,
          "fn": 244,
          "accuracy": 0.9491666666666667
        },
        "0.01": {
          "tp": 4166,
          "fn": 634,
          "accuracy": 0.8679166666666667
        }
      },
      "auroc": 0.9819539496527777
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4556,
          "fn": 244,
          "accuracy": 0.9491666666666667
        },
        "0.01": {
          "tp": 4166,
          "fn": 634,
          "accuracy": 0.8679166666666667
        }
      },
      "auroc": 0.9819539496527777
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 25651,
          "fn": 749,
          "accuracy": 0.9716287878787879
        },
        "0.01": {
          "tp": 24449,
          "fn": 1951,
          "accuracy": 0.9260984848484849
        }
      },
      "auroc": 0.9897159248737374
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 14251,
          "fn": 149,
          "accuracy": 0.9896527777777778
        },
        "0.01": {
          "tp": 13434,
          "fn": 966,
          "accuracy": 0.9329166666666666
        }
      },
      "auroc": 0.9946155960648149
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 39902,
          "fn": 898,
          "accuracy": 0.9779901960784314
        },
        "0.01": {
          "tp": 37883,
          "fn": 2917,
          "accuracy": 0.9285049019607843
        }
      },
      "auroc": 0.9914452205882353
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 24406,
          "fn": 1994,
          "accuracy": 0.9244696969696969
        },
        "0.01": {
          "tp": 22128,
          "fn": 4272,
          "accuracy": 0.8381818181818181
        }
      },
      "auroc": 0.9737975220959596
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 13904,
          "fn": 496,
          "accuracy": 0.9655555555555555
        },
        "0.01": {
          "tp": 12621,
          "fn": 1779,
          "accuracy": 0.8764583333333333
        }
      },
      "auroc": 0.9872592158564814
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 38310,
          "fn": 2490,
          "accuracy": 0.9389705882352941
        },
        "0.01": {
          "tp": 34749,
          "fn": 6051,
          "accuracy": 0.8516911764705882
        }
      },
      "auroc": 0.9785487081290849
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 50057,
          "fn": 2743,
          "accuracy": 0.9480492424242424
        },
        "0.01": {
          "tp": 46577,
          "fn": 6223,
          "accuracy": 0.8821401515151515
        }
      },
      "auroc": 0.9817567234848485
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 28155,
          "fn": 645,
          "accuracy": 0.9776041666666667
        },
        "0.01": {
          "tp": 26055,
          "fn": 2745,
          "accuracy": 0.9046875
        }
      },
      "auroc": 0.9909374059606482
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 78212,
          "fn": 3388,
          "accuracy": 0.9584803921568628
        },
        "0.01": {
          "tp": 72632,
          "fn": 8968,
          "accuracy": 0.8900980392156863
        }
      },
      "auroc": 0.9849969643586602
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9910833333333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911666666666668
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9389562499999999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9908052083333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": null
      },
      "auroc": 0.9648807291666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.965103125
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9909442708333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 753,
          "fn": 47,
          "accuracy": 0.94125
        },
        "0.01": null
      },
      "auroc": 0.9780236979166668
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9910833333333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911666666666668
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9885604166666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9899052083333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.989821875
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9905359375
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9876395833333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9902739583333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9889567708333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9894447916666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9907619791666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9901033854166666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.94515
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9790687500000002
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.962109375
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9682
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9851593750000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 758,
          "fn": 42,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9766796874999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912031250000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912031250000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9894729166666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9894729166666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9903380208333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9903380208333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9773520833333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9773520833333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.942696875
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.942696875
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": null
      },
      "auroc": 0.9600244791666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": null
      },
      "auroc": 0.9600244791666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.990775
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.990775
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9851427083333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9851427083333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9879588541666666
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9879588541666666
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2187,
          "fn": 13,
          "accuracy": 0.9940909090909091
        },
        "0.01": null
      },
      "auroc": 0.9899391098484849
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911944444444445
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3387,
          "fn": 13,
          "accuracy": 0.9961764705882353
        },
        "0.01": null
      },
      "auroc": 0.9903821691176471
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2068,
          "fn": 132,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9768462121212121
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1187,
          "fn": 13,
          "accuracy": 0.9891666666666666
        },
        "0.01": null
      },
      "auroc": 0.9885347222222222
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3255,
          "fn": 145,
          "accuracy": 0.9573529411764706
        },
        "0.01": null
      },
      "auroc": 0.9809715686274509
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 4255,
          "fn": 145,
          "accuracy": 0.9670454545454545
        },
        "0.01": null
      },
      "auroc": 0.9833926609848485
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2387,
          "fn": 13,
          "accuracy": 0.9945833333333334
        },
        "0.01": null
      },
      "auroc": 0.9898645833333334
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 6642,
          "fn": 158,
          "accuracy": 0.976764705882353
        },
        "0.01": null
      },
      "auroc": 0.985676868872549
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9910833333333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911666666666668
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9389562499999999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9908052083333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": null
      },
      "auroc": 0.9648807291666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.965103125
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9909442708333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 753,
          "fn": 47,
          "accuracy": 0.94125
        },
        "0.01": null
      },
      "auroc": 0.9780236979166668
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9910833333333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911666666666668
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9885604166666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9899052083333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.989821875
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9905359375
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9876395833333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9902739583333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9889567708333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9894447916666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9907619791666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9901033854166666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.94515
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9790687500000002
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.962109375
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9682
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9851593750000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 758,
          "fn": 42,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9766796874999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912031250000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912031250000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9894729166666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9894729166666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9903380208333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9903380208333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9773520833333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9773520833333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.942696875
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.942696875
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": null
      },
      "auroc": 0.9600244791666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": null
      },
      "auroc": 0.9600244791666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.990775
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.990775
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9851427083333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9851427083333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9879588541666666
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9879588541666666
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2187,
          "fn": 13,
          "accuracy": 0.9940909090909091
        },
        "0.01": null
      },
      "auroc": 0.9899391098484849
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911944444444445
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3387,
          "fn": 13,
          "accuracy": 0.9961764705882353
        },
        "0.01": null
      },
      "auroc": 0.9903821691176471
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2068,
          "fn": 132,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9768462121212121
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1187,
          "fn": 13,
          "accuracy": 0.9891666666666666
        },
        "0.01": null
      },
      "auroc": 0.9885347222222222
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3255,
          "fn": 145,
          "accuracy": 0.9573529411764706
        },
        "0.01": null
      },
      "auroc": 0.9809715686274509
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 4255,
          "fn": 145,
          "accuracy": 0.9670454545454545
        },
        "0.01": null
      },
      "auroc": 0.9833926609848485
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2387,
          "fn": 13,
          "accuracy": 0.9945833333333334
        },
        "0.01": null
      },
      "auroc": 0.9898645833333334
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 6642,
          "fn": 158,
          "accuracy": 0.976764705882353
        },
        "0.01": null
      },
      "auroc": 0.985676868872549
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912031250000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912265625
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912265625
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912382812499999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9910364583333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911432291666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.952659375
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.990784375
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9717218750000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": null
      },
      "auroc": 0.9719546875
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9909104166666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 768,
          "fn": 32,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9814325520833334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9910833333333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911666666666668
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.988115625
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9896828125000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9895994791666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.9904247395833333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9889479166666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.990734375
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9898411458333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9900989583333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9909921875000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.9905455729166667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9550416666666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9815458333333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9682937500000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9731458333333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9863979166666668
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 768,
          "fn": 32,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.979771875
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912031250000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912031250000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.990128125
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.990128125
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.990665625
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.990665625
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9784541666666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9784541666666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.9444625
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.9444625
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9614583333333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9614583333333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9907114583333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9907114583333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9857572916666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9857572916666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.988234375
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.988234375
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2184,
          "fn": 16,
          "accuracy": 0.9927272727272727
        },
        "0.01": null
      },
      "auroc": 0.9900335227272727
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911866319444445
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3384,
          "fn": 16,
          "accuracy": 0.9952941176470588
        },
        "0.01": null
      },
      "auroc": 0.9904405024509804
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2097,
          "fn": 103,
          "accuracy": 0.9531818181818181
        },
        "0.01": null
      },
      "auroc": 0.9793860795454546
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1189,
          "fn": 11,
          "accuracy": 0.9908333333333333
        },
        "0.01": null
      },
      "auroc": 0.9889388888888888
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3286,
          "fn": 114,
          "accuracy": 0.9664705882352941
        },
        "0.01": null
      },
      "auroc": 0.9827576593137255
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 4281,
          "fn": 119,
          "accuracy": 0.9729545454545454
        },
        "0.01": null
      },
      "auroc": 0.9847098011363636
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2389,
          "fn": 11,
          "accuracy": 0.9954166666666666
        },
        "0.01": null
      },
      "auroc": 0.9900627604166667
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 6670,
          "fn": 130,
          "accuracy": 0.9808823529411764
        },
        "0.01": null
      },
      "auroc": 0.9865990808823529
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912031250000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912265625
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912265625
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912382812499999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9910364583333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911432291666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.9476822916666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9908052083333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.96924375
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": null
      },
      "auroc": 0.9694661458333332
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9909208333333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 760,
          "fn": 40,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9801934895833333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9909187500000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.991084375
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9877479166666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9894989583333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9893333333333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.9902916666666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.986765625
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9895572916666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9881614583333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9890078125
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9904036458333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9897057291666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": null
      },
      "auroc": 0.948053125
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9845427083333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": null
      },
      "auroc": 0.9662979166666668
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9696515625
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9878963541666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 765,
          "fn": 35,
          "accuracy": 0.95625
        },
        "0.01": null
      },
      "auroc": 0.9787739583333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9909135416666666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9909135416666666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9866822916666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9866822916666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9887979166666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9887979166666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9580229166666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9580229166666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": null
      },
      "auroc": 0.9089604166666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": null
      },
      "auroc": 0.9089604166666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.9334916666666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.9334916666666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912031250000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912031250000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912265625
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912265625
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.98741875
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.98741875
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9773895833333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9773895833333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9824041666666666
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9824041666666666
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2164,
          "fn": 36,
          "accuracy": 0.9836363636363636
        },
        "0.01": null
      },
      "auroc": 0.9878504734848484
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911592013888889
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3364,
          "fn": 36,
          "accuracy": 0.9894117647058823
        },
        "0.01": null
      },
      "auroc": 0.9890182598039216
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2053,
          "fn": 147,
          "accuracy": 0.9331818181818182
        },
        "0.01": null
      },
      "auroc": 0.9737942234848485
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1189,
          "fn": 11,
          "accuracy": 0.9908333333333333
        },
        "0.01": null
      },
      "auroc": 0.989184375
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3242,
          "fn": 158,
          "accuracy": 0.9535294117647058
        },
        "0.01": null
      },
      "auroc": 0.9792260416666667
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 4217,
          "fn": 183,
          "accuracy": 0.9584090909090909
        },
        "0.01": null
      },
      "auroc": 0.9808223484848484
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2389,
          "fn": 11,
          "accuracy": 0.9954166666666666
        },
        "0.01": null
      },
      "auroc": 0.9901717881944444
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 6606,
          "fn": 194,
          "accuracy": 0.9714705882352941
        },
        "0.01": null
      },
      "auroc": 0.9841221507352941
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9910364583333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911432291666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.9353364583333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9908052083333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": null
      },
      "auroc": 0.9630708333333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9632932291666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9909208333333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 751,
          "fn": 49,
          "accuracy": 0.93875
        },
        "0.01": null
      },
      "auroc": 0.9771070312500001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.991115625
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911828125
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9879895833333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9896197916666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9895526041666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9904013020833333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9875958333333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9903510416666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9889734375
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9894229166666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9908005208333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9901117187499999
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9456260416666666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9790500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": null
      },
      "auroc": 0.9623380208333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9684380208333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.98515
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 757,
          "fn": 43,
          "accuracy": 0.94625
        },
        "0.01": null
      },
      "auroc": 0.9767940104166667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912031250000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912031250000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9893625
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9893625
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9902828125
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9902828125
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9772489583333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9772489583333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.9406010416666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.9406010416666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": null
      },
      "auroc": 0.958925
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": null
      },
      "auroc": 0.958925
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9906708333333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9906708333333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9849885416666666
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9849885416666666
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9878296875
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9878296875
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2190,
          "fn": 10,
          "accuracy": 0.9954545454545455
        },
        "0.01": null
      },
      "auroc": 0.9899202651515152
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911920138888889
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3390,
          "fn": 10,
          "accuracy": 0.9970588235294118
        },
        "0.01": null
      },
      "auroc": 0.9903691176470589
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2071,
          "fn": 129,
          "accuracy": 0.9413636363636364
        },
        "0.01": null
      },
      "auroc": 0.9763418560606061
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1186,
          "fn": 14,
          "accuracy": 0.9883333333333333
        },
        "0.01": null
      },
      "auroc": 0.9884493055555555
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3257,
          "fn": 143,
          "accuracy": 0.9579411764705882
        },
        "0.01": null
      },
      "auroc": 0.9806150735294118
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 4261,
          "fn": 139,
          "accuracy": 0.9684090909090909
        },
        "0.01": null
      },
      "auroc": 0.9831310606060607
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2386,
          "fn": 14,
          "accuracy": 0.9941666666666666
        },
        "0.01": null
      },
      "auroc": 0.9898206597222222
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 6647,
          "fn": 153,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9854920955882354
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9910895833333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.98719375
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9891416666666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.99075
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.983271875
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9870109375
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9909197916666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9852328125
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 785,
          "fn": 15,
          "accuracy": 0.98125
        },
        "0.01": null
      },
      "auroc": 0.9880763020833333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.989690625
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9768052083333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": null
      },
      "auroc": 0.9832479166666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": null
      },
      "auroc": 0.8956125
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9654770833333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 289,
          "fn": 111,
          "accuracy": 0.7225
        },
        "0.01": null
      },
      "auroc": 0.9305447916666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.9426515625
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9711411458333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 662,
          "fn": 138,
          "accuracy": 0.8275
        },
        "0.01": null
      },
      "auroc": 0.9568963541666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9889645833333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9873083333333332
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9881364583333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9876208333333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9587083333333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": null
      },
      "auroc": 0.9731645833333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9882927083333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9730083333333335
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 752,
          "fn": 48,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9806505208333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9906312500000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9906510416666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9906411458333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9753229166666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9704770833333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9729
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.9829770833333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9805640624999999
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 764,
          "fn": 36,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9817705729166667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.99044375
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9797322916666666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9850880208333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": null
      },
      "auroc": 0.8916156249999999
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": null
      },
      "auroc": 0.9009875
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        },
        "0.01": null
      },
      "auroc": 0.8963015625
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.9410296875
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9403598958333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 620,
          "fn": 180,
          "accuracy": 0.775
        },
        "0.01": null
      },
      "auroc": 0.9406947916666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911822916666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.990859375
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9910208333333332
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9904437500000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9879333333333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9891885416666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9908130208333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9893963541666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 793,
          "fn": 7,
          "accuracy": 0.99125
        },
        "0.01": null
      },
      "auroc": 0.9901046874999999
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9883343750000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9883343750000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9794708333333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9794708333333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9839026041666666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9839026041666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": null
      },
      "auroc": 0.9160135416666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": null
      },
      "auroc": 0.9160135416666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": null
      },
      "auroc": 0.8576375
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": null
      },
      "auroc": 0.8576375
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.8868255208333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.8868255208333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9906770833333334
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9906770833333334
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9906718750000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9906718750000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9906744791666666
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9906744791666666
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9874875000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9874875000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9815052083333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9815052083333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9844963541666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9844963541666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9761229166666666
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9761229166666666
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9575083333333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9575083333333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": null
      },
      "auroc": 0.966815625
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": null
      },
      "auroc": 0.966815625
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2119,
          "fn": 81,
          "accuracy": 0.9631818181818181
        },
        "0.01": null
      },
      "auroc": 0.9818761363636364
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1154,
          "fn": 46,
          "accuracy": 0.9616666666666667
        },
        "0.01": null
      },
      "auroc": 0.985425
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3273,
          "fn": 127,
          "accuracy": 0.9626470588235294
        },
        "0.01": null
      },
      "auroc": 0.9831286764705883
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1875,
          "fn": 325,
          "accuracy": 0.8522727272727273
        },
        "0.01": null
      },
      "auroc": 0.9543781249999999
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1032,
          "fn": 168,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9611425347222222
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2907,
          "fn": 493,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9567655637254902
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3994,
          "fn": 406,
          "accuracy": 0.9077272727272727
        },
        "0.01": null
      },
      "auroc": 0.9681271306818182
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2186,
          "fn": 214,
          "accuracy": 0.9108333333333334
        },
        "0.01": null
      },
      "auroc": 0.9732837673611111
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 6180,
          "fn": 620,
          "accuracy": 0.9088235294117647
        },
        "0.01": null
      },
      "auroc": 0.9699471200980392
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9910833333333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911666666666668
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.9394541666666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9906239583333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9650390624999999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": null
      },
      "auroc": 0.9653520833333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9908536458333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 752,
          "fn": 48,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9781028645833334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9910833333333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911666666666668
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9885729166666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9899114583333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9898281250000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.9905390625
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.988521875
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9903208333333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9894213541666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9898859375
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9907854166666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9903356770833333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.9449645833333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.979165625
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": null
      },
      "auroc": 0.9620651041666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": null
      },
      "auroc": 0.9681072916666666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9852078125000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 759,
          "fn": 41,
          "accuracy": 0.94875
        },
        "0.01": null
      },
      "auroc": 0.9766575520833333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912031250000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912031250000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9894854166666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9894854166666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9903442708333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9903442708333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9774489583333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9774489583333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.940846875
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.940846875
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9591479166666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9591479166666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.990775
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.990775
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9854708333333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9854708333333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9881229166666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9881229166666667
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2188,
          "fn": 12,
          "accuracy": 0.9945454545454545
        },
        "0.01": null
      },
      "auroc": 0.9899479166666667
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911944444444445
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3388,
          "fn": 12,
          "accuracy": 0.9964705882352941
        },
        "0.01": null
      },
      "auroc": 0.9903878676470589
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2070,
          "fn": 130,
          "accuracy": 0.9409090909090909
        },
        "0.01": null
      },
      "auroc": 0.9768176136363635
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1188,
          "fn": 12,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9885305555555555
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3258,
          "fn": 142,
          "accuracy": 0.9582352941176471
        },
        "0.01": null
      },
      "auroc": 0.980951593137255
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 4258,
          "fn": 142,
          "accuracy": 0.9677272727272728
        },
        "0.01": null
      },
      "auroc": 0.9833827651515152
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2388,
          "fn": 12,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9898625
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 6646,
          "fn": 154,
          "accuracy": 0.9773529411764705
        },
        "0.01": null
      },
      "auroc": 0.9856697303921569
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9910833333333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911666666666668
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9389020833333332
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9908052083333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": null
      },
      "auroc": 0.9648536458333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9650760416666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9909442708333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 753,
          "fn": 47,
          "accuracy": 0.94125
        },
        "0.01": null
      },
      "auroc": 0.9780101562500001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9910833333333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911666666666668
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9885604166666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9899052083333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.989821875
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9905359375
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9876395833333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9902739583333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9889567708333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9894447916666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9907619791666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9901033854166666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.94515
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9790687500000002
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.962109375
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9682
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9851593750000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 758,
          "fn": 42,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9766796874999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912031250000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912031250000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9894729166666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9894729166666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9903380208333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9903380208333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9773520833333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9773520833333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.942696875
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.942696875
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": null
      },
      "auroc": 0.9600244791666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": null
      },
      "auroc": 0.9600244791666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.990775
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.990775
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9851427083333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9851427083333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9879588541666666
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9879588541666666
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2187,
          "fn": 13,
          "accuracy": 0.9940909090909091
        },
        "0.01": null
      },
      "auroc": 0.9899391098484849
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911944444444445
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3387,
          "fn": 13,
          "accuracy": 0.9961764705882353
        },
        "0.01": null
      },
      "auroc": 0.9903821691176471
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2068,
          "fn": 132,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9768412878787879
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1187,
          "fn": 13,
          "accuracy": 0.9891666666666666
        },
        "0.01": null
      },
      "auroc": 0.9885347222222222
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3255,
          "fn": 145,
          "accuracy": 0.9573529411764706
        },
        "0.01": null
      },
      "auroc": 0.9809683823529411
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 4255,
          "fn": 145,
          "accuracy": 0.9670454545454545
        },
        "0.01": null
      },
      "auroc": 0.9833901988636363
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2387,
          "fn": 13,
          "accuracy": 0.9945833333333334
        },
        "0.01": null
      },
      "auroc": 0.9898645833333334
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 6642,
          "fn": 158,
          "accuracy": 0.976764705882353
        },
        "0.01": null
      },
      "auroc": 0.9856752757352941
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9850375
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9806395833333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9828385416666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.98321875
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9758979166666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": null
      },
      "auroc": 0.9795583333333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.984128125
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": null
      },
      "auroc": 0.97826875
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 745,
          "fn": 55,
          "accuracy": 0.93125
        },
        "0.01": null
      },
      "auroc": 0.9811984375
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.986134375
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9874708333333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9868026041666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": null
      },
      "auroc": 0.9109375
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9810625
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 287,
          "fn": 113,
          "accuracy": 0.7175
        },
        "0.01": null
      },
      "auroc": 0.946
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        },
        "0.01": null
      },
      "auroc": 0.9485359375
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9842666666666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 677,
          "fn": 123,
          "accuracy": 0.84625
        },
        "0.01": null
      },
      "auroc": 0.9664013020833333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.98569375
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": null
      },
      "auroc": 0.9651479166666668
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9754208333333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9800375
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": null
      },
      "auroc": 0.9413354166666668
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.9606864583333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.982865625
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 280,
          "fn": 120,
          "accuracy": 0.7
        },
        "0.01": null
      },
      "auroc": 0.9532416666666668
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 658,
          "fn": 142,
          "accuracy": 0.8225
        },
        "0.01": null
      },
      "auroc": 0.9680536458333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.99064375
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9886739583333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9896588541666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": null
      },
      "auroc": 0.9215135416666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": null
      },
      "auroc": 0.9424770833333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        },
        "0.01": null
      },
      "auroc": 0.9319953125
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": null
      },
      "auroc": 0.9560786458333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        },
        "0.01": null
      },
      "auroc": 0.9655755208333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 622,
          "fn": 178,
          "accuracy": 0.7775
        },
        "0.01": null
      },
      "auroc": 0.9608270833333332
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": null
      },
      "auroc": 0.931178125
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": null
      },
      "auroc": 0.9452177083333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        },
        "0.01": null
      },
      "auroc": 0.9381979166666666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        },
        "0.01": null
      },
      "auroc": 0.9612140625000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        },
        "0.01": null
      },
      "auroc": 0.9682338541666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 658,
          "fn": 142,
          "accuracy": 0.8225
        },
        "0.01": null
      },
      "auroc": 0.9647239583333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9883864583333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9819322916666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9851593750000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9829427083333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.96633125
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": null
      },
      "auroc": 0.9746369791666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9856645833333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": null
      },
      "auroc": 0.9741317708333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 732,
          "fn": 68,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9798981770833334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9830958333333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9830958333333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.9670385416666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.9670385416666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": null
      },
      "auroc": 0.9750671875
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": null
      },
      "auroc": 0.9750671875
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        },
        "0.01": null
      },
      "auroc": 0.9267979166666668
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        },
        "0.01": null
      },
      "auroc": 0.9267979166666668
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        },
        "0.01": null
      },
      "auroc": 0.9064458333333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        },
        "0.01": null
      },
      "auroc": 0.9064458333333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        },
        "0.01": null
      },
      "auroc": 0.916621875
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        },
        "0.01": null
      },
      "auroc": 0.916621875
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.987259375
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.987259375
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9785020833333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9785020833333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9828807291666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9828807291666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9909239583333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9909239583333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9799135416666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9799135416666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.98541875
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.98541875
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.9636208333333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.9636208333333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.953078125
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.953078125
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.9583494791666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.9583494791666667
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2041,
          "fn": 159,
          "accuracy": 0.9277272727272727
        },
        "0.01": null
      },
      "auroc": 0.9798948863636363
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1122,
          "fn": 78,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9825190972222222
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3163,
          "fn": 237,
          "accuracy": 0.9302941176470588
        },
        "0.01": null
      },
      "auroc": 0.9808210784313726
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1663,
          "fn": 537,
          "accuracy": 0.7559090909090909
        },
        "0.01": null
      },
      "auroc": 0.9540732954545456
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 896,
          "fn": 304,
          "accuracy": 0.7466666666666667
        },
        "0.01": null
      },
      "auroc": 0.9587203125
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2559,
          "fn": 841,
          "accuracy": 0.7526470588235294
        },
        "0.01": null
      },
      "auroc": 0.9557134191176471
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3704,
          "fn": 696,
          "accuracy": 0.8418181818181818
        },
        "0.01": null
      },
      "auroc": 0.9669840909090909
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2018,
          "fn": 382,
          "accuracy": 0.8408333333333333
        },
        "0.01": null
      },
      "auroc": 0.9706197048611112
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5722,
          "fn": 1078,
          "accuracy": 0.8414705882352941
        },
        "0.01": null
      },
      "auroc": 0.9682672487745098
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9910833333333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911666666666668
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": null
      },
      "auroc": 0.9477822916666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9908052083333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": null
      },
      "auroc": 0.9692937500000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9695161458333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9909442708333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 761,
          "fn": 39,
          "accuracy": 0.95125
        },
        "0.01": null
      },
      "auroc": 0.9802302083333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9910833333333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911666666666668
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9883166666666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9897833333333335
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9897
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.990475
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9879291666666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.99028125
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9891052083333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9895895833333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.990765625
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9901776041666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9503458333333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.98238125
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9663635416666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.9707979166666668
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.986815625
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 766,
          "fn": 34,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9788067708333335
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911572916666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911572916666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9890114583333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9890114583333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.990084375
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.990084375
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.97455
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.97455
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9394822916666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9394822916666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": null
      },
      "auroc": 0.9570161458333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": null
      },
      "auroc": 0.9570161458333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.990821875
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.990821875
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.983978125
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.983978125
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9873999999999999
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9873999999999999
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2187,
          "fn": 13,
          "accuracy": 0.9940909090909091
        },
        "0.01": null
      },
      "auroc": 0.9896844696969697
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911944444444445
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3387,
          "fn": 13,
          "accuracy": 0.9961764705882353
        },
        "0.01": null
      },
      "auroc": 0.9902174019607843
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2086,
          "fn": 114,
          "accuracy": 0.9481818181818182
        },
        "0.01": null
      },
      "auroc": 0.977707196969697
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1190,
          "fn": 10,
          "accuracy": 0.9916666666666667
        },
        "0.01": null
      },
      "auroc": 0.9890473958333333
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3276,
          "fn": 124,
          "accuracy": 0.9635294117647059
        },
        "0.01": null
      },
      "auroc": 0.9817096200980393
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 4273,
          "fn": 127,
          "accuracy": 0.9711363636363637
        },
        "0.01": null
      },
      "auroc": 0.9836958333333332
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2390,
          "fn": 10,
          "accuracy": 0.9958333333333333
        },
        "0.01": null
      },
      "auroc": 0.9901209201388889
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 6663,
          "fn": 137,
          "accuracy": 0.9798529411764706
        },
        "0.01": null
      },
      "auroc": 0.9859635110294118
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9910833333333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911666666666668
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.9367770833333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9909250000000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9638510416666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9640135416666665
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9910041666666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 752,
          "fn": 48,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9775088541666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9910833333333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911666666666668
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9882864583333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9897682291666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9896848958333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9904674479166666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9875635416666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9900302083333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.988796875
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9894067708333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9906401041666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9900234375
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9447197916666665
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9791791666666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9619494791666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9679848958333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9852145833333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 758,
          "fn": 42,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9765997395833335
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912031250000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912031250000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9893791666666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9893791666666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9902911458333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9902911458333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9772875
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9772875
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.9442062499999999
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.9442062499999999
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": null
      },
      "auroc": 0.960746875
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": null
      },
      "auroc": 0.960746875
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.99066875
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.99066875
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9841677083333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9841677083333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9874182291666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9874182291666667
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2187,
          "fn": 13,
          "accuracy": 0.9940909090909091
        },
        "0.01": null
      },
      "auroc": 0.9899235795454544
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911944444444445
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3387,
          "fn": 13,
          "accuracy": 0.9961764705882353
        },
        "0.01": null
      },
      "auroc": 0.9903721200980392
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2066,
          "fn": 134,
          "accuracy": 0.9390909090909091
        },
        "0.01": null
      },
      "auroc": 0.9766421401515153
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1188,
          "fn": 12,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9884868055555555
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3254,
          "fn": 146,
          "accuracy": 0.9570588235294117
        },
        "0.01": null
      },
      "auroc": 0.9808226102941177
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 4253,
          "fn": 147,
          "accuracy": 0.9665909090909091
        },
        "0.01": null
      },
      "auroc": 0.9832828598484848
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2388,
          "fn": 12,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.989840625
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 6641,
          "fn": 159,
          "accuracy": 0.9766176470588235
        },
        "0.01": null
      },
      "auroc": 0.9855973651960784
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": null
      },
      "auroc": 0.9691270833333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.9607645833333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        },
        "0.01": null
      },
      "auroc": 0.9649458333333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.9638729166666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": null
      },
      "auroc": 0.9522677083333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        },
        "0.01": null
      },
      "auroc": 0.9580703125000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        },
        "0.01": null
      },
      "auroc": 0.9664999999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 271,
          "fn": 129,
          "accuracy": 0.6775
        },
        "0.01": null
      },
      "auroc": 0.9565161458333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 587,
          "fn": 213,
          "accuracy": 0.73375
        },
        "0.01": null
      },
      "auroc": 0.9615080729166667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9763520833333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.97168125
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": null
      },
      "auroc": 0.9740166666666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": null
      },
      "auroc": 0.9149083333333332
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": null
      },
      "auroc": 0.9531572916666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        },
        "0.01": null
      },
      "auroc": 0.9340328124999999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 255,
          "fn": 145,
          "accuracy": 0.6375
        },
        "0.01": null
      },
      "auroc": 0.9456302083333332
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 287,
          "fn": 113,
          "accuracy": 0.7175
        },
        "0.01": null
      },
      "auroc": 0.9624192708333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 542,
          "fn": 258,
          "accuracy": 0.6775
        },
        "0.01": null
      },
      "auroc": 0.9540247395833334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.9730010416666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.9452510416666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        },
        "0.01": null
      },
      "auroc": 0.9591260416666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.9681635416666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        },
        "0.01": null
      },
      "auroc": 0.9286208333333335
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        },
        "0.01": null
      },
      "auroc": 0.9483921875000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.9705822916666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 226,
          "accuracy": 0.435
        },
        "0.01": null
      },
      "auroc": 0.9369359375000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 496,
          "fn": 304,
          "accuracy": 0.62
        },
        "0.01": null
      },
      "auroc": 0.9537591145833333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9865708333333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9792708333333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9829208333333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        },
        "0.01": null
      },
      "auroc": 0.923084375
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        },
        "0.01": null
      },
      "auroc": 0.9279895833333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 240,
          "accuracy": 0.4
        },
        "0.01": null
      },
      "auroc": 0.9255369791666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 278,
          "fn": 122,
          "accuracy": 0.695
        },
        "0.01": null
      },
      "auroc": 0.9548276041666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        },
        "0.01": null
      },
      "auroc": 0.9536302083333335
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 540,
          "fn": 260,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.95422890625
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9907906249999999
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9912500000000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9910203125
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.9383114583333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        },
        "0.01": null
      },
      "auroc": 0.9384479166666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 229,
          "fn": 171,
          "accuracy": 0.5725
        },
        "0.01": null
      },
      "auroc": 0.9383796875000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": null
      },
      "auroc": 0.9645510416666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9648489583333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 629,
          "fn": 171,
          "accuracy": 0.78625
        },
        "0.01": null
      },
      "auroc": 0.9647000000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9756
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": null
      },
      "auroc": 0.9548166666666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        },
        "0.01": null
      },
      "auroc": 0.9652083333333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": null
      },
      "auroc": 0.9651260416666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        },
        "0.01": null
      },
      "auroc": 0.9369791666666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 242,
          "fn": 158,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.9510526041666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.9703630208333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        },
        "0.01": null
      },
      "auroc": 0.9458979166666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 545,
          "fn": 255,
          "accuracy": 0.68125
        },
        "0.01": null
      },
      "auroc": 0.9581304687500001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.9693
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.9693
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": null
      },
      "auroc": 0.9532937499999999
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": null
      },
      "auroc": 0.9532937499999999
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        },
        "0.01": null
      },
      "auroc": 0.9612968749999999
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        },
        "0.01": null
      },
      "auroc": 0.9612968749999999
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": null
      },
      "auroc": 0.9322708333333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": null
      },
      "auroc": 0.9322708333333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": null
      },
      "auroc": 0.9142927083333332
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": null
      },
      "auroc": 0.9142927083333332
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 222,
          "accuracy": 0.445
        },
        "0.01": null
      },
      "auroc": 0.9232817708333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 222,
          "accuracy": 0.445
        },
        "0.01": null
      },
      "auroc": 0.9232817708333334
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9784208333333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9784208333333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": null
      },
      "auroc": 0.9620989583333334
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": null
      },
      "auroc": 0.9620989583333334
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": null
      },
      "auroc": 0.9702598958333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": null
      },
      "auroc": 0.9702598958333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9896583333333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9896583333333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9716364583333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9716364583333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9806473958333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9806473958333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": null
      },
      "auroc": 0.9547208333333335
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": null
      },
      "auroc": 0.9547208333333335
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": null
      },
      "auroc": 0.9419593749999999
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": null
      },
      "auroc": 0.9419593749999999
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        },
        "0.01": null
      },
      "auroc": 0.9483401041666666
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        },
        "0.01": null
      },
      "auroc": 0.9483401041666666
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1841,
          "fn": 359,
          "accuracy": 0.8368181818181818
        },
        "0.01": null
      },
      "auroc": 0.9723465909090909
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 918,
          "fn": 282,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.9671723958333334
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2759,
          "fn": 641,
          "accuracy": 0.8114705882352942
        },
        "0.01": null
      },
      "auroc": 0.9705204044117648
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1395,
          "fn": 805,
          "accuracy": 0.634090909090909
        },
        "0.01": null
      },
      "auroc": 0.9469770833333333
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 599,
          "fn": 601,
          "accuracy": 0.49916666666666665
        },
        "0.01": null
      },
      "auroc": 0.9395770833333333
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1994,
          "fn": 1406,
          "accuracy": 0.5864705882352941
        },
        "0.01": null
      },
      "auroc": 0.944365318627451
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3236,
          "fn": 1164,
          "accuracy": 0.7354545454545455
        },
        "0.01": null
      },
      "auroc": 0.9596618371212121
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1517,
          "fn": 883,
          "accuracy": 0.6320833333333333
        },
        "0.01": null
      },
      "auroc": 0.9533747395833334
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4753,
          "fn": 2047,
          "accuracy": 0.6989705882352941
        },
        "0.01": null
      },
      "auroc": 0.9574428615196078
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2353,
          "fn": 47,
          "accuracy": 0.9804166666666667
        },
        "0.01": null
      },
      "auroc": 0.9888753472222223
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2320,
          "fn": 80,
          "accuracy": 0.9666666666666667
        },
        "0.01": null
      },
      "auroc": 0.9874873263888888
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4673,
          "fn": 127,
          "accuracy": 0.9735416666666666
        },
        "0.01": null
      },
      "auroc": 0.9881813368055556
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2346,
          "fn": 54,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9882576388888888
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2298,
          "fn": 102,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9860494791666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4644,
          "fn": 156,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9871535590277778
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4699,
          "fn": 101,
          "accuracy": 0.9789583333333334
        },
        "0.01": null
      },
      "auroc": 0.9885664930555556
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4618,
          "fn": 182,
          "accuracy": 0.9620833333333333
        },
        "0.01": null
      },
      "auroc": 0.9867684027777777
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9317,
          "fn": 283,
          "accuracy": 0.9705208333333334
        },
        "0.01": null
      },
      "auroc": 0.9876674479166667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2367,
          "fn": 33,
          "accuracy": 0.98625
        },
        "0.01": null
      },
      "auroc": 0.9894522569444445
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2335,
          "fn": 65,
          "accuracy": 0.9729166666666667
        },
        "0.01": null
      },
      "auroc": 0.9879638888888889
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4702,
          "fn": 98,
          "accuracy": 0.9795833333333334
        },
        "0.01": null
      },
      "auroc": 0.9887080729166666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1698,
          "fn": 702,
          "accuracy": 0.7075
        },
        "0.01": null
      },
      "auroc": 0.9331637152777777
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2284,
          "fn": 116,
          "accuracy": 0.9516666666666667
        },
        "0.01": null
      },
      "auroc": 0.9847384548611111
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3982,
          "fn": 818,
          "accuracy": 0.8295833333333333
        },
        "0.01": null
      },
      "auroc": 0.9589510850694445
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4065,
          "fn": 735,
          "accuracy": 0.846875
        },
        "0.01": null
      },
      "auroc": 0.9613079861111111
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4619,
          "fn": 181,
          "accuracy": 0.9622916666666667
        },
        "0.01": null
      },
      "auroc": 0.986351171875
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8684,
          "fn": 916,
          "accuracy": 0.9045833333333333
        },
        "0.01": null
      },
      "auroc": 0.9738295789930556
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2357,
          "fn": 43,
          "accuracy": 0.9820833333333333
        },
        "0.01": null
      },
      "auroc": 0.98907578125
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2252,
          "fn": 148,
          "accuracy": 0.9383333333333334
        },
        "0.01": null
      },
      "auroc": 0.9847770833333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4609,
          "fn": 191,
          "accuracy": 0.9602083333333333
        },
        "0.01": null
      },
      "auroc": 0.9869264322916667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2337,
          "fn": 63,
          "accuracy": 0.97375
        },
        "0.01": null
      },
      "auroc": 0.9880893229166667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2128,
          "fn": 272,
          "accuracy": 0.8866666666666667
        },
        "0.01": null
      },
      "auroc": 0.9769479166666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4465,
          "fn": 335,
          "accuracy": 0.9302083333333333
        },
        "0.01": null
      },
      "auroc": 0.9825186197916667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4694,
          "fn": 106,
          "accuracy": 0.9779166666666667
        },
        "0.01": null
      },
      "auroc": 0.9885825520833333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4380,
          "fn": 420,
          "accuracy": 0.9125
        },
        "0.01": null
      },
      "auroc": 0.9808625
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9074,
          "fn": 526,
          "accuracy": 0.9452083333333333
        },
        "0.01": null
      },
      "auroc": 0.9847225260416667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2393,
          "fn": 7,
          "accuracy": 0.9970833333333333
        },
        "0.01": null
      },
      "auroc": 0.9907579861111111
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2382,
          "fn": 18,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9899871527777777
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4775,
          "fn": 25,
          "accuracy": 0.9947916666666666
        },
        "0.01": null
      },
      "auroc": 0.9903725694444444
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2140,
          "fn": 260,
          "accuracy": 0.8916666666666667
        },
        "0.01": null
      },
      "auroc": 0.9758469618055555
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2164,
          "fn": 236,
          "accuracy": 0.9016666666666666
        },
        "0.01": null
      },
      "auroc": 0.9794200520833334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4304,
          "fn": 496,
          "accuracy": 0.8966666666666666
        },
        "0.01": null
      },
      "auroc": 0.9776335069444445
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4533,
          "fn": 267,
          "accuracy": 0.944375
        },
        "0.01": null
      },
      "auroc": 0.9833024739583334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4546,
          "fn": 254,
          "accuracy": 0.9470833333333334
        },
        "0.01": null
      },
      "auroc": 0.9847036024305555
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9079,
          "fn": 521,
          "accuracy": 0.9457291666666666
        },
        "0.01": null
      },
      "auroc": 0.9840030381944443
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2399,
          "fn": 1,
          "accuracy": 0.9995833333333334
        },
        "0.01": null
      },
      "auroc": 0.99114453125
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2388,
          "fn": 12,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9902901909722223
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4787,
          "fn": 13,
          "accuracy": 0.9972916666666667
        },
        "0.01": null
      },
      "auroc": 0.9907173611111111
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1861,
          "fn": 539,
          "accuracy": 0.7754166666666666
        },
        "0.01": null
      },
      "auroc": 0.9404421875000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2106,
          "fn": 294,
          "accuracy": 0.8775
        },
        "0.01": null
      },
      "auroc": 0.9673103298611111
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3967,
          "fn": 833,
          "accuracy": 0.8264583333333333
        },
        "0.01": null
      },
      "auroc": 0.9538762586805556
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4260,
          "fn": 540,
          "accuracy": 0.8875
        },
        "0.01": null
      },
      "auroc": 0.965793359375
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4494,
          "fn": 306,
          "accuracy": 0.93625
        },
        "0.01": null
      },
      "auroc": 0.9788002604166666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8754,
          "fn": 846,
          "accuracy": 0.911875
        },
        "0.01": null
      },
      "auroc": 0.9722968098958333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2373,
          "fn": 27,
          "accuracy": 0.98875
        },
        "0.01": null
      },
      "auroc": 0.9897015625
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2317,
          "fn": 83,
          "accuracy": 0.9654166666666667
        },
        "0.01": null
      },
      "auroc": 0.9874048611111111
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4690,
          "fn": 110,
          "accuracy": 0.9770833333333333
        },
        "0.01": null
      },
      "auroc": 0.9885532118055556
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2342,
          "fn": 58,
          "accuracy": 0.9758333333333333
        },
        "0.01": null
      },
      "auroc": 0.9883135416666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2238,
          "fn": 162,
          "accuracy": 0.9325
        },
        "0.01": null
      },
      "auroc": 0.9843744791666665
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4580,
          "fn": 220,
          "accuracy": 0.9541666666666667
        },
        "0.01": null
      },
      "auroc": 0.9863440104166666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4715,
          "fn": 85,
          "accuracy": 0.9822916666666667
        },
        "0.01": null
      },
      "auroc": 0.9890075520833332
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4555,
          "fn": 245,
          "accuracy": 0.9489583333333333
        },
        "0.01": null
      },
      "auroc": 0.985889670138889
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9270,
          "fn": 330,
          "accuracy": 0.965625
        },
        "0.01": null
      },
      "auroc": 0.987448611111111
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2348,
          "fn": 52,
          "accuracy": 0.9783333333333334
        },
        "0.01": null
      },
      "auroc": 0.9884352430555556
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2348,
          "fn": 52,
          "accuracy": 0.9783333333333334
        },
        "0.01": null
      },
      "auroc": 0.9884352430555556
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2269,
          "fn": 131,
          "accuracy": 0.9454166666666667
        },
        "0.01": null
      },
      "auroc": 0.9835225694444445
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2269,
          "fn": 131,
          "accuracy": 0.9454166666666667
        },
        "0.01": null
      },
      "auroc": 0.9835225694444445
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4617,
          "fn": 183,
          "accuracy": 0.961875
        },
        "0.01": null
      },
      "auroc": 0.98597890625
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4617,
          "fn": 183,
          "accuracy": 0.961875
        },
        "0.01": null
      },
      "auroc": 0.98597890625
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2039,
          "fn": 361,
          "accuracy": 0.8495833333333334
        },
        "0.01": null
      },
      "auroc": 0.9625125868055555
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2039,
          "fn": 361,
          "accuracy": 0.8495833333333334
        },
        "0.01": null
      },
      "auroc": 0.9625125868055555
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1747,
          "fn": 653,
          "accuracy": 0.7279166666666667
        },
        "0.01": null
      },
      "auroc": 0.9270855034722222
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1747,
          "fn": 653,
          "accuracy": 0.7279166666666667
        },
        "0.01": null
      },
      "auroc": 0.9270855034722222
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3786,
          "fn": 1014,
          "accuracy": 0.78875
        },
        "0.01": null
      },
      "auroc": 0.9447990451388889
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3786,
          "fn": 1014,
          "accuracy": 0.78875
        },
        "0.01": null
      },
      "auroc": 0.9447990451388889
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2371,
          "fn": 29,
          "accuracy": 0.9879166666666667
        },
        "0.01": null
      },
      "auroc": 0.9898006076388889
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2371,
          "fn": 29,
          "accuracy": 0.9879166666666667
        },
        "0.01": null
      },
      "auroc": 0.9898006076388889
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2327,
          "fn": 73,
          "accuracy": 0.9695833333333334
        },
        "0.01": null
      },
      "auroc": 0.9877063368055555
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2327,
          "fn": 73,
          "accuracy": 0.9695833333333334
        },
        "0.01": null
      },
      "auroc": 0.9877063368055555
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4698,
          "fn": 102,
          "accuracy": 0.97875
        },
        "0.01": null
      },
      "auroc": 0.9887534722222223
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4698,
          "fn": 102,
          "accuracy": 0.97875
        },
        "0.01": null
      },
      "auroc": 0.9887534722222223
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2397,
          "fn": 3,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9907766493055555
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2397,
          "fn": 3,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9907766493055555
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2347,
          "fn": 53,
          "accuracy": 0.9779166666666667
        },
        "0.01": null
      },
      "auroc": 0.9878587673611111
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2347,
          "fn": 53,
          "accuracy": 0.9779166666666667
        },
        "0.01": null
      },
      "auroc": 0.9878587673611111
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4744,
          "fn": 56,
          "accuracy": 0.9883333333333333
        },
        "0.01": null
      },
      "auroc": 0.9893177083333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4744,
          "fn": 56,
          "accuracy": 0.9883333333333333
        },
        "0.01": null
      },
      "auroc": 0.9893177083333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2265,
          "fn": 135,
          "accuracy": 0.94375
        },
        "0.01": null
      },
      "auroc": 0.9839880208333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2265,
          "fn": 135,
          "accuracy": 0.94375
        },
        "0.01": null
      },
      "auroc": 0.9839880208333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2166,
          "fn": 234,
          "accuracy": 0.9025
        },
        "0.01": null
      },
      "auroc": 0.9758105034722222
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2166,
          "fn": 234,
          "accuracy": 0.9025
        },
        "0.01": null
      },
      "auroc": 0.9758105034722222
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4431,
          "fn": 369,
          "accuracy": 0.923125
        },
        "0.01": null
      },
      "auroc": 0.9798992621527778
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4431,
          "fn": 369,
          "accuracy": 0.923125
        },
        "0.01": null
      },
      "auroc": 0.9798992621527778
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 25662,
          "fn": 738,
          "accuracy": 0.9720454545454545
        },
        "0.01": null
      },
      "auroc": 0.9867745975378788
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 13994,
          "fn": 406,
          "accuracy": 0.9718055555555556
        },
        "0.01": null
      },
      "auroc": 0.987985083912037
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 39656,
          "fn": 1144,
          "accuracy": 0.9719607843137255
        },
        "0.01": null
      },
      "auroc": 0.9872018280228759
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 23580,
          "fn": 2820,
          "accuracy": 0.8931818181818182
        },
        "0.01": null
      },
      "auroc": 0.9705542771464646
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 13218,
          "fn": 1182,
          "accuracy": 0.9179166666666667
        },
        "0.01": null
      },
      "auroc": 0.979806785300926
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 36798,
          "fn": 4002,
          "accuracy": 0.9019117647058823
        },
        "0.01": null
      },
      "auroc": 0.9738198682598039
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 49242,
          "fn": 3558,
          "accuracy": 0.9326136363636364
        },
        "0.01": null
      },
      "auroc": 0.9786644373421718
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 27212,
          "fn": 1588,
          "accuracy": 0.9448611111111112
        },
        "0.01": null
      },
      "auroc": 0.9838959346064814
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 76454,
          "fn": 5146,
          "accuracy": 0.9369362745098039
        },
        "0.01": null
      },
      "auroc": 0.9805108481413398
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9942520833333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943135416666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        }
      },
      "auroc": 0.94540625
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9917781250000001
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        }
      },
      "auroc": 0.9685921875
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        }
      },
      "auroc": 0.969890625
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9930151041666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 755,
          "fn": 45,
          "accuracy": 0.94375
        },
        "0.01": {
          "tp": 706,
          "fn": 94,
          "accuracy": 0.8825
        }
      },
      "auroc": 0.9814528645833334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.991596875
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9929859375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9929666666666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 791,
          "fn": 9,
          "accuracy": 0.98875
        }
      },
      "auroc": 0.9936708333333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9887906249999999
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9935864583333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.9911885416666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        }
      },
      "auroc": 0.9915828125
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9939807291666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 751,
          "fn": 49,
          "accuracy": 0.93875
        }
      },
      "auroc": 0.9927817708333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9935208333333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9939479166666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.9119541666666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9763552083333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": {
          "tp": 267,
          "fn": 133,
          "accuracy": 0.6675
        }
      },
      "auroc": 0.9441546875000001
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        }
      },
      "auroc": 0.9531645833333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9849380208333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 721,
          "fn": 79,
          "accuracy": 0.90125
        },
        "0.01": {
          "tp": 657,
          "fn": 143,
          "accuracy": 0.82125
        }
      },
      "auroc": 0.9690513020833333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9943653645833334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9892614583333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9892614583333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9918182291666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9918182291666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9848947916666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9848947916666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9729875
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9729875
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        }
      },
      "auroc": 0.9789411458333334
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        }
      },
      "auroc": 0.9789411458333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9927052083333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9927052083333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9925947916666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9925947916666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9926499999999999
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9926499999999999
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2185,
          "fn": 15,
          "accuracy": 0.9931818181818182
        },
        "0.01": {
          "tp": 2166,
          "fn": 34,
          "accuracy": 0.9845454545454545
        }
      },
      "auroc": 0.9933613636363636
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        },
        "0.01": {
          "tp": 1187,
          "fn": 13,
          "accuracy": 0.9891666666666666
        }
      },
      "auroc": 0.9942057291666666
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3383,
          "fn": 17,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 3353,
          "fn": 47,
          "accuracy": 0.9861764705882353
        }
      },
      "auroc": 0.993659375
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2062,
          "fn": 138,
          "accuracy": 0.9372727272727273
        },
        "0.01": {
          "tp": 1929,
          "fn": 271,
          "accuracy": 0.8768181818181818
        }
      },
      "auroc": 0.9793447916666667
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1179,
          "fn": 21,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 1137,
          "fn": 63,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9903444444444445
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3241,
          "fn": 159,
          "accuracy": 0.9532352941176471
        },
        "0.01": {
          "tp": 3066,
          "fn": 334,
          "accuracy": 0.9017647058823529
        }
      },
      "auroc": 0.9832270220588235
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 4247,
          "fn": 153,
          "accuracy": 0.9652272727272727
        },
        "0.01": {
          "tp": 4095,
          "fn": 305,
          "accuracy": 0.9306818181818182
        }
      },
      "auroc": 0.9863530776515151
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2377,
          "fn": 23,
          "accuracy": 0.9904166666666666
        },
        "0.01": {
          "tp": 2324,
          "fn": 76,
          "accuracy": 0.9683333333333334
        }
      },
      "auroc": 0.9922750868055555
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 6624,
          "fn": 176,
          "accuracy": 0.9741176470588235
        },
        "0.01": {
          "tp": 6419,
          "fn": 381,
          "accuracy": 0.9439705882352941
        }
      },
      "auroc": 0.9884431985294116
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9942520833333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943135416666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        }
      },
      "auroc": 0.94540625
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9917781250000001
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        }
      },
      "auroc": 0.9685921875
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        }
      },
      "auroc": 0.969890625
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9930151041666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 755,
          "fn": 45,
          "accuracy": 0.94375
        },
        "0.01": {
          "tp": 706,
          "fn": 94,
          "accuracy": 0.8825
        }
      },
      "auroc": 0.9814528645833334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.991596875
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9929859375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9929666666666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 791,
          "fn": 9,
          "accuracy": 0.98875
        }
      },
      "auroc": 0.9936708333333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9887895833333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9935864583333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.9911880208333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        }
      },
      "auroc": 0.9915822916666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9939807291666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 751,
          "fn": 49,
          "accuracy": 0.93875
        }
      },
      "auroc": 0.9927815104166666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.993525
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.99395
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.9119791666666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9763552083333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": {
          "tp": 267,
          "fn": 133,
          "accuracy": 0.6675
        }
      },
      "auroc": 0.9441671875
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        }
      },
      "auroc": 0.9531770833333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9849401041666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 721,
          "fn": 79,
          "accuracy": 0.90125
        },
        "0.01": {
          "tp": 657,
          "fn": 143,
          "accuracy": 0.82125
        }
      },
      "auroc": 0.96905859375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9943653645833334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9892645833333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9892645833333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9918197916666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9918197916666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9847989583333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9847989583333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9729927083333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9729927083333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        }
      },
      "auroc": 0.9788958333333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        }
      },
      "auroc": 0.9788958333333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.99270625
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.99270625
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9925916666666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9925916666666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9926489583333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9926489583333333
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2185,
          "fn": 15,
          "accuracy": 0.9931818181818182
        },
        "0.01": {
          "tp": 2166,
          "fn": 34,
          "accuracy": 0.9845454545454545
        }
      },
      "auroc": 0.9933527462121212
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        },
        "0.01": {
          "tp": 1187,
          "fn": 13,
          "accuracy": 0.9891666666666666
        }
      },
      "auroc": 0.9942064236111111
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3383,
          "fn": 17,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 3353,
          "fn": 47,
          "accuracy": 0.9861764705882353
        }
      },
      "auroc": 0.9936540441176471
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2062,
          "fn": 138,
          "accuracy": 0.9372727272727273
        },
        "0.01": {
          "tp": 1929,
          "fn": 271,
          "accuracy": 0.8768181818181818
        }
      },
      "auroc": 0.9793474431818181
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1179,
          "fn": 21,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 1137,
          "fn": 63,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9903444444444445
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3241,
          "fn": 159,
          "accuracy": 0.9532352941176471
        },
        "0.01": {
          "tp": 3066,
          "fn": 334,
          "accuracy": 0.9017647058823529
        }
      },
      "auroc": 0.9832287377450981
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 4247,
          "fn": 153,
          "accuracy": 0.9652272727272727
        },
        "0.01": {
          "tp": 4095,
          "fn": 305,
          "accuracy": 0.9306818181818182
        }
      },
      "auroc": 0.9863500946969697
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2377,
          "fn": 23,
          "accuracy": 0.9904166666666666
        },
        "0.01": {
          "tp": 2324,
          "fn": 76,
          "accuracy": 0.9683333333333334
        }
      },
      "auroc": 0.9922754340277778
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 6624,
          "fn": 176,
          "accuracy": 0.9741176470588235
        },
        "0.01": {
          "tp": 6419,
          "fn": 381,
          "accuracy": 0.9439705882352941
        }
      },
      "auroc": 0.9884413909313725
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9942520833333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943135416666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9531447916666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9918885416666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        }
      },
      "auroc": 0.9725166666666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        }
      },
      "auroc": 0.9737598958333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9930703125000001
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 766,
          "fn": 34,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 733,
          "fn": 67,
          "accuracy": 0.91625
        }
      },
      "auroc": 0.9834151041666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9917427083333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9930588541666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9930588541666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9937169270833333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9913500000000001
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9934885416666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9924192708333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9928625
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9939125
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 768,
          "fn": 32,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9933875
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9934125
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.99389375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        }
      },
      "auroc": 0.9159666666666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9808635416666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": {
          "tp": 289,
          "fn": 111,
          "accuracy": 0.7225
        }
      },
      "auroc": 0.9484151041666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9551708333333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.9871380208333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 731,
          "fn": 69,
          "accuracy": 0.91375
        },
        "0.01": {
          "tp": 683,
          "fn": 117,
          "accuracy": 0.85375
        }
      },
      "auroc": 0.9711544270833333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9943653645833334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9897270833333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9897270833333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9920317708333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9920317708333334
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9854510416666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9854510416666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9721770833333332
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9721770833333332
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        }
      },
      "auroc": 0.9788140625
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        }
      },
      "auroc": 0.9788140625
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.99346875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.99346875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9939218750000001
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9939218750000001
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9917979166666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9917979166666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.992403125
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.992403125
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9921005208333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9921005208333333
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2186,
          "fn": 14,
          "accuracy": 0.9936363636363637
        },
        "0.01": {
          "tp": 2163,
          "fn": 37,
          "accuracy": 0.9831818181818182
        }
      },
      "auroc": 0.9933259469696969
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        },
        "0.01": {
          "tp": 1191,
          "fn": 9,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.994187673611111
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3384,
          "fn": 16,
          "accuracy": 0.9952941176470588
        },
        "0.01": {
          "tp": 3354,
          "fn": 46,
          "accuracy": 0.9864705882352941
        }
      },
      "auroc": 0.9936300857843137
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2082,
          "fn": 118,
          "accuracy": 0.9463636363636364
        },
        "0.01": {
          "tp": 1981,
          "fn": 219,
          "accuracy": 0.9004545454545455
        }
      },
      "auroc": 0.9805180871212121
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1184,
          "fn": 16,
          "accuracy": 0.9866666666666667
        },
        "0.01": {
          "tp": 1147,
          "fn": 53,
          "accuracy": 0.9558333333333333
        }
      },
      "auroc": 0.9911222222222222
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3266,
          "fn": 134,
          "accuracy": 0.9605882352941176
        },
        "0.01": {
          "tp": 3128,
          "fn": 272,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9842607230392156
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 4268,
          "fn": 132,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 4144,
          "fn": 256,
          "accuracy": 0.9418181818181818
        }
      },
      "auroc": 0.9869220170454546
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2382,
          "fn": 18,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 2338,
          "fn": 62,
          "accuracy": 0.9741666666666666
        }
      },
      "auroc": 0.9926549479166666
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 6650,
          "fn": 150,
          "accuracy": 0.9779411764705882
        },
        "0.01": {
          "tp": 6482,
          "fn": 318,
          "accuracy": 0.9532352941176471
        }
      },
      "auroc": 0.9889454044117647
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9942979166666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9942958333333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.994296875
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        }
      },
      "auroc": 0.9445375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.99199375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        }
      },
      "auroc": 0.9682656249999999
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        }
      },
      "auroc": 0.9694177083333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9931447916666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 759,
          "fn": 41,
          "accuracy": 0.94875
        },
        "0.01": {
          "tp": 718,
          "fn": 82,
          "accuracy": 0.8975
        }
      },
      "auroc": 0.98128125
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9941979166666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9942864583333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9911979166666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9927864583333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9926979166666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 795,
          "fn": 5,
          "accuracy": 0.99375
        },
        "0.01": {
          "tp": 787,
          "fn": 13,
          "accuracy": 0.98375
        }
      },
      "auroc": 0.9935364583333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9942708333333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943229166666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.9809708333333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9925479166666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        }
      },
      "auroc": 0.9867593750000001
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        }
      },
      "auroc": 0.9876729166666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9934093749999999
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 785,
          "fn": 15,
          "accuracy": 0.98125
        },
        "0.01": {
          "tp": 732,
          "fn": 68,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9905411458333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9884552083333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9914151041666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        }
      },
      "auroc": 0.91031875
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9872135416666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 271,
          "fn": 129,
          "accuracy": 0.6775
        }
      },
      "auroc": 0.9487661458333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        }
      },
      "auroc": 0.952346875
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.987834375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 719,
          "fn": 81,
          "accuracy": 0.89875
        },
        "0.01": {
          "tp": 645,
          "fn": 155,
          "accuracy": 0.80625
        }
      },
      "auroc": 0.9700906250000001
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9942572916666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943161458333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943161458333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943455729166667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9910697916666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9910697916666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9857989583333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9857989583333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.988434375
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.988434375
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9680697916666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9680697916666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        }
      },
      "auroc": 0.95716875
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        }
      },
      "auroc": 0.95716875
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        }
      },
      "auroc": 0.9626192708333334
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        }
      },
      "auroc": 0.9626192708333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9922822916666666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9922822916666666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9933286458333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9933286458333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9852729166666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9852729166666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9849083333333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9849083333333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        }
      },
      "auroc": 0.985090625
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        }
      },
      "auroc": 0.985090625
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2158,
          "fn": 42,
          "accuracy": 0.980909090909091
        },
        "0.01": {
          "tp": 2121,
          "fn": 79,
          "accuracy": 0.9640909090909091
        }
      },
      "auroc": 0.9908486742424243
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1191,
          "fn": 9,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 1170,
          "fn": 30,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9933282986111112
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3349,
          "fn": 51,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 3291,
          "fn": 109,
          "accuracy": 0.9679411764705882
        }
      },
      "auroc": 0.9917238357843138
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2029,
          "fn": 171,
          "accuracy": 0.9222727272727272
        },
        "0.01": {
          "tp": 1842,
          "fn": 358,
          "accuracy": 0.8372727272727273
        }
      },
      "auroc": 0.9757714015151515
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1181,
          "fn": 19,
          "accuracy": 0.9841666666666666
        },
        "0.01": {
          "tp": 1146,
          "fn": 54,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9919309027777778
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3210,
          "fn": 190,
          "accuracy": 0.9441176470588235
        },
        "0.01": {
          "tp": 2988,
          "fn": 412,
          "accuracy": 0.8788235294117647
        }
      },
      "auroc": 0.9814747549019609
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 4187,
          "fn": 213,
          "accuracy": 0.951590909090909
        },
        "0.01": {
          "tp": 3963,
          "fn": 437,
          "accuracy": 0.9006818181818181
        }
      },
      "auroc": 0.9833100378787879
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2372,
          "fn": 28,
          "accuracy": 0.9883333333333333
        },
        "0.01": {
          "tp": 2316,
          "fn": 84,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9926296006944444
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 6559,
          "fn": 241,
          "accuracy": 0.9645588235294118
        },
        "0.01": {
          "tp": 6279,
          "fn": 521,
          "accuracy": 0.9233823529411764
        }
      },
      "auroc": 0.9865992953431372
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9942520833333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943135416666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        }
      },
      "auroc": 0.941565625
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9918427083333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9667041666666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9679703125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9930473958333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 751,
          "fn": 49,
          "accuracy": 0.93875
        },
        "0.01": {
          "tp": 704,
          "fn": 96,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9805088541666668
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943135416666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943442708333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9910562500000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9927156250000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9926848958333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9935299479166667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.987090625
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9935010416666668
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        }
      },
      "auroc": 0.9902958333333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9907328125
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9939380208333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 788,
          "fn": 12,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 753,
          "fn": 47,
          "accuracy": 0.94125
        }
      },
      "auroc": 0.9923354166666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9936947916666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9940348958333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.9036979166666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9761437499999999
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        }
      },
      "auroc": 0.9399208333333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        }
      },
      "auroc": 0.9490364583333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        }
      },
      "auroc": 0.9849192708333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 720,
          "fn": 80,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 656,
          "fn": 144,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9669778645833333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9943653645833334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9885979166666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9885979166666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9914864583333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9914864583333334
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9842677083333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9842677083333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9715760416666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9715760416666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        }
      },
      "auroc": 0.977921875
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        }
      },
      "auroc": 0.977921875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9940666666666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9940666666666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9942208333333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9942208333333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9926802083333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9926802083333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9922833333333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9922833333333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9924817708333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9924817708333334
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2183,
          "fn": 17,
          "accuracy": 0.9922727272727273
        },
        "0.01": {
          "tp": 2167,
          "fn": 33,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9933020833333333
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 1,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 1190,
          "fn": 10,
          "accuracy": 0.9916666666666667
        }
      },
      "auroc": 0.9942309027777778
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3382,
          "fn": 18,
          "accuracy": 0.9947058823529412
        },
        "0.01": {
          "tp": 3357,
          "fn": 43,
          "accuracy": 0.9873529411764705
        }
      },
      "auroc": 0.9936299019607844
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2053,
          "fn": 147,
          "accuracy": 0.9331818181818182
        },
        "0.01": {
          "tp": 1926,
          "fn": 274,
          "accuracy": 0.8754545454545455
        }
      },
      "auroc": 0.977849053030303
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1180,
          "fn": 20,
          "accuracy": 0.9833333333333333
        },
        "0.01": {
          "tp": 1133,
          "fn": 67,
          "accuracy": 0.9441666666666667
        }
      },
      "auroc": 0.990215625
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3233,
          "fn": 167,
          "accuracy": 0.9508823529411765
        },
        "0.01": {
          "tp": 3059,
          "fn": 341,
          "accuracy": 0.8997058823529411
        }
      },
      "auroc": 0.9822137254901961
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 4236,
          "fn": 164,
          "accuracy": 0.9627272727272728
        },
        "0.01": {
          "tp": 4093,
          "fn": 307,
          "accuracy": 0.9302272727272727
        }
      },
      "auroc": 0.9855755681818182
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2379,
          "fn": 21,
          "accuracy": 0.99125
        },
        "0.01": {
          "tp": 2323,
          "fn": 77,
          "accuracy": 0.9679166666666666
        }
      },
      "auroc": 0.9922232638888889
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 6615,
          "fn": 185,
          "accuracy": 0.9727941176470588
        },
        "0.01": {
          "tp": 6416,
          "fn": 384,
          "accuracy": 0.9435294117647058
        }
      },
      "auroc": 0.9879218137254901
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9936635416666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9919593750000001
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9928114583333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9934177083333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9850145833333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        }
      },
      "auroc": 0.9892161458333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9935406250000001
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9884869791666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 784,
          "fn": 16,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 755,
          "fn": 45,
          "accuracy": 0.94375
        }
      },
      "auroc": 0.9910138020833333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.990715625
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.9864666666666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        }
      },
      "auroc": 0.9885911458333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9172916666666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        }
      },
      "auroc": 0.9673854166666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 299,
          "fn": 101,
          "accuracy": 0.7475
        },
        "0.01": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.9423385416666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        }
      },
      "auroc": 0.9540036458333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        }
      },
      "auroc": 0.9769260416666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 681,
          "fn": 119,
          "accuracy": 0.85125
        },
        "0.01": {
          "tp": 520,
          "fn": 280,
          "accuracy": 0.65
        }
      },
      "auroc": 0.96546484375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9931927083333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9902270833333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.9917098958333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9877958333333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        }
      },
      "auroc": 0.9802979166666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        }
      },
      "auroc": 0.984046875
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        }
      },
      "auroc": 0.9904942708333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9852625
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 766,
          "fn": 34,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 697,
          "fn": 103,
          "accuracy": 0.87125
        }
      },
      "auroc": 0.9878783854166666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9903291666666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9891656249999999
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9897473958333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        }
      },
      "auroc": 0.9651635416666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        }
      },
      "auroc": 0.9700572916666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.9676104166666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        }
      },
      "auroc": 0.9777463541666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        }
      },
      "auroc": 0.9796114583333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 729,
          "fn": 71,
          "accuracy": 0.91125
        },
        "0.01": {
          "tp": 547,
          "fn": 253,
          "accuracy": 0.68375
        }
      },
      "auroc": 0.97867890625
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.98823125
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        }
      },
      "auroc": 0.9621677083333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9751994791666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.8693989583333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.9099541666666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 123,
          "fn": 277,
          "accuracy": 0.3075
        }
      },
      "auroc": 0.8896765624999999
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 295,
          "fn": 105,
          "accuracy": 0.7375
        },
        "0.01": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        }
      },
      "auroc": 0.9288151041666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.9360609375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 593,
          "fn": 207,
          "accuracy": 0.74125
        },
        "0.01": {
          "tp": 413,
          "fn": 387,
          "accuracy": 0.51625
        }
      },
      "auroc": 0.9324380208333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9938541666666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9914375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9926458333333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9934072916666666
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9812354166666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9873213541666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9936307291666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9863364583333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 786,
          "fn": 14,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 738,
          "fn": 62,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9899835937500001
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9814947916666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9814947916666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9733239583333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9733239583333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        }
      },
      "auroc": 0.9774093749999999
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        }
      },
      "auroc": 0.9774093749999999
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.9364052083333334
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.9364052083333334
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        },
        "0.01": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        }
      },
      "auroc": 0.9282854166666668
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        },
        "0.01": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        }
      },
      "auroc": 0.9282854166666668
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 186,
          "fn": 214,
          "accuracy": 0.465
        }
      },
      "auroc": 0.9323453125000001
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 186,
          "fn": 214,
          "accuracy": 0.465
        }
      },
      "auroc": 0.9323453125000001
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.994234375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.994234375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9940802083333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9940802083333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9941572916666668
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9941572916666668
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9936031249999999
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9936031249999999
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9773322916666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9773322916666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9854677083333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9854677083333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9729010416666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9729010416666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.9685875
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.9685875
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9707442708333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9707442708333334
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2097,
          "fn": 103,
          "accuracy": 0.9531818181818181
        },
        "0.01": {
          "tp": 1937,
          "fn": 263,
          "accuracy": 0.8804545454545455
        }
      },
      "auroc": 0.9844204545454545
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1135,
          "fn": 65,
          "accuracy": 0.9458333333333333
        },
        "0.01": {
          "tp": 984,
          "fn": 216,
          "accuracy": 0.82
        }
      },
      "auroc": 0.985237326388889
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3232,
          "fn": 168,
          "accuracy": 0.9505882352941176
        },
        "0.01": {
          "tp": 2921,
          "fn": 479,
          "accuracy": 0.8591176470588235
        }
      },
      "auroc": 0.984708762254902
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1862,
          "fn": 338,
          "accuracy": 0.8463636363636363
        },
        "0.01": {
          "tp": 1493,
          "fn": 707,
          "accuracy": 0.6786363636363636
        }
      },
      "auroc": 0.9607349431818182
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1034,
          "fn": 166,
          "accuracy": 0.8616666666666667
        },
        "0.01": {
          "tp": 808,
          "fn": 392,
          "accuracy": 0.6733333333333333
        }
      },
      "auroc": 0.9656574652777778
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2896,
          "fn": 504,
          "accuracy": 0.851764705882353
        },
        "0.01": {
          "tp": 2301,
          "fn": 1099,
          "accuracy": 0.6767647058823529
        }
      },
      "auroc": 0.9624723039215687
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3959,
          "fn": 441,
          "accuracy": 0.8997727272727273
        },
        "0.01": {
          "tp": 3430,
          "fn": 970,
          "accuracy": 0.7795454545454545
        }
      },
      "auroc": 0.9725776988636363
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2169,
          "fn": 231,
          "accuracy": 0.90375
        },
        "0.01": {
          "tp": 1792,
          "fn": 608,
          "accuracy": 0.7466666666666667
        }
      },
      "auroc": 0.9754473958333334
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 6128,
          "fn": 672,
          "accuracy": 0.9011764705882352
        },
        "0.01": {
          "tp": 5222,
          "fn": 1578,
          "accuracy": 0.7679411764705882
        }
      },
      "auroc": 0.9735905330882353
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9942520833333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943135416666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        }
      },
      "auroc": 0.95036875
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9918281250000001
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9710984375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        }
      },
      "auroc": 0.9723718749999999
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9930401041666668
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 760,
          "fn": 40,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 714,
          "fn": 86,
          "accuracy": 0.8925
        }
      },
      "auroc": 0.9827059895833334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9916812500000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.993028125
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.993028125
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 793,
          "fn": 7,
          "accuracy": 0.99125
        }
      },
      "auroc": 0.9937015625
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9942687499999999
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943218749999999
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9892062500000001
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.99365
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.9914281250000001
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        }
      },
      "auroc": 0.9917375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9940125
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 792,
          "fn": 8,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 750,
          "fn": 50,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.992875
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.993625
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.994
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.91623125
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9779406249999999
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        },
        "0.01": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        }
      },
      "auroc": 0.9470859375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        },
        "0.01": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        }
      },
      "auroc": 0.955303125
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9857828125
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 727,
          "fn": 73,
          "accuracy": 0.90875
        },
        "0.01": {
          "tp": 665,
          "fn": 135,
          "accuracy": 0.83125
        }
      },
      "auroc": 0.97054296875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9943653645833334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9897614583333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9897614583333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9920682291666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9920682291666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9859760416666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9859760416666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9762822916666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9762822916666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9811291666666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9811291666666667
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9912260416666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9912260416666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9933635416666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9933635416666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9922947916666668
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9922947916666668
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2188,
          "fn": 12,
          "accuracy": 0.9945454545454545
        },
        "0.01": {
          "tp": 2171,
          "fn": 29,
          "accuracy": 0.9868181818181818
        }
      },
      "auroc": 0.9933155303030303
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        },
        "0.01": {
          "tp": 1191,
          "fn": 9,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9942295138888889
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3386,
          "fn": 14,
          "accuracy": 0.9958823529411764
        },
        "0.01": {
          "tp": 3362,
          "fn": 38,
          "accuracy": 0.9888235294117647
        }
      },
      "auroc": 0.9936381127450982
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2073,
          "fn": 127,
          "accuracy": 0.9422727272727273
        },
        "0.01": {
          "tp": 1945,
          "fn": 255,
          "accuracy": 0.884090909090909
        }
      },
      "auroc": 0.9806374053030303
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1182,
          "fn": 18,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 1138,
          "fn": 62,
          "accuracy": 0.9483333333333334
        }
      },
      "auroc": 0.9906416666666666
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3255,
          "fn": 145,
          "accuracy": 0.9573529411764706
        },
        "0.01": {
          "tp": 3083,
          "fn": 317,
          "accuracy": 0.9067647058823529
        }
      },
      "auroc": 0.9841683210784313
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 4261,
          "fn": 139,
          "accuracy": 0.9684090909090909
        },
        "0.01": {
          "tp": 4116,
          "fn": 284,
          "accuracy": 0.9354545454545454
        }
      },
      "auroc": 0.9869764678030302
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2380,
          "fn": 20,
          "accuracy": 0.9916666666666667
        },
        "0.01": {
          "tp": 2329,
          "fn": 71,
          "accuracy": 0.9704166666666667
        }
      },
      "auroc": 0.9924355902777777
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 6641,
          "fn": 159,
          "accuracy": 0.9766176470588235
        },
        "0.01": {
          "tp": 6445,
          "fn": 355,
          "accuracy": 0.9477941176470588
        }
      },
      "auroc": 0.9889032169117647
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9942520833333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943135416666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        }
      },
      "auroc": 0.9457302083333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9917781250000001
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        }
      },
      "auroc": 0.9687541666666668
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        }
      },
      "auroc": 0.9700526041666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9930151041666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 755,
          "fn": 45,
          "accuracy": 0.94375
        },
        "0.01": {
          "tp": 706,
          "fn": 94,
          "accuracy": 0.8825
        }
      },
      "auroc": 0.9815338541666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.991603125
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9929890625000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9929697916666668
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 791,
          "fn": 9,
          "accuracy": 0.98875
        }
      },
      "auroc": 0.9936723958333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9887895833333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9935864583333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.9911880208333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        }
      },
      "auroc": 0.9915822916666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9939807291666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 751,
          "fn": 49,
          "accuracy": 0.93875
        }
      },
      "auroc": 0.9927815104166666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.993525
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.99395
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.9119791666666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.976246875
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 267,
          "fn": 133,
          "accuracy": 0.6675
        }
      },
      "auroc": 0.9441130208333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        }
      },
      "auroc": 0.9531770833333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9848859375000001
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 720,
          "fn": 80,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 657,
          "fn": 143,
          "accuracy": 0.82125
        }
      },
      "auroc": 0.9690315104166667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9943653645833334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9892645833333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9892645833333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9918197916666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9918197916666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9848552083333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9848552083333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9729927083333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9729927083333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        }
      },
      "auroc": 0.9789239583333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        }
      },
      "auroc": 0.9789239583333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.99270625
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.99270625
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9925916666666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9925916666666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9926489583333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9926489583333333
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2185,
          "fn": 15,
          "accuracy": 0.9931818181818182
        },
        "0.01": {
          "tp": 2166,
          "fn": 34,
          "accuracy": 0.9845454545454545
        }
      },
      "auroc": 0.9933578598484849
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        },
        "0.01": {
          "tp": 1187,
          "fn": 13,
          "accuracy": 0.9891666666666666
        }
      },
      "auroc": 0.9942064236111111
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3383,
          "fn": 17,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 3353,
          "fn": 47,
          "accuracy": 0.9861764705882353
        }
      },
      "auroc": 0.9936573529411765
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2062,
          "fn": 138,
          "accuracy": 0.9372727272727273
        },
        "0.01": {
          "tp": 1929,
          "fn": 271,
          "accuracy": 0.8768181818181818
        }
      },
      "auroc": 0.9793768939393939
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1178,
          "fn": 22,
          "accuracy": 0.9816666666666667
        },
        "0.01": {
          "tp": 1137,
          "fn": 63,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9903274305555556
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3240,
          "fn": 160,
          "accuracy": 0.9529411764705882
        },
        "0.01": {
          "tp": 3066,
          "fn": 334,
          "accuracy": 0.9017647058823529
        }
      },
      "auroc": 0.9832417892156863
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 4247,
          "fn": 153,
          "accuracy": 0.9652272727272727
        },
        "0.01": {
          "tp": 4095,
          "fn": 305,
          "accuracy": 0.9306818181818182
        }
      },
      "auroc": 0.9863673768939394
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2376,
          "fn": 24,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 2324,
          "fn": 76,
          "accuracy": 0.9683333333333334
        }
      },
      "auroc": 0.9922669270833333
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 6623,
          "fn": 177,
          "accuracy": 0.9739705882352941
        },
        "0.01": {
          "tp": 6419,
          "fn": 381,
          "accuracy": 0.9439705882352941
        }
      },
      "auroc": 0.9884495710784315
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.9806864583333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.97365625
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        }
      },
      "auroc": 0.9771713541666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.9778791666666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        }
      },
      "auroc": 0.9690979166666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 161,
          "fn": 239,
          "accuracy": 0.4025
        }
      },
      "auroc": 0.9734885416666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        }
      },
      "auroc": 0.9792828124999999
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 137,
          "fn": 263,
          "accuracy": 0.3425
        }
      },
      "auroc": 0.9713770833333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 685,
          "fn": 115,
          "accuracy": 0.85625
        },
        "0.01": {
          "tp": 334,
          "fn": 466,
          "accuracy": 0.4175
        }
      },
      "auroc": 0.9753299479166666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        }
      },
      "auroc": 0.97715625
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        }
      },
      "auroc": 0.9878822916666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        }
      },
      "auroc": 0.9825192708333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.9011916666666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9811989583333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        },
        "0.01": {
          "tp": 116,
          "fn": 284,
          "accuracy": 0.29
        }
      },
      "auroc": 0.9411953125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        },
        "0.01": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        }
      },
      "auroc": 0.9391739583333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        }
      },
      "auroc": 0.9845406250000001
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 635,
          "fn": 165,
          "accuracy": 0.79375
        },
        "0.01": {
          "tp": 317,
          "fn": 483,
          "accuracy": 0.39625
        }
      },
      "auroc": 0.9618572916666668
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        }
      },
      "auroc": 0.976384375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        }
      },
      "auroc": 0.9627375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 133,
          "fn": 267,
          "accuracy": 0.3325
        }
      },
      "auroc": 0.9695609375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.972571875
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.9521333333333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 84,
          "fn": 316,
          "accuracy": 0.21
        }
      },
      "auroc": 0.9623526041666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": {
          "tp": 150,
          "fn": 250,
          "accuracy": 0.375
        }
      },
      "auroc": 0.974478125
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        },
        "0.01": {
          "tp": 67,
          "fn": 333,
          "accuracy": 0.1675
        }
      },
      "auroc": 0.9574354166666668
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 604,
          "fn": 196,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 217,
          "fn": 583,
          "accuracy": 0.27125
        }
      },
      "auroc": 0.9659567708333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        }
      },
      "auroc": 0.9855375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        }
      },
      "auroc": 0.9623895833333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        },
        "0.01": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        }
      },
      "auroc": 0.9739635416666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.8948697916666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.9449635416666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 240,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 28,
          "fn": 372,
          "accuracy": 0.07
        }
      },
      "auroc": 0.9199166666666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        },
        "0.01": {
          "tp": 146,
          "fn": 254,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9402036458333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 66,
          "fn": 334,
          "accuracy": 0.165
        }
      },
      "auroc": 0.9536765625000001
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 489,
          "fn": 311,
          "accuracy": 0.61125
        },
        "0.01": {
          "tp": 212,
          "fn": 588,
          "accuracy": 0.265
        }
      },
      "auroc": 0.9469401041666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        }
      },
      "auroc": 0.96870625
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        }
      },
      "auroc": 0.9320885416666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 97,
          "fn": 303,
          "accuracy": 0.2425
        }
      },
      "auroc": 0.9503973958333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        },
        "0.01": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        }
      },
      "auroc": 0.8746395833333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        },
        "0.01": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        }
      },
      "auroc": 0.9429270833333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 239,
          "accuracy": 0.4025
        },
        "0.01": {
          "tp": 70,
          "fn": 330,
          "accuracy": 0.175
        }
      },
      "auroc": 0.9087833333333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 87,
          "fn": 313,
          "accuracy": 0.2175
        }
      },
      "auroc": 0.9216729166666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": {
          "tp": 80,
          "fn": 320,
          "accuracy": 0.2
        }
      },
      "auroc": 0.9375078125
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 405,
          "fn": 395,
          "accuracy": 0.50625
        },
        "0.01": {
          "tp": 167,
          "fn": 633,
          "accuracy": 0.20875
        }
      },
      "auroc": 0.9295903645833333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.98724375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.9785083333333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.9828760416666666
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.9690041666666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        }
      },
      "auroc": 0.9614583333333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        },
        "0.01": {
          "tp": 115,
          "fn": 285,
          "accuracy": 0.2875
        }
      },
      "auroc": 0.96523125
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        }
      },
      "auroc": 0.9781239583333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        },
        "0.01": {
          "tp": 126,
          "fn": 274,
          "accuracy": 0.315
        }
      },
      "auroc": 0.9699833333333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 670,
          "fn": 130,
          "accuracy": 0.8375
        },
        "0.01": {
          "tp": 323,
          "fn": 477,
          "accuracy": 0.40375
        }
      },
      "auroc": 0.9740536458333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        }
      },
      "auroc": 0.94094375
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        }
      },
      "auroc": 0.94094375
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.9123562500000001
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.9123562500000001
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        },
        "0.01": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        }
      },
      "auroc": 0.92665
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        },
        "0.01": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        }
      },
      "auroc": 0.92665
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.9164281249999999
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.9164281249999999
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.9065916666666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.9065916666666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 271,
          "accuracy": 0.3225
        },
        "0.01": {
          "tp": 22,
          "fn": 378,
          "accuracy": 0.055
        }
      },
      "auroc": 0.9115098958333334
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 271,
          "accuracy": 0.3225
        },
        "0.01": {
          "tp": 22,
          "fn": 378,
          "accuracy": 0.055
        }
      },
      "auroc": 0.9115098958333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.9806010416666666
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.9806010416666666
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        }
      },
      "auroc": 0.969659375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        }
      },
      "auroc": 0.969659375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": {
          "tp": 134,
          "fn": 266,
          "accuracy": 0.335
        }
      },
      "auroc": 0.9751302083333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": {
          "tp": 134,
          "fn": 266,
          "accuracy": 0.335
        }
      },
      "auroc": 0.9751302083333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        }
      },
      "auroc": 0.9802708333333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        }
      },
      "auroc": 0.9802708333333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.936275
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.936275
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 287,
          "fn": 113,
          "accuracy": 0.7175
        },
        "0.01": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        }
      },
      "auroc": 0.9582729166666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 287,
          "fn": 113,
          "accuracy": 0.7175
        },
        "0.01": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        }
      },
      "auroc": 0.9582729166666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.9420197916666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.9420197916666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        }
      },
      "auroc": 0.9301125
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        }
      },
      "auroc": 0.9301125
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 82,
          "fn": 318,
          "accuracy": 0.205
        }
      },
      "auroc": 0.9360661458333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 82,
          "fn": 318,
          "accuracy": 0.205
        }
      },
      "auroc": 0.9360661458333333
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1734,
          "fn": 466,
          "accuracy": 0.7881818181818182
        },
        "0.01": {
          "tp": 874,
          "fn": 1326,
          "accuracy": 0.3972727272727273
        }
      },
      "auroc": 0.9669071022727272
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 914,
          "fn": 286,
          "accuracy": 0.7616666666666667
        },
        "0.01": {
          "tp": 386,
          "fn": 814,
          "accuracy": 0.32166666666666666
        }
      },
      "auroc": 0.9662104166666667
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2648,
          "fn": 752,
          "accuracy": 0.7788235294117647
        },
        "0.01": {
          "tp": 1260,
          "fn": 2140,
          "accuracy": 0.37058823529411766
        }
      },
      "auroc": 0.9666612132352941
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1184,
          "fn": 1016,
          "accuracy": 0.5381818181818182
        },
        "0.01": {
          "tp": 421,
          "fn": 1779,
          "accuracy": 0.19136363636363637
        }
      },
      "auroc": 0.9313773674242425
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 829,
          "fn": 371,
          "accuracy": 0.6908333333333333
        },
        "0.01": {
          "tp": 293,
          "fn": 907,
          "accuracy": 0.24416666666666667
        }
      },
      "auroc": 0.9586298611111111
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2013,
          "fn": 1387,
          "accuracy": 0.5920588235294117
        },
        "0.01": {
          "tp": 714,
          "fn": 2686,
          "accuracy": 0.21
        }
      },
      "auroc": 0.9409958946078432
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2918,
          "fn": 1482,
          "accuracy": 0.6631818181818182
        },
        "0.01": {
          "tp": 1295,
          "fn": 3105,
          "accuracy": 0.2943181818181818
        }
      },
      "auroc": 0.9491422348484848
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1743,
          "fn": 657,
          "accuracy": 0.72625
        },
        "0.01": {
          "tp": 679,
          "fn": 1721,
          "accuracy": 0.28291666666666665
        }
      },
      "auroc": 0.9624201388888889
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4661,
          "fn": 2139,
          "accuracy": 0.6854411764705882
        },
        "0.01": {
          "tp": 1974,
          "fn": 4826,
          "accuracy": 0.2902941176470588
        }
      },
      "auroc": 0.9538285539215686
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943135416666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9942520833333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9942828125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        }
      },
      "auroc": 0.9547020833333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9917781250000001
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        }
      },
      "auroc": 0.9732401041666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        }
      },
      "auroc": 0.9745078125000001
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9930151041666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 765,
          "fn": 35,
          "accuracy": 0.95625
        },
        "0.01": {
          "tp": 718,
          "fn": 82,
          "accuracy": 0.8975
        }
      },
      "auroc": 0.9837614583333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.991784375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9930796875000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9930796875000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9937273437499999
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9911927083333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9937822916666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        }
      },
      "auroc": 0.9924875000000001
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9927838541666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9940786458333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 771,
          "fn": 29,
          "accuracy": 0.96375
        }
      },
      "auroc": 0.99343125
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9941489583333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9942619791666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.9298916666666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9821135416666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        }
      },
      "auroc": 0.9560026041666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9621333333333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        }
      },
      "auroc": 0.98813125
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 738,
          "fn": 62,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 679,
          "fn": 121,
          "accuracy": 0.84875
        }
      },
      "auroc": 0.9751322916666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9943653645833334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9892822916666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9892822916666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.991809375
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.991809375
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9862916666666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9862916666666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9725354166666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9725354166666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9794135416666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9794135416666667
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9923927083333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9923927083333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9920052083333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9920052083333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9921989583333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9921989583333334
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2188,
          "fn": 12,
          "accuracy": 0.9945454545454545
        },
        "0.01": {
          "tp": 2164,
          "fn": 36,
          "accuracy": 0.9836363636363636
        }
      },
      "auroc": 0.9934508522727272
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1196,
          "fn": 4,
          "accuracy": 0.9966666666666667
        }
      },
      "auroc": 0.9943168402777777
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3388,
          "fn": 12,
          "accuracy": 0.9964705882352941
        },
        "0.01": {
          "tp": 3360,
          "fn": 40,
          "accuracy": 0.9882352941176471
        }
      },
      "auroc": 0.9937564950980392
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2089,
          "fn": 111,
          "accuracy": 0.9495454545454546
        },
        "0.01": {
          "tp": 1972,
          "fn": 228,
          "accuracy": 0.8963636363636364
        }
      },
      "auroc": 0.9819461174242424
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1183,
          "fn": 17,
          "accuracy": 0.9858333333333333
        },
        "0.01": {
          "tp": 1145,
          "fn": 55,
          "accuracy": 0.9541666666666667
        }
      },
      "auroc": 0.9913680555555556
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3272,
          "fn": 128,
          "accuracy": 0.9623529411764706
        },
        "0.01": {
          "tp": 3117,
          "fn": 283,
          "accuracy": 0.9167647058823529
        }
      },
      "auroc": 0.9852715073529411
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 4277,
          "fn": 123,
          "accuracy": 0.9720454545454545
        },
        "0.01": {
          "tp": 4136,
          "fn": 264,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9876984848484848
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2383,
          "fn": 17,
          "accuracy": 0.9929166666666667
        },
        "0.01": {
          "tp": 2341,
          "fn": 59,
          "accuracy": 0.9754166666666667
        }
      },
      "auroc": 0.9928424479166666
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 6660,
          "fn": 140,
          "accuracy": 0.9794117647058823
        },
        "0.01": {
          "tp": 6477,
          "fn": 323,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9895140012254902
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9942520833333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943135416666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        }
      },
      "auroc": 0.9420072916666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9918
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        }
      },
      "auroc": 0.9669036458333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 320,
          "fn": 80,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9681911458333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9930260416666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 755,
          "fn": 45,
          "accuracy": 0.94375
        },
        "0.01": {
          "tp": 707,
          "fn": 93,
          "accuracy": 0.88375
        }
      },
      "auroc": 0.9806085937500001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9915375000000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9929562500000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9929369791666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 791,
          "fn": 9,
          "accuracy": 0.98875
        }
      },
      "auroc": 0.9936559895833333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9887812500000001
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9935552083333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.9911682291666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        }
      },
      "auroc": 0.991578125
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9939651041666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 751,
          "fn": 49,
          "accuracy": 0.93875
        }
      },
      "auroc": 0.9927716145833333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9935364583333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9939557291666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        }
      },
      "auroc": 0.9118260416666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9764041666666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        },
        "0.01": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        }
      },
      "auroc": 0.9441151041666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        }
      },
      "auroc": 0.9531005208333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.9849703125
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 717,
          "fn": 83,
          "accuracy": 0.89625
        },
        "0.01": {
          "tp": 655,
          "fn": 145,
          "accuracy": 0.81875
        }
      },
      "auroc": 0.9690354166666668
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9943653645833334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9888208333333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9888208333333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9915979166666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9915979166666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9838083333333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9838083333333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9725989583333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9725989583333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        }
      },
      "auroc": 0.9782036458333334
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        }
      },
      "auroc": 0.9782036458333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.994375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943364583333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9943557291666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9925010416666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9925010416666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.992234375
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.992234375
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9923677083333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9923677083333333
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2186,
          "fn": 14,
          "accuracy": 0.9936363636363637
        },
        "0.01": {
          "tp": 2165,
          "fn": 35,
          "accuracy": 0.9840909090909091
        }
      },
      "auroc": 0.9932440340909091
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        },
        "0.01": {
          "tp": 1189,
          "fn": 11,
          "accuracy": 0.9908333333333333
        }
      },
      "auroc": 0.9942083333333334
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3384,
          "fn": 16,
          "accuracy": 0.9952941176470588
        },
        "0.01": {
          "tp": 3354,
          "fn": 46,
          "accuracy": 0.9864705882352941
        }
      },
      "auroc": 0.993584375
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2058,
          "fn": 142,
          "accuracy": 0.9354545454545454
        },
        "0.01": {
          "tp": 1926,
          "fn": 274,
          "accuracy": 0.8754545454545455
        }
      },
      "auroc": 0.9789151515151515
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1179,
          "fn": 21,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 1134,
          "fn": 66,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9903411458333333
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3237,
          "fn": 163,
          "accuracy": 0.9520588235294117
        },
        "0.01": {
          "tp": 3060,
          "fn": 340,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9829478553921569
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 4244,
          "fn": 156,
          "accuracy": 0.9645454545454546
        },
        "0.01": {
          "tp": 4091,
          "fn": 309,
          "accuracy": 0.9297727272727273
        }
      },
      "auroc": 0.9860795928030304
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2377,
          "fn": 23,
          "accuracy": 0.9904166666666666
        },
        "0.01": {
          "tp": 2323,
          "fn": 77,
          "accuracy": 0.9679166666666666
        }
      },
      "auroc": 0.9922747395833333
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 6621,
          "fn": 179,
          "accuracy": 0.9736764705882353
        },
        "0.01": {
          "tp": 6414,
          "fn": 386,
          "accuracy": 0.9432352941176471
        }
      },
      "auroc": 0.9882661151960784
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.9586666666666668
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.9514520833333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        },
        "0.01": {
          "tp": 25,
          "fn": 375,
          "accuracy": 0.0625
        }
      },
      "auroc": 0.955059375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.9550322916666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.9469541666666665
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        },
        "0.01": {
          "tp": 15,
          "fn": 385,
          "accuracy": 0.0375
        }
      },
      "auroc": 0.9509932291666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        },
        "0.01": {
          "tp": 25,
          "fn": 375,
          "accuracy": 0.0625
        }
      },
      "auroc": 0.9568494791666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": {
          "tp": 15,
          "fn": 385,
          "accuracy": 0.0375
        }
      },
      "auroc": 0.949203125
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 438,
          "fn": 362,
          "accuracy": 0.5475
        },
        "0.01": {
          "tp": 40,
          "fn": 760,
          "accuracy": 0.05
        }
      },
      "auroc": 0.9530263020833333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.9581010416666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.9601864583333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 51,
          "fn": 349,
          "accuracy": 0.1275
        }
      },
      "auroc": 0.95914375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.9153552083333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.9554104166666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        },
        "0.01": {
          "tp": 48,
          "fn": 352,
          "accuracy": 0.12
        }
      },
      "auroc": 0.9353828125000001
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 51,
          "fn": 349,
          "accuracy": 0.1275
        }
      },
      "auroc": 0.936728125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 48,
          "fn": 352,
          "accuracy": 0.12
        }
      },
      "auroc": 0.9577984375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 436,
          "fn": 364,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 99,
          "fn": 701,
          "accuracy": 0.12375
        }
      },
      "auroc": 0.9472632812499999
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.9552083333333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.9450989583333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        },
        "0.01": {
          "tp": 16,
          "fn": 384,
          "accuracy": 0.04
        }
      },
      "auroc": 0.9501536458333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.9472875000000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.9322781250000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 244,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 11,
          "fn": 389,
          "accuracy": 0.0275
        }
      },
      "auroc": 0.9397828125000002
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 21,
          "fn": 379,
          "accuracy": 0.0525
        }
      },
      "auroc": 0.9512479166666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 259,
          "accuracy": 0.3525
        },
        "0.01": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9386885416666668
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 447,
          "accuracy": 0.44125
        },
        "0.01": {
          "tp": 27,
          "fn": 773,
          "accuracy": 0.03375
        }
      },
      "auroc": 0.9449682291666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        }
      },
      "auroc": 0.968634375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.9455979166666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 232,
          "fn": 168,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 53,
          "fn": 347,
          "accuracy": 0.1325
        }
      },
      "auroc": 0.9571161458333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9147208333333332
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.9276895833333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 91,
          "fn": 309,
          "accuracy": 0.2275
        },
        "0.01": {
          "tp": 7,
          "fn": 393,
          "accuracy": 0.0175
        }
      },
      "auroc": 0.9212052083333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        },
        "0.01": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        }
      },
      "auroc": 0.9416776041666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 267,
          "accuracy": 0.3325
        },
        "0.01": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        }
      },
      "auroc": 0.93664375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 477,
          "accuracy": 0.40375
        },
        "0.01": {
          "tp": 60,
          "fn": 740,
          "accuracy": 0.075
        }
      },
      "auroc": 0.9391606770833334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.9449572916666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.9287177083333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 266,
          "accuracy": 0.335
        },
        "0.01": {
          "tp": 41,
          "fn": 359,
          "accuracy": 0.1025
        }
      },
      "auroc": 0.9368375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.90691875
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        },
        "0.01": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        }
      },
      "auroc": 0.9349479166666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 289,
          "accuracy": 0.2775
        },
        "0.01": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        }
      },
      "auroc": 0.9209333333333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 265,
          "accuracy": 0.3375
        },
        "0.01": {
          "tp": 46,
          "fn": 354,
          "accuracy": 0.115
        }
      },
      "auroc": 0.9259380208333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 290,
          "accuracy": 0.275
        },
        "0.01": {
          "tp": 49,
          "fn": 351,
          "accuracy": 0.1225
        }
      },
      "auroc": 0.9318328124999999
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 245,
          "fn": 555,
          "accuracy": 0.30625
        },
        "0.01": {
          "tp": 95,
          "fn": 705,
          "accuracy": 0.11875
        }
      },
      "auroc": 0.9288854166666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.9640906249999999
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.9535333333333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        },
        "0.01": {
          "tp": 26,
          "fn": 374,
          "accuracy": 0.065
        }
      },
      "auroc": 0.9588119791666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.9470916666666668
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.9416885416666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 237,
          "accuracy": 0.4075
        },
        "0.01": {
          "tp": 16,
          "fn": 384,
          "accuracy": 0.04
        }
      },
      "auroc": 0.9443901041666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 235,
          "fn": 165,
          "accuracy": 0.5875
        },
        "0.01": {
          "tp": 32,
          "fn": 368,
          "accuracy": 0.08
        }
      },
      "auroc": 0.9555911458333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        },
        "0.01": {
          "tp": 10,
          "fn": 390,
          "accuracy": 0.025
        }
      },
      "auroc": 0.9476109374999999
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 408,
          "fn": 392,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 42,
          "fn": 758,
          "accuracy": 0.0525
        }
      },
      "auroc": 0.9516010416666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9280156249999999
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9280156249999999
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9133541666666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9133541666666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 315,
          "accuracy": 0.2125
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9206848958333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 315,
          "accuracy": 0.2125
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9206848958333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.9152906250000001
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.9152906250000001
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9110354166666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9110354166666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 345,
          "accuracy": 0.1375
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.9131630208333332
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 345,
          "accuracy": 0.1375
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.9131630208333332
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.9563312500000001
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.9563312500000001
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.9445260416666665
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.9445260416666665
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 19,
          "fn": 381,
          "accuracy": 0.0475
        }
      },
      "auroc": 0.9504286458333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 19,
          "fn": 381,
          "accuracy": 0.0475
        }
      },
      "auroc": 0.9504286458333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.9597135416666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.9597135416666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9271135416666666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9271135416666666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        },
        "0.01": {
          "tp": 15,
          "fn": 385,
          "accuracy": 0.0375
        }
      },
      "auroc": 0.9434135416666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        },
        "0.01": {
          "tp": 15,
          "fn": 385,
          "accuracy": 0.0375
        }
      },
      "auroc": 0.9434135416666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        }
      },
      "auroc": 0.9313968750000001
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        }
      },
      "auroc": 0.9313968750000001
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.9281354166666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.9281354166666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 285,
          "accuracy": 0.2875
        },
        "0.01": {
          "tp": 17,
          "fn": 383,
          "accuracy": 0.0425
        }
      },
      "auroc": 0.9297661458333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 285,
          "accuracy": 0.2875
        },
        "0.01": {
          "tp": 17,
          "fn": 383,
          "accuracy": 0.0425
        }
      },
      "auroc": 0.9297661458333334
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1148,
          "fn": 1052,
          "accuracy": 0.5218181818181818
        },
        "0.01": {
          "tp": 189,
          "fn": 2011,
          "accuracy": 0.08590909090909091
        }
      },
      "auroc": 0.9491278409090909
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 535,
          "fn": 665,
          "accuracy": 0.44583333333333336
        },
        "0.01": {
          "tp": 60,
          "fn": 1140,
          "accuracy": 0.05
        }
      },
      "auroc": 0.9474310763888889
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1683,
          "fn": 1717,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 249,
          "fn": 3151,
          "accuracy": 0.07323529411764707
        }
      },
      "auroc": 0.9485289828431372
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 699,
          "fn": 1501,
          "accuracy": 0.31772727272727275
        },
        "0.01": {
          "tp": 96,
          "fn": 2104,
          "accuracy": 0.04363636363636364
        }
      },
      "auroc": 0.9282337121212122
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 473,
          "fn": 727,
          "accuracy": 0.39416666666666667
        },
        "0.01": {
          "tp": 74,
          "fn": 1126,
          "accuracy": 0.06166666666666667
        }
      },
      "auroc": 0.939828125
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1172,
          "fn": 2228,
          "accuracy": 0.3447058823529412
        },
        "0.01": {
          "tp": 170,
          "fn": 3230,
          "accuracy": 0.05
        }
      },
      "auroc": 0.9323258578431372
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1847,
          "fn": 2553,
          "accuracy": 0.4197727272727273
        },
        "0.01": {
          "tp": 285,
          "fn": 4115,
          "accuracy": 0.06477272727272727
        }
      },
      "auroc": 0.9386807765151516
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1008,
          "fn": 1392,
          "accuracy": 0.42
        },
        "0.01": {
          "tp": 134,
          "fn": 2266,
          "accuracy": 0.05583333333333333
        }
      },
      "auroc": 0.9436296006944443
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2855,
          "fn": 3945,
          "accuracy": 0.4198529411764706
        },
        "0.01": {
          "tp": 419,
          "fn": 6381,
          "accuracy": 0.06161764705882353
        }
      },
      "auroc": 0.9404274203431372
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2305,
          "fn": 95,
          "accuracy": 0.9604166666666667
        },
        "0.01": {
          "tp": 2106,
          "fn": 294,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.9901993055555556
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2268,
          "fn": 132,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 2076,
          "fn": 324,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9888702256944445
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4573,
          "fn": 227,
          "accuracy": 0.9527083333333334
        },
        "0.01": {
          "tp": 4182,
          "fn": 618,
          "accuracy": 0.87125
        }
      },
      "auroc": 0.9895347656250001
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2293,
          "fn": 107,
          "accuracy": 0.9554166666666667
        },
        "0.01": {
          "tp": 2102,
          "fn": 298,
          "accuracy": 0.8758333333333334
        }
      },
      "auroc": 0.9896420138888888
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2241,
          "fn": 159,
          "accuracy": 0.93375
        },
        "0.01": {
          "tp": 2045,
          "fn": 355,
          "accuracy": 0.8520833333333333
        }
      },
      "auroc": 0.9875368055555557
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4534,
          "fn": 266,
          "accuracy": 0.9445833333333333
        },
        "0.01": {
          "tp": 4147,
          "fn": 653,
          "accuracy": 0.8639583333333334
        }
      },
      "auroc": 0.9885894097222223
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4598,
          "fn": 202,
          "accuracy": 0.9579166666666666
        },
        "0.01": {
          "tp": 4208,
          "fn": 592,
          "accuracy": 0.8766666666666667
        }
      },
      "auroc": 0.9899206597222223
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4509,
          "fn": 291,
          "accuracy": 0.939375
        },
        "0.01": {
          "tp": 4121,
          "fn": 679,
          "accuracy": 0.8585416666666666
        }
      },
      "auroc": 0.988203515625
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9107,
          "fn": 493,
          "accuracy": 0.9486458333333333
        },
        "0.01": {
          "tp": 8329,
          "fn": 1271,
          "accuracy": 0.8676041666666666
        }
      },
      "auroc": 0.9890620876736111
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2292,
          "fn": 108,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 2101,
          "fn": 299,
          "accuracy": 0.8754166666666666
        }
      },
      "auroc": 0.9896007812500001
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2310,
          "fn": 90,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 2060,
          "fn": 340,
          "accuracy": 0.8583333333333333
        }
      },
      "auroc": 0.9902373263888888
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4602,
          "fn": 198,
          "accuracy": 0.95875
        },
        "0.01": {
          "tp": 4161,
          "fn": 639,
          "accuracy": 0.866875
        }
      },
      "auroc": 0.9899190538194444
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1703,
          "fn": 697,
          "accuracy": 0.7095833333333333
        },
        "0.01": {
          "tp": 1248,
          "fn": 1152,
          "accuracy": 0.52
        }
      },
      "auroc": 0.9380589409722222
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2268,
          "fn": 132,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 1939,
          "fn": 461,
          "accuracy": 0.8079166666666666
        }
      },
      "auroc": 0.9858717013888889
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3971,
          "fn": 829,
          "accuracy": 0.8272916666666666
        },
        "0.01": {
          "tp": 3187,
          "fn": 1613,
          "accuracy": 0.6639583333333333
        }
      },
      "auroc": 0.9619653211805554
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3995,
          "fn": 805,
          "accuracy": 0.8322916666666667
        },
        "0.01": {
          "tp": 3349,
          "fn": 1451,
          "accuracy": 0.6977083333333334
        }
      },
      "auroc": 0.9638298611111111
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4578,
          "fn": 222,
          "accuracy": 0.95375
        },
        "0.01": {
          "tp": 3999,
          "fn": 801,
          "accuracy": 0.833125
        }
      },
      "auroc": 0.9880545138888889
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8573,
          "fn": 1027,
          "accuracy": 0.8930208333333334
        },
        "0.01": {
          "tp": 7348,
          "fn": 2252,
          "accuracy": 0.7654166666666666
        }
      },
      "auroc": 0.9759421875000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2287,
          "fn": 113,
          "accuracy": 0.9529166666666666
        },
        "0.01": {
          "tp": 2097,
          "fn": 303,
          "accuracy": 0.87375
        }
      },
      "auroc": 0.9895133680555555
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2217,
          "fn": 183,
          "accuracy": 0.92375
        },
        "0.01": {
          "tp": 2012,
          "fn": 388,
          "accuracy": 0.8383333333333334
        }
      },
      "auroc": 0.9872538194444445
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4504,
          "fn": 296,
          "accuracy": 0.9383333333333334
        },
        "0.01": {
          "tp": 4109,
          "fn": 691,
          "accuracy": 0.8560416666666667
        }
      },
      "auroc": 0.98838359375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2251,
          "fn": 149,
          "accuracy": 0.9379166666666666
        },
        "0.01": {
          "tp": 2047,
          "fn": 353,
          "accuracy": 0.8529166666666667
        }
      },
      "auroc": 0.9880858506944445
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2155,
          "fn": 245,
          "accuracy": 0.8979166666666667
        },
        "0.01": {
          "tp": 1903,
          "fn": 497,
          "accuracy": 0.7929166666666667
        }
      },
      "auroc": 0.9823755208333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4406,
          "fn": 394,
          "accuracy": 0.9179166666666667
        },
        "0.01": {
          "tp": 3950,
          "fn": 850,
          "accuracy": 0.8229166666666666
        }
      },
      "auroc": 0.9852306857638888
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4538,
          "fn": 262,
          "accuracy": 0.9454166666666667
        },
        "0.01": {
          "tp": 4144,
          "fn": 656,
          "accuracy": 0.8633333333333333
        }
      },
      "auroc": 0.988799609375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4372,
          "fn": 428,
          "accuracy": 0.9108333333333334
        },
        "0.01": {
          "tp": 3915,
          "fn": 885,
          "accuracy": 0.815625
        }
      },
      "auroc": 0.9848146701388888
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8910,
          "fn": 690,
          "accuracy": 0.928125
        },
        "0.01": {
          "tp": 8059,
          "fn": 1541,
          "accuracy": 0.8394791666666667
        }
      },
      "auroc": 0.9868071397569446
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2337,
          "fn": 63,
          "accuracy": 0.97375
        },
        "0.01": {
          "tp": 2169,
          "fn": 231,
          "accuracy": 0.90375
        }
      },
      "auroc": 0.991147482638889
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2215,
          "fn": 185,
          "accuracy": 0.9229166666666667
        },
        "0.01": {
          "tp": 2013,
          "fn": 387,
          "accuracy": 0.83875
        }
      },
      "auroc": 0.9871987847222223
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4552,
          "fn": 248,
          "accuracy": 0.9483333333333334
        },
        "0.01": {
          "tp": 4182,
          "fn": 618,
          "accuracy": 0.87125
        }
      },
      "auroc": 0.9891731336805555
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1988,
          "fn": 412,
          "accuracy": 0.8283333333333334
        },
        "0.01": {
          "tp": 1531,
          "fn": 869,
          "accuracy": 0.6379166666666667
        }
      },
      "auroc": 0.9724763020833334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2116,
          "fn": 284,
          "accuracy": 0.8816666666666667
        },
        "0.01": {
          "tp": 1884,
          "fn": 516,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9819995659722222
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4104,
          "fn": 696,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 3415,
          "fn": 1385,
          "accuracy": 0.7114583333333333
        }
      },
      "auroc": 0.9772379340277779
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4325,
          "fn": 475,
          "accuracy": 0.9010416666666666
        },
        "0.01": {
          "tp": 3700,
          "fn": 1100,
          "accuracy": 0.7708333333333334
        }
      },
      "auroc": 0.9818118923611111
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4331,
          "fn": 469,
          "accuracy": 0.9022916666666667
        },
        "0.01": {
          "tp": 3897,
          "fn": 903,
          "accuracy": 0.811875
        }
      },
      "auroc": 0.9845991753472222
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8656,
          "fn": 944,
          "accuracy": 0.9016666666666666
        },
        "0.01": {
          "tp": 7597,
          "fn": 2003,
          "accuracy": 0.7913541666666667
        }
      },
      "auroc": 0.9832055338541666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2235,
          "fn": 165,
          "accuracy": 0.93125
        },
        "0.01": {
          "tp": 2058,
          "fn": 342,
          "accuracy": 0.8575
        }
      },
      "auroc": 0.9876058159722223
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2076,
          "fn": 324,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 1884,
          "fn": 516,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9800348090277777
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4311,
          "fn": 489,
          "accuracy": 0.898125
        },
        "0.01": {
          "tp": 3942,
          "fn": 858,
          "accuracy": 0.82125
        }
      },
      "auroc": 0.9838203125
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1465,
          "fn": 935,
          "accuracy": 0.6104166666666667
        },
        "0.01": {
          "tp": 1046,
          "fn": 1354,
          "accuracy": 0.43583333333333335
        }
      },
      "auroc": 0.9062335069444445
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1981,
          "fn": 419,
          "accuracy": 0.8254166666666667
        },
        "0.01": {
          "tp": 1641,
          "fn": 759,
          "accuracy": 0.68375
        }
      },
      "auroc": 0.96645546875
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3446,
          "fn": 1354,
          "accuracy": 0.7179166666666666
        },
        "0.01": {
          "tp": 2687,
          "fn": 2113,
          "accuracy": 0.5597916666666667
        }
      },
      "auroc": 0.9363444878472222
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3700,
          "fn": 1100,
          "accuracy": 0.7708333333333334
        },
        "0.01": {
          "tp": 3104,
          "fn": 1696,
          "accuracy": 0.6466666666666666
        }
      },
      "auroc": 0.9469196614583333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4057,
          "fn": 743,
          "accuracy": 0.8452083333333333
        },
        "0.01": {
          "tp": 3525,
          "fn": 1275,
          "accuracy": 0.734375
        }
      },
      "auroc": 0.9732451388888888
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7757,
          "fn": 1843,
          "accuracy": 0.8080208333333333
        },
        "0.01": {
          "tp": 6629,
          "fn": 2971,
          "accuracy": 0.6905208333333334
        }
      },
      "auroc": 0.960082400173611
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2332,
          "fn": 68,
          "accuracy": 0.9716666666666667
        },
        "0.01": {
          "tp": 2143,
          "fn": 257,
          "accuracy": 0.8929166666666667
        }
      },
      "auroc": 0.9912136284722223
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2276,
          "fn": 124,
          "accuracy": 0.9483333333333334
        },
        "0.01": {
          "tp": 2073,
          "fn": 327,
          "accuracy": 0.86375
        }
      },
      "auroc": 0.989404513888889
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4608,
          "fn": 192,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 4216,
          "fn": 584,
          "accuracy": 0.8783333333333333
        }
      },
      "auroc": 0.9903090711805556
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2256,
          "fn": 144,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 2060,
          "fn": 340,
          "accuracy": 0.8583333333333333
        }
      },
      "auroc": 0.9882141493055555
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2200,
          "fn": 200,
          "accuracy": 0.9166666666666666
        },
        "0.01": {
          "tp": 2017,
          "fn": 383,
          "accuracy": 0.8404166666666667
        }
      },
      "auroc": 0.9861366319444445
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4456,
          "fn": 344,
          "accuracy": 0.9283333333333333
        },
        "0.01": {
          "tp": 4077,
          "fn": 723,
          "accuracy": 0.849375
        }
      },
      "auroc": 0.9871753906249999
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4588,
          "fn": 212,
          "accuracy": 0.9558333333333333
        },
        "0.01": {
          "tp": 4203,
          "fn": 597,
          "accuracy": 0.875625
        }
      },
      "auroc": 0.9897138888888889
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4476,
          "fn": 324,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 4090,
          "fn": 710,
          "accuracy": 0.8520833333333333
        }
      },
      "auroc": 0.9877705729166667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9064,
          "fn": 536,
          "accuracy": 0.9441666666666667
        },
        "0.01": {
          "tp": 8293,
          "fn": 1307,
          "accuracy": 0.8638541666666667
        }
      },
      "auroc": 0.9887422309027778
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2154,
          "fn": 246,
          "accuracy": 0.8975
        },
        "0.01": {
          "tp": 1997,
          "fn": 403,
          "accuracy": 0.8320833333333333
        }
      },
      "auroc": 0.9830372395833333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2154,
          "fn": 246,
          "accuracy": 0.8975
        },
        "0.01": {
          "tp": 1997,
          "fn": 403,
          "accuracy": 0.8320833333333333
        }
      },
      "auroc": 0.9830372395833333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2029,
          "fn": 371,
          "accuracy": 0.8454166666666667
        },
        "0.01": {
          "tp": 1860,
          "fn": 540,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9749011284722222
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2029,
          "fn": 371,
          "accuracy": 0.8454166666666667
        },
        "0.01": {
          "tp": 1860,
          "fn": 540,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9749011284722222
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4183,
          "fn": 617,
          "accuracy": 0.8714583333333333
        },
        "0.01": {
          "tp": 3857,
          "fn": 943,
          "accuracy": 0.8035416666666667
        }
      },
      "auroc": 0.9789691840277777
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4183,
          "fn": 617,
          "accuracy": 0.8714583333333333
        },
        "0.01": {
          "tp": 3857,
          "fn": 943,
          "accuracy": 0.8035416666666667
        }
      },
      "auroc": 0.9789691840277777
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1946,
          "fn": 454,
          "accuracy": 0.8108333333333333
        },
        "0.01": {
          "tp": 1672,
          "fn": 728,
          "accuracy": 0.6966666666666667
        }
      },
      "auroc": 0.9680447916666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1946,
          "fn": 454,
          "accuracy": 0.8108333333333333
        },
        "0.01": {
          "tp": 1672,
          "fn": 728,
          "accuracy": 0.6966666666666667
        }
      },
      "auroc": 0.9680447916666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1839,
          "fn": 561,
          "accuracy": 0.76625
        },
        "0.01": {
          "tp": 1565,
          "fn": 835,
          "accuracy": 0.6520833333333333
        }
      },
      "auroc": 0.9572686631944445
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1839,
          "fn": 561,
          "accuracy": 0.76625
        },
        "0.01": {
          "tp": 1565,
          "fn": 835,
          "accuracy": 0.6520833333333333
        }
      },
      "auroc": 0.9572686631944445
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3785,
          "fn": 1015,
          "accuracy": 0.7885416666666667
        },
        "0.01": {
          "tp": 3237,
          "fn": 1563,
          "accuracy": 0.674375
        }
      },
      "auroc": 0.9626567274305555
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3785,
          "fn": 1015,
          "accuracy": 0.7885416666666667
        },
        "0.01": {
          "tp": 3237,
          "fn": 1563,
          "accuracy": 0.674375
        }
      },
      "auroc": 0.9626567274305555
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2311,
          "fn": 89,
          "accuracy": 0.9629166666666666
        },
        "0.01": {
          "tp": 2086,
          "fn": 314,
          "accuracy": 0.8691666666666666
        }
      },
      "auroc": 0.9900451388888889
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2311,
          "fn": 89,
          "accuracy": 0.9629166666666666
        },
        "0.01": {
          "tp": 2086,
          "fn": 314,
          "accuracy": 0.8691666666666666
        }
      },
      "auroc": 0.9900451388888889
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2263,
          "fn": 137,
          "accuracy": 0.9429166666666666
        },
        "0.01": {
          "tp": 2062,
          "fn": 338,
          "accuracy": 0.8591666666666666
        }
      },
      "auroc": 0.98813671875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2263,
          "fn": 137,
          "accuracy": 0.9429166666666666
        },
        "0.01": {
          "tp": 2062,
          "fn": 338,
          "accuracy": 0.8591666666666666
        }
      },
      "auroc": 0.98813671875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4574,
          "fn": 226,
          "accuracy": 0.9529166666666666
        },
        "0.01": {
          "tp": 4148,
          "fn": 652,
          "accuracy": 0.8641666666666666
        }
      },
      "auroc": 0.9890909288194444
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4574,
          "fn": 226,
          "accuracy": 0.9529166666666666
        },
        "0.01": {
          "tp": 4148,
          "fn": 652,
          "accuracy": 0.8641666666666666
        }
      },
      "auroc": 0.9890909288194444
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2311,
          "fn": 89,
          "accuracy": 0.9629166666666666
        },
        "0.01": {
          "tp": 2099,
          "fn": 301,
          "accuracy": 0.8745833333333334
        }
      },
      "auroc": 0.990246875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2311,
          "fn": 89,
          "accuracy": 0.9629166666666666
        },
        "0.01": {
          "tp": 2099,
          "fn": 301,
          "accuracy": 0.8745833333333334
        }
      },
      "auroc": 0.990246875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2128,
          "fn": 272,
          "accuracy": 0.8866666666666667
        },
        "0.01": {
          "tp": 1971,
          "fn": 429,
          "accuracy": 0.82125
        }
      },
      "auroc": 0.9822131076388889
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2128,
          "fn": 272,
          "accuracy": 0.8866666666666667
        },
        "0.01": {
          "tp": 1971,
          "fn": 429,
          "accuracy": 0.82125
        }
      },
      "auroc": 0.9822131076388889
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4439,
          "fn": 361,
          "accuracy": 0.9247916666666667
        },
        "0.01": {
          "tp": 4070,
          "fn": 730,
          "accuracy": 0.8479166666666667
        }
      },
      "auroc": 0.9862299913194444
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4439,
          "fn": 361,
          "accuracy": 0.9247916666666667
        },
        "0.01": {
          "tp": 4070,
          "fn": 730,
          "accuracy": 0.8479166666666667
        }
      },
      "auroc": 0.9862299913194444
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2113,
          "fn": 287,
          "accuracy": 0.8804166666666666
        },
        "0.01": {
          "tp": 1921,
          "fn": 479,
          "accuracy": 0.8004166666666667
        }
      },
      "auroc": 0.9808588541666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2113,
          "fn": 287,
          "accuracy": 0.8804166666666666
        },
        "0.01": {
          "tp": 1921,
          "fn": 479,
          "accuracy": 0.8004166666666667
        }
      },
      "auroc": 0.9808588541666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2100,
          "fn": 300,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 1897,
          "fn": 503,
          "accuracy": 0.7904166666666667
        }
      },
      "auroc": 0.9793176215277777
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2100,
          "fn": 300,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 1897,
          "fn": 503,
          "accuracy": 0.7904166666666667
        }
      },
      "auroc": 0.9793176215277777
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4213,
          "fn": 587,
          "accuracy": 0.8777083333333333
        },
        "0.01": {
          "tp": 3818,
          "fn": 982,
          "accuracy": 0.7954166666666667
        }
      },
      "auroc": 0.9800882378472222
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4213,
          "fn": 587,
          "accuracy": 0.8777083333333333
        },
        "0.01": {
          "tp": 3818,
          "fn": 982,
          "accuracy": 0.7954166666666667
        }
      },
      "auroc": 0.9800882378472222
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 24623,
          "fn": 1777,
          "accuracy": 0.932689393939394
        },
        "0.01": {
          "tp": 22449,
          "fn": 3951,
          "accuracy": 0.8503409090909091
        }
      },
      "auroc": 0.9865012073863636
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 13362,
          "fn": 1038,
          "accuracy": 0.9279166666666666
        },
        "0.01": {
          "tp": 12118,
          "fn": 2282,
          "accuracy": 0.8415277777777778
        }
      },
      "auroc": 0.9871665798611111
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 37985,
          "fn": 2815,
          "accuracy": 0.9310049019607843
        },
        "0.01": {
          "tp": 34567,
          "fn": 6233,
          "accuracy": 0.8472303921568628
        }
      },
      "auroc": 0.9867360447303921
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 22315,
          "fn": 4085,
          "accuracy": 0.8452651515151515
        },
        "0.01": {
          "tp": 19389,
          "fn": 7011,
          "accuracy": 0.7344318181818181
        }
      },
      "auroc": 0.9695043639520202
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 12961,
          "fn": 1439,
          "accuracy": 0.9000694444444445
        },
        "0.01": {
          "tp": 11429,
          "fn": 2971,
          "accuracy": 0.7936805555555555
        }
      },
      "auroc": 0.9817292824074073
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 35276,
          "fn": 5524,
          "accuracy": 0.8646078431372549
        },
        "0.01": {
          "tp": 30818,
          "fn": 9982,
          "accuracy": 0.755343137254902
        }
      },
      "auroc": 0.9738190410539216
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 46938,
          "fn": 5862,
          "accuracy": 0.8889772727272728
        },
        "0.01": {
          "tp": 41838,
          "fn": 10962,
          "accuracy": 0.7923863636363636
        }
      },
      "auroc": 0.9780027856691922
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 26323,
          "fn": 2477,
          "accuracy": 0.9139930555555555
        },
        "0.01": {
          "tp": 23547,
          "fn": 5253,
          "accuracy": 0.8176041666666667
        }
      },
      "auroc": 0.9844479311342593
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 73261,
          "fn": 8339,
          "accuracy": 0.8978063725490196
        },
        "0.01": {
          "tp": 65385,
          "fn": 16215,
          "accuracy": 0.8012867647058823
        }
      },
      "auroc": 0.9802775428921569
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989309375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893421875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893421875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893585937499999
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9846291666666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892437500000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9869364583333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": null
      },
      "auroc": 0.9653104166666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9845770833333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9749437500000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": null
      },
      "auroc": 0.9749697916666668
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9869104166666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 758,
          "fn": 42,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9809401041666668
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892468750000001
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9875322916666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9883895833333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9795208333333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9844479166666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893109375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9835265625
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 782,
          "fn": 18,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.98641875
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9873385416666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9883567708333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.980965625
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9886864583333332
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9848260416666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9841520833333335
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9890307291666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 791,
          "fn": 9,
          "accuracy": 0.98875
        },
        "0.01": null
      },
      "auroc": 0.98659140625
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9816625
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9832697916666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9824661458333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": null
      },
      "auroc": 0.9311166666666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9846541666666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9578854166666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": null
      },
      "auroc": 0.9563895833333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9839619791666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 737,
          "fn": 63,
          "accuracy": 0.92125
        },
        "0.01": null
      },
      "auroc": 0.97017578125
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892916666666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.988984375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9891380208333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893333333333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9891796875000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892565104166666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9840520833333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9840520833333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9797416666666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9797416666666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.981896875
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.981896875
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9650458333333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9650458333333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9312999999999999
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9312999999999999
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        },
        "0.01": null
      },
      "auroc": 0.9481729166666666
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        },
        "0.01": null
      },
      "auroc": 0.9481729166666666
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9803875
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9803875
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.979553125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.979553125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9799703125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9799703125
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9875416666666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9875416666666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9793562499999999
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9793562499999999
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9834489583333335
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9834489583333335
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2161,
          "fn": 39,
          "accuracy": 0.9822727272727273
        },
        "0.01": null
      },
      "auroc": 0.984366287878788
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1193,
          "fn": 7,
          "accuracy": 0.9941666666666666
        },
        "0.01": null
      },
      "auroc": 0.9880284722222222
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3354,
          "fn": 46,
          "accuracy": 0.9864705882352941
        },
        "0.01": null
      },
      "auroc": 0.9856588235294118
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2041,
          "fn": 159,
          "accuracy": 0.9277272727272727
        },
        "0.01": null
      },
      "auroc": 0.9731600378787878
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1172,
          "fn": 28,
          "accuracy": 0.9766666666666667
        },
        "0.01": null
      },
      "auroc": 0.9859553819444444
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3213,
          "fn": 187,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9776760416666667
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 4202,
          "fn": 198,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9787631628787878
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2365,
          "fn": 35,
          "accuracy": 0.9854166666666667
        },
        "0.01": null
      },
      "auroc": 0.9869919270833334
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 6567,
          "fn": 233,
          "accuracy": 0.965735294117647
        },
        "0.01": null
      },
      "auroc": 0.9816674325980392
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989309375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893421875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893421875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893585937499999
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9846291666666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892437500000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9869364583333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": null
      },
      "auroc": 0.9653104166666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9845770833333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9749437500000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": null
      },
      "auroc": 0.9749697916666668
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9869104166666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 758,
          "fn": 42,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9809401041666668
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892468750000001
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9875322916666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9883895833333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9795208333333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9844479166666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893109375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9835265625
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 782,
          "fn": 18,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.98641875
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9873385416666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9883567708333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.980965625
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9886864583333332
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9848260416666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9841520833333335
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9890307291666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 791,
          "fn": 9,
          "accuracy": 0.98875
        },
        "0.01": null
      },
      "auroc": 0.98659140625
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9816625
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9832697916666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9824661458333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": null
      },
      "auroc": 0.9311166666666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9846541666666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9578854166666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": null
      },
      "auroc": 0.9563895833333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9839619791666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 737,
          "fn": 63,
          "accuracy": 0.92125
        },
        "0.01": null
      },
      "auroc": 0.97017578125
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892916666666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.988984375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9891380208333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893333333333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9891796875000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892565104166666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9840520833333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9840520833333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9797416666666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9797416666666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.981896875
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.981896875
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9650458333333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9650458333333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9312999999999999
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9312999999999999
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        },
        "0.01": null
      },
      "auroc": 0.9481729166666666
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        },
        "0.01": null
      },
      "auroc": 0.9481729166666666
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9803875
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9803875
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.979553125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.979553125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9799703125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9799703125
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9875416666666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9875416666666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9793562499999999
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9793562499999999
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9834489583333335
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9834489583333335
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2161,
          "fn": 39,
          "accuracy": 0.9822727272727273
        },
        "0.01": null
      },
      "auroc": 0.984366287878788
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1193,
          "fn": 7,
          "accuracy": 0.9941666666666666
        },
        "0.01": null
      },
      "auroc": 0.9880284722222222
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3354,
          "fn": 46,
          "accuracy": 0.9864705882352941
        },
        "0.01": null
      },
      "auroc": 0.9856588235294118
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2041,
          "fn": 159,
          "accuracy": 0.9277272727272727
        },
        "0.01": null
      },
      "auroc": 0.9731600378787878
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1172,
          "fn": 28,
          "accuracy": 0.9766666666666667
        },
        "0.01": null
      },
      "auroc": 0.9859553819444444
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3213,
          "fn": 187,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9776760416666667
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 4202,
          "fn": 198,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9787631628787878
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2365,
          "fn": 35,
          "accuracy": 0.9854166666666667
        },
        "0.01": null
      },
      "auroc": 0.9869919270833334
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 6567,
          "fn": 233,
          "accuracy": 0.965735294117647
        },
        "0.01": null
      },
      "auroc": 0.9816674325980392
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9847791666666668
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892437500000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9870114583333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9749541666666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9847791666666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.9798666666666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": null
      },
      "auroc": 0.9798666666666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9870114583333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 769,
          "fn": 31,
          "accuracy": 0.96125
        },
        "0.01": null
      },
      "auroc": 0.9834390625
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892468750000001
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9878322916666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9885395833333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989309375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9788645833333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9840869791666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989278125
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.9833484375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 781,
          "fn": 19,
          "accuracy": 0.97625
        },
        "0.01": null
      },
      "auroc": 0.98631328125
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9878166666666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9885958333333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9837572916666668
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9891156249999999
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9864364583333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9857869791666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892453125
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9875161458333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9819052083333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9835364583333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9827208333333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9415875
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9869760416666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9642817708333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9617463541666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9852562500000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 753,
          "fn": 47,
          "accuracy": 0.94125
        },
        "0.01": null
      },
      "auroc": 0.9735013020833333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989309375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9889125000000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9891109375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893421875
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.98914375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.98924296875
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9845541666666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9845541666666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.98014375
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.98014375
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9823489583333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9823489583333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9663375000000001
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9663375000000001
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9333072916666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9333072916666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        },
        "0.01": null
      },
      "auroc": 0.9498223958333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        },
        "0.01": null
      },
      "auroc": 0.9498223958333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9803916666666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9803916666666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9803385416666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9803385416666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9803651041666668
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9803651041666668
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9879552083333334
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9879552083333334
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9785791666666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9785791666666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9832671875
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9832671875
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2166,
          "fn": 34,
          "accuracy": 0.9845454545454545
        },
        "0.01": null
      },
      "auroc": 0.9846464962121212
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1193,
          "fn": 7,
          "accuracy": 0.9941666666666666
        },
        "0.01": null
      },
      "auroc": 0.9881229166666666
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3359,
          "fn": 41,
          "accuracy": 0.9879411764705882
        },
        "0.01": null
      },
      "auroc": 0.985873468137255
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2065,
          "fn": 135,
          "accuracy": 0.9386363636363636
        },
        "0.01": null
      },
      "auroc": 0.9754578598484848
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1174,
          "fn": 26,
          "accuracy": 0.9783333333333334
        },
        "0.01": null
      },
      "auroc": 0.9863371527777778
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3239,
          "fn": 161,
          "accuracy": 0.9526470588235294
        },
        "0.01": null
      },
      "auroc": 0.9792976102941177
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 4231,
          "fn": 169,
          "accuracy": 0.961590909090909
        },
        "0.01": null
      },
      "auroc": 0.9800521780303031
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2367,
          "fn": 33,
          "accuracy": 0.98625
        },
        "0.01": null
      },
      "auroc": 0.9872300347222223
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 6598,
          "fn": 202,
          "accuracy": 0.9702941176470589
        },
        "0.01": null
      },
      "auroc": 0.9825855392156864
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.98918125
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989278125
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989278125
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893265625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.98474375
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9891125
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.986928125
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9682510416666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9844
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": null
      },
      "auroc": 0.9763255208333332
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9764973958333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.98675625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 759,
          "fn": 41,
          "accuracy": 0.94875
        },
        "0.01": null
      },
      "auroc": 0.9816268229166666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9891979166666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9852302083333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9872140625
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9890364583333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9762229166666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.9826296875
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9891171875
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": null
      },
      "auroc": 0.9807265625
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 774,
          "fn": 26,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9849218750000001
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9877635416666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9891895833333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9884765625
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9791520833333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9885020833333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9838270833333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9834578125000001
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9888458333333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 788,
          "fn": 12,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9861518229166666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9814416666666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9836739583333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9825578125
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9384187500000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9847625
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9615906249999999
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": null
      },
      "auroc": 0.9599302083333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9842182291666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 740,
          "fn": 60,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.97207421875
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892437500000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9885645833333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9889041666666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989309375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9889697916666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9891395833333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9838479166666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9838479166666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.978509375
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.978509375
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9811786458333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9811786458333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.959571875
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.959571875
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.9195916666666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.9195916666666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": null
      },
      "auroc": 0.9395817708333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": null
      },
      "auroc": 0.9395817708333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9806718749999999
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9806718749999999
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9787781249999999
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9787781249999999
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9797250000000001
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9797250000000001
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989309375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989309375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893421875
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893421875
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9857291666666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9857291666666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9776875
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9776875
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9817083333333334
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9817083333333334
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2146,
          "fn": 54,
          "accuracy": 0.9754545454545455
        },
        "0.01": null
      },
      "auroc": 0.9837357007575758
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1190,
          "fn": 10,
          "accuracy": 0.9916666666666667
        },
        "0.01": null
      },
      "auroc": 0.987659375
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3336,
          "fn": 64,
          "accuracy": 0.9811764705882353
        },
        "0.01": null
      },
      "auroc": 0.9851205269607843
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2034,
          "fn": 166,
          "accuracy": 0.9245454545454546
        },
        "0.01": null
      },
      "auroc": 0.9724866477272728
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1167,
          "fn": 33,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9852722222222222
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3201,
          "fn": 199,
          "accuracy": 0.9414705882352942
        },
        "0.01": null
      },
      "auroc": 0.9769992034313725
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 4180,
          "fn": 220,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9781111742424242
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2357,
          "fn": 43,
          "accuracy": 0.9820833333333333
        },
        "0.01": null
      },
      "auroc": 0.9864657986111112
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 6537,
          "fn": 263,
          "accuracy": 0.9613235294117647
        },
        "0.01": null
      },
      "auroc": 0.9810598651960784
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892437500000001
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989309375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989309375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893421875
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9842947916666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892437500000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9867692708333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.9629395833333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9840927083333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9735161458333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9736171875
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9866682291666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 756,
          "fn": 44,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9801427083333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892468750000001
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9874770833333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9883619791666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9788166666666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9840958333333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893109375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.983146875
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 783,
          "fn": 17,
          "accuracy": 0.97875
        },
        "0.01": null
      },
      "auroc": 0.98622890625
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9871895833333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9882822916666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9807479166666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9885593749999999
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9846536458333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9839687500000001
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9889671875
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.98646796875
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.981134375
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9827687500000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9819515624999999
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9248395833333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9842135416666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        },
        "0.01": null
      },
      "auroc": 0.9545265625000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        },
        "0.01": null
      },
      "auroc": 0.9529869791666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9834911458333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 733,
          "fn": 67,
          "accuracy": 0.91625
        },
        "0.01": null
      },
      "auroc": 0.9682390624999999
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892833333333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.988925
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9891041666666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893291666666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.98915
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892395833333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.98380625
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.98380625
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.979165625
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.979165625
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9814859375
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9814859375
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.962390625
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.962390625
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.9287062500000001
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.9287062500000001
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": null
      },
      "auroc": 0.9455484375000001
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": null
      },
      "auroc": 0.9455484375000001
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9800125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9800125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9795479166666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9795479166666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9797802083333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9797802083333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9871385416666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9871385416666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9784114583333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9784114583333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.982775
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.982775
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2157,
          "fn": 43,
          "accuracy": 0.9804545454545455
        },
        "0.01": null
      },
      "auroc": 0.9839398674242424
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1193,
          "fn": 7,
          "accuracy": 0.9941666666666666
        },
        "0.01": null
      },
      "auroc": 0.9879357638888889
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3350,
          "fn": 50,
          "accuracy": 0.9852941176470589
        },
        "0.01": null
      },
      "auroc": 0.9853501838235295
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2033,
          "fn": 167,
          "accuracy": 0.9240909090909091
        },
        "0.01": null
      },
      "auroc": 0.971978787878788
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1172,
          "fn": 28,
          "accuracy": 0.9766666666666667
        },
        "0.01": null
      },
      "auroc": 0.9856418402777778
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3205,
          "fn": 195,
          "accuracy": 0.9426470588235294
        },
        "0.01": null
      },
      "auroc": 0.9768010416666667
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 4190,
          "fn": 210,
          "accuracy": 0.9522727272727273
        },
        "0.01": null
      },
      "auroc": 0.977959327651515
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2365,
          "fn": 35,
          "accuracy": 0.9854166666666667
        },
        "0.01": null
      },
      "auroc": 0.9867888020833333
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 6555,
          "fn": 245,
          "accuracy": 0.9639705882352941
        },
        "0.01": null
      },
      "auroc": 0.981075612745098
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.98588125
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9839270833333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9849041666666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9860166666666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9808635416666668
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9834401041666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9859489583333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9823953125
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 776,
          "fn": 24,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9841721354166667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9759072916666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9817270833333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9788171875
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": null
      },
      "auroc": 0.9296854166666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.9581947916666668
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        },
        "0.01": null
      },
      "auroc": 0.9439401041666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": null
      },
      "auroc": 0.9527963541666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": null
      },
      "auroc": 0.9699609375
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 672,
          "fn": 128,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.9613786458333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9834072916666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9686583333333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9760328125
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9829875
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": null
      },
      "auroc": 0.9388677083333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": null
      },
      "auroc": 0.9609276041666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9831973958333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": null
      },
      "auroc": 0.9537630208333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 687,
          "fn": 113,
          "accuracy": 0.85875
        },
        "0.01": null
      },
      "auroc": 0.9684802083333335
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9784708333333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9861791666666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": null
      },
      "auroc": 0.982325
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9719166666666668
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": null
      },
      "auroc": 0.9693083333333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9725854166666668
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9790479166666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 726,
          "fn": 74,
          "accuracy": 0.9075
        },
        "0.01": null
      },
      "auroc": 0.9758166666666668
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9738843749999999
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9730885416666668
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9734864583333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": null
      },
      "auroc": 0.9134885416666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": null
      },
      "auroc": 0.9479989583333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        },
        "0.01": null
      },
      "auroc": 0.93074375
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        },
        "0.01": null
      },
      "auroc": 0.9436864583333335
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": null
      },
      "auroc": 0.96054375
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 625,
          "fn": 175,
          "accuracy": 0.78125
        },
        "0.01": null
      },
      "auroc": 0.9521151041666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.987778125
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9851447916666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9864614583333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9806864583333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9714624999999999
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9760744791666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9842322916666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9783036458333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 759,
          "fn": 41,
          "accuracy": 0.94875
        },
        "0.01": null
      },
      "auroc": 0.9812679687500001
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9749583333333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9749583333333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9704197916666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9704197916666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": null
      },
      "auroc": 0.9726890625
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": null
      },
      "auroc": 0.9726890625
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": null
      },
      "auroc": 0.916728125
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": null
      },
      "auroc": 0.916728125
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": null
      },
      "auroc": 0.8801270833333332
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": null
      },
      "auroc": 0.8801270833333332
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        },
        "0.01": null
      },
      "auroc": 0.8984276041666668
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        },
        "0.01": null
      },
      "auroc": 0.8984276041666668
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.972640625
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.972640625
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9746989583333332
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9746989583333332
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": null
      },
      "auroc": 0.9736697916666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": null
      },
      "auroc": 0.9736697916666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9871416666666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9871416666666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.974153125
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.974153125
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9806473958333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9806473958333334
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.976325
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.976325
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9622822916666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9622822916666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9693036458333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9693036458333333
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2018,
          "fn": 182,
          "accuracy": 0.9172727272727272
        },
        "0.01": null
      },
      "auroc": 0.9739202651515152
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1134,
          "fn": 66,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9797875
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3152,
          "fn": 248,
          "accuracy": 0.9270588235294117
        },
        "0.01": null
      },
      "auroc": 0.9759910539215686
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1820,
          "fn": 380,
          "accuracy": 0.8272727272727273
        },
        "0.01": null
      },
      "auroc": 0.956476893939394
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 992,
          "fn": 208,
          "accuracy": 0.8266666666666667
        },
        "0.01": null
      },
      "auroc": 0.9615506944444444
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2812,
          "fn": 588,
          "accuracy": 0.8270588235294117
        },
        "0.01": null
      },
      "auroc": 0.9582676470588236
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3838,
          "fn": 562,
          "accuracy": 0.8722727272727273
        },
        "0.01": null
      },
      "auroc": 0.9651985795454545
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2126,
          "fn": 274,
          "accuracy": 0.8858333333333334
        },
        "0.01": null
      },
      "auroc": 0.9706690972222222
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 5964,
          "fn": 836,
          "accuracy": 0.8770588235294118
        },
        "0.01": null
      },
      "auroc": 0.967129350490196
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989309375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893421875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893421875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893585937499999
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9846291666666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989309375
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9869692708333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": null
      },
      "auroc": 0.9666489583333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.984625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9756369791666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": null
      },
      "auroc": 0.9756390625000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9869671875
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 758,
          "fn": 42,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9813031249999999
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892468750000001
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9875979166666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9884223958333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9795260416666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9844505208333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893109375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9835619791666668
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 782,
          "fn": 18,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9864364583333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9871895833333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9882822916666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9809874999999999
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9886864583333332
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9848369791666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9840885416666668
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9890307291666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 791,
          "fn": 9,
          "accuracy": 0.98875
        },
        "0.01": null
      },
      "auroc": 0.9865596354166666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9816447916666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9832697916666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9824572916666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": null
      },
      "auroc": 0.93115
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.984771875
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": null
      },
      "auroc": 0.9579609375
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": null
      },
      "auroc": 0.9563973958333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9840208333333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 738,
          "fn": 62,
          "accuracy": 0.9225
        },
        "0.01": null
      },
      "auroc": 0.9702091145833334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892916666666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.988984375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9891380208333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893333333333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9891796875000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892565104166666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9840520833333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9840520833333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9797416666666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9797416666666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.981896875
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.981896875
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9650458333333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9650458333333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.931278125
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.931278125
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        },
        "0.01": null
      },
      "auroc": 0.9481619791666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        },
        "0.01": null
      },
      "auroc": 0.9481619791666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9803875
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9803875
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.979553125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.979553125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9799703125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9799703125
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9875416666666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9875416666666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9793562499999999
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9793562499999999
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9834489583333335
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9834489583333335
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2161,
          "fn": 39,
          "accuracy": 0.9822727272727273
        },
        "0.01": null
      },
      "auroc": 0.9843511363636364
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1193,
          "fn": 7,
          "accuracy": 0.9941666666666666
        },
        "0.01": null
      },
      "auroc": 0.9880503472222222
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3354,
          "fn": 46,
          "accuracy": 0.9864705882352941
        },
        "0.01": null
      },
      "auroc": 0.9856567401960785
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2041,
          "fn": 159,
          "accuracy": 0.9277272727272727
        },
        "0.01": null
      },
      "auroc": 0.9732847537878788
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1173,
          "fn": 27,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9859838541666667
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3214,
          "fn": 186,
          "accuracy": 0.9452941176470588
        },
        "0.01": null
      },
      "auroc": 0.9777667892156863
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 4202,
          "fn": 198,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9788179450757575
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2366,
          "fn": 34,
          "accuracy": 0.9858333333333333
        },
        "0.01": null
      },
      "auroc": 0.9870171006944444
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 6568,
          "fn": 232,
          "accuracy": 0.9658823529411765
        },
        "0.01": null
      },
      "auroc": 0.9817117647058824
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989309375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893421875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893421875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893585937499999
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9846291666666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892437500000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9869364583333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.9652145833333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9846166666666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": null
      },
      "auroc": 0.9749156250000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.974921875
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9869302083333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 757,
          "fn": 43,
          "accuracy": 0.94625
        },
        "0.01": null
      },
      "auroc": 0.9809260416666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892468750000001
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9875322916666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9883895833333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9792510416666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9843130208333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893109375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.9833916666666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 781,
          "fn": 19,
          "accuracy": 0.97625
        },
        "0.01": null
      },
      "auroc": 0.9863513020833333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9873427083333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9883588541666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.980965625
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9887041666666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9848348958333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9841541666666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9890395833333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 791,
          "fn": 9,
          "accuracy": 0.98875
        },
        "0.01": null
      },
      "auroc": 0.986596875
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9816447916666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9832697916666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9824572916666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": null
      },
      "auroc": 0.9311166666666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9847645833333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": null
      },
      "auroc": 0.957940625
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": null
      },
      "auroc": 0.9563807291666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9840171875000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 738,
          "fn": 62,
          "accuracy": 0.9225
        },
        "0.01": null
      },
      "auroc": 0.9701989583333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892916666666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.988984375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9891380208333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893333333333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9891796875000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892565104166666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9840520833333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9840520833333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9797416666666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9797416666666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.981896875
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.981896875
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9650458333333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9650458333333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9312999999999999
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9312999999999999
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        },
        "0.01": null
      },
      "auroc": 0.9481729166666666
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        },
        "0.01": null
      },
      "auroc": 0.9481729166666666
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9803875
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9803875
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.979553125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.979553125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9799703125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9799703125
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9875416666666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9875416666666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9793562499999999
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9793562499999999
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9834489583333335
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9834489583333335
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2161,
          "fn": 39,
          "accuracy": 0.9822727272727273
        },
        "0.01": null
      },
      "auroc": 0.9843650568181819
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1193,
          "fn": 7,
          "accuracy": 0.9941666666666666
        },
        "0.01": null
      },
      "auroc": 0.9880284722222222
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3354,
          "fn": 46,
          "accuracy": 0.9864705882352941
        },
        "0.01": null
      },
      "auroc": 0.9856580269607844
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2040,
          "fn": 160,
          "accuracy": 0.9272727272727272
        },
        "0.01": null
      },
      "auroc": 0.9731513257575757
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1172,
          "fn": 28,
          "accuracy": 0.9766666666666667
        },
        "0.01": null
      },
      "auroc": 0.9859383680555556
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3212,
          "fn": 188,
          "accuracy": 0.9447058823529412
        },
        "0.01": null
      },
      "auroc": 0.977664399509804
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 4201,
          "fn": 199,
          "accuracy": 0.9547727272727272
        },
        "0.01": null
      },
      "auroc": 0.9787581912878788
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2365,
          "fn": 35,
          "accuracy": 0.9854166666666667
        },
        "0.01": null
      },
      "auroc": 0.9869834201388888
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 6566,
          "fn": 234,
          "accuracy": 0.9655882352941176
        },
        "0.01": null
      },
      "auroc": 0.9816612132352942
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9868989583333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9837854166666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9853421875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.985984375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9831697916666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9845770833333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9864416666666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9834776041666666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 779,
          "fn": 21,
          "accuracy": 0.97375
        },
        "0.01": null
      },
      "auroc": 0.9849596354166666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9813322916666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.982753125
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9820427083333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        },
        "0.01": null
      },
      "auroc": 0.9293156249999999
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": null
      },
      "auroc": 0.9638614583333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        },
        "0.01": null
      },
      "auroc": 0.9465885416666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.9553239583333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9733072916666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 640,
          "fn": 160,
          "accuracy": 0.8
        },
        "0.01": null
      },
      "auroc": 0.964315625
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.983934375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.9367125000000001
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9603234375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9786697916666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        },
        "0.01": null
      },
      "auroc": 0.9064927083333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 242,
          "fn": 158,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.94258125
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9813020833333332
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 243,
          "accuracy": 0.3925
        },
        "0.01": null
      },
      "auroc": 0.9216026041666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 536,
          "fn": 264,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.9514523437499999
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9862281249999999
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9879364583333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9870822916666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": null
      },
      "auroc": 0.9337458333333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        },
        "0.01": null
      },
      "auroc": 0.9317864583333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        },
        "0.01": null
      },
      "auroc": 0.9327661458333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        },
        "0.01": null
      },
      "auroc": 0.9599869791666668
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        },
        "0.01": null
      },
      "auroc": 0.9598614583333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 589,
          "fn": 211,
          "accuracy": 0.73625
        },
        "0.01": null
      },
      "auroc": 0.9599242187500001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9788479166666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9805364583333332
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9796921875
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": null
      },
      "auroc": 0.9072645833333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": null
      },
      "auroc": 0.9365552083333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 222,
          "accuracy": 0.445
        },
        "0.01": null
      },
      "auroc": 0.9219098958333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        },
        "0.01": null
      },
      "auroc": 0.9430562499999999
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": null
      },
      "auroc": 0.9585458333333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 562,
          "fn": 238,
          "accuracy": 0.7025
        },
        "0.01": null
      },
      "auroc": 0.9508010416666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9879104166666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9817781250000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9848442708333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9828677083333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.9599354166666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": null
      },
      "auroc": 0.9714015625
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9853890625
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9708567708333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 729,
          "fn": 71,
          "accuracy": 0.91125
        },
        "0.01": null
      },
      "auroc": 0.9781229166666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9748354166666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9748354166666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.96355
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.96355
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9691927083333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9691927083333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": null
      },
      "auroc": 0.9303979166666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": null
      },
      "auroc": 0.9303979166666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.921209375
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.921209375
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        },
        "0.01": null
      },
      "auroc": 0.9258036458333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        },
        "0.01": null
      },
      "auroc": 0.9258036458333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9830208333333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9830208333333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9764760416666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9764760416666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.9797484375000001
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.9797484375000001
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9878572916666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9878572916666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9733572916666666
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9733572916666666
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.9806072916666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.9806072916666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9676781250000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9676781250000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": null
      },
      "auroc": 0.9483979166666666
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": null
      },
      "auroc": 0.9483979166666666
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": null
      },
      "auroc": 0.9580380208333334
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": null
      },
      "auroc": 0.9580380208333334
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2035,
          "fn": 165,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.977176515151515
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1069,
          "fn": 131,
          "accuracy": 0.8908333333333334
        },
        "0.01": null
      },
      "auroc": 0.9755836805555556
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3104,
          "fn": 296,
          "accuracy": 0.9129411764705883
        },
        "0.01": null
      },
      "auroc": 0.9766143382352942
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1608,
          "fn": 592,
          "accuracy": 0.730909090909091
        },
        "0.01": null
      },
      "auroc": 0.9546216856060606
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 767,
          "fn": 433,
          "accuracy": 0.6391666666666667
        },
        "0.01": null
      },
      "auroc": 0.9469668402777778
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2375,
          "fn": 1025,
          "accuracy": 0.6985294117647058
        },
        "0.01": null
      },
      "auroc": 0.9519199754901959
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3643,
          "fn": 757,
          "accuracy": 0.8279545454545455
        },
        "0.01": null
      },
      "auroc": 0.9658991003787878
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1836,
          "fn": 564,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.9612752604166667
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5479,
          "fn": 1321,
          "accuracy": 0.8057352941176471
        },
        "0.01": null
      },
      "auroc": 0.9642671568627451
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989309375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893421875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893421875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893585937499999
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9846833333333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892437500000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9869635416666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.967440625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9845791666666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9760098958333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.9760619791666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9869114583333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 764,
          "fn": 36,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.98148671875
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892166666666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9876822916666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9884494791666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9797104166666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9845427083333335
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892958333333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9836963541666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 783,
          "fn": 17,
          "accuracy": 0.97875
        },
        "0.01": null
      },
      "auroc": 0.98649609375
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9876697916666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989309375
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9884895833333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.982815625
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9888354166666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9858255208333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9852427083333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9890723958333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 793,
          "fn": 7,
          "accuracy": 0.99125
        },
        "0.01": null
      },
      "auroc": 0.9871575520833333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9820635416666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.983521875
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9827927083333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.93195625
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.98605625
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": null
      },
      "auroc": 0.9590062500000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9570098958333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9847890625
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 744,
          "fn": 56,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9708994791666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892833333333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9889895833333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9891364583333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893291666666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9891822916666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892557291666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9838125000000001
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9838125000000001
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9796864583333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9796864583333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9817494791666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9817494791666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9626520833333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9626520833333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.9256968750000001
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.9256968750000001
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": null
      },
      "auroc": 0.9441744791666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": null
      },
      "auroc": 0.9441744791666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9808625
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9808625
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9799375
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9799375
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9804
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9804
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9876885416666666
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9876885416666666
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9789791666666666
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9789791666666666
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9833338541666666
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9833338541666666
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2159,
          "fn": 41,
          "accuracy": 0.9813636363636363
        },
        "0.01": null
      },
      "auroc": 0.9842521780303031
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1193,
          "fn": 7,
          "accuracy": 0.9941666666666666
        },
        "0.01": null
      },
      "auroc": 0.9880845486111112
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3352,
          "fn": 48,
          "accuracy": 0.9858823529411764
        },
        "0.01": null
      },
      "auroc": 0.9856047794117647
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2050,
          "fn": 150,
          "accuracy": 0.9318181818181818
        },
        "0.01": null
      },
      "auroc": 0.973083712121212
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1175,
          "fn": 25,
          "accuracy": 0.9791666666666666
        },
        "0.01": null
      },
      "auroc": 0.9862467013888889
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3225,
          "fn": 175,
          "accuracy": 0.9485294117647058
        },
        "0.01": null
      },
      "auroc": 0.9777294730392158
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 4209,
          "fn": 191,
          "accuracy": 0.956590909090909
        },
        "0.01": null
      },
      "auroc": 0.9786679450757576
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2368,
          "fn": 32,
          "accuracy": 0.9866666666666667
        },
        "0.01": null
      },
      "auroc": 0.987165625
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 6577,
          "fn": 223,
          "accuracy": 0.9672058823529411
        },
        "0.01": null
      },
      "auroc": 0.9816671262254901
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989309375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893421875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893421875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893585937499999
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9845447916666668
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892437500000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9868942708333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.9646427083333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9845416666666668
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": null
      },
      "auroc": 0.9745921874999999
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.97459375
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9868927083333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 757,
          "fn": 43,
          "accuracy": 0.94625
        },
        "0.01": null
      },
      "auroc": 0.9807432291666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892468750000001
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9875458333333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9883963541666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9788197916666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9840973958333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893109375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.9831828125000001
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 781,
          "fn": 19,
          "accuracy": 0.97625
        },
        "0.01": null
      },
      "auroc": 0.986246875
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9873427083333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9883588541666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9805635416666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.988646875
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9846052083333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9839531250000001
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9890109375
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 791,
          "fn": 9,
          "accuracy": 0.98875
        },
        "0.01": null
      },
      "auroc": 0.98648203125
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9813937500000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9830041666666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9821989583333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": null
      },
      "auroc": 0.9301291666666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.98445
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9572895833333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9557614583333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9837270833333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 735,
          "fn": 65,
          "accuracy": 0.91875
        },
        "0.01": null
      },
      "auroc": 0.9697442708333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892729166666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.988984375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9891286458333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9893239583333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9891796875000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892518229166667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.983803125
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.983803125
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9791135416666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9791135416666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9814583333333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9814583333333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9640958333333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9640958333333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.9290375
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.9290375
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        },
        "0.01": null
      },
      "auroc": 0.9465666666666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        },
        "0.01": null
      },
      "auroc": 0.9465666666666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9801375000000001
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9801375000000001
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9793562499999999
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9793562499999999
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.979746875
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.979746875
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.989375
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9874375
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9874375
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9788843749999999
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9788843749999999
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9831609375000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9831609375000001
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2160,
          "fn": 40,
          "accuracy": 0.9818181818181818
        },
        "0.01": null
      },
      "auroc": 0.9841933712121211
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1193,
          "fn": 7,
          "accuracy": 0.9941666666666666
        },
        "0.01": null
      },
      "auroc": 0.9879864583333333
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3353,
          "fn": 47,
          "accuracy": 0.9861764705882353
        },
        "0.01": null
      },
      "auroc": 0.9855321078431373
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2036,
          "fn": 164,
          "accuracy": 0.9254545454545454
        },
        "0.01": null
      },
      "auroc": 0.9726477272727272
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1170,
          "fn": 30,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9857920138888889
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3206,
          "fn": 194,
          "accuracy": 0.9429411764705883
        },
        "0.01": null
      },
      "auroc": 0.977286887254902
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 4196,
          "fn": 204,
          "accuracy": 0.9536363636363636
        },
        "0.01": null
      },
      "auroc": 0.9784205492424243
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2363,
          "fn": 37,
          "accuracy": 0.9845833333333334
        },
        "0.01": null
      },
      "auroc": 0.9868892361111112
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 6559,
          "fn": 241,
          "accuracy": 0.9645588235294118
        },
        "0.01": null
      },
      "auroc": 0.9814094975490196
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9803343750000001
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.974384375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": null
      },
      "auroc": 0.977359375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9787343749999999
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9721447916666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": null
      },
      "auroc": 0.9754395833333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9795343750000001
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": null
      },
      "auroc": 0.9732645833333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 738,
          "fn": 62,
          "accuracy": 0.9225
        },
        "0.01": null
      },
      "auroc": 0.9763994791666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9787572916666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.9679208333333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": null
      },
      "auroc": 0.9733390625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": null
      },
      "auroc": 0.9294895833333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": null
      },
      "auroc": 0.9466614583333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        },
        "0.01": null
      },
      "auroc": 0.9380755208333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.9541234375000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        },
        "0.01": null
      },
      "auroc": 0.9572911458333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 539,
          "fn": 261,
          "accuracy": 0.67375
        },
        "0.01": null
      },
      "auroc": 0.9557072916666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9733572916666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        },
        "0.01": null
      },
      "auroc": 0.9291989583333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        },
        "0.01": null
      },
      "auroc": 0.951278125
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.9630583333333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        },
        "0.01": null
      },
      "auroc": 0.912146875
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        },
        "0.01": null
      },
      "auroc": 0.9376026041666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": null
      },
      "auroc": 0.9682078125
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 308,
          "accuracy": 0.23
        },
        "0.01": null
      },
      "auroc": 0.9206729166666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 437,
          "fn": 363,
          "accuracy": 0.54625
        },
        "0.01": null
      },
      "auroc": 0.9444403645833334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9813999999999999
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9816458333333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9815229166666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": null
      },
      "auroc": 0.9282979166666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": null
      },
      "auroc": 0.9283864583333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 270,
          "accuracy": 0.325
        },
        "0.01": null
      },
      "auroc": 0.9283421875
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.9548489583333335
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 253,
          "fn": 147,
          "accuracy": 0.6325
        },
        "0.01": null
      },
      "auroc": 0.9550161458333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 523,
          "fn": 277,
          "accuracy": 0.65375
        },
        "0.01": null
      },
      "auroc": 0.9549325520833334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9767802083333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.975828125
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": null
      },
      "auroc": 0.9763041666666668
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        },
        "0.01": null
      },
      "auroc": 0.9143041666666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        },
        "0.01": null
      },
      "auroc": 0.9308583333333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 279,
          "accuracy": 0.3025
        },
        "0.01": null
      },
      "auroc": 0.92258125
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        },
        "0.01": null
      },
      "auroc": 0.9455421875000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 243,
          "fn": 157,
          "accuracy": 0.6075
        },
        "0.01": null
      },
      "auroc": 0.9533432291666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 482,
          "fn": 318,
          "accuracy": 0.6025
        },
        "0.01": null
      },
      "auroc": 0.9494427083333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9801229166666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.9660187500000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9730708333333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9698687500000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": null
      },
      "auroc": 0.9464614583333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 278,
          "fn": 122,
          "accuracy": 0.695
        },
        "0.01": null
      },
      "auroc": 0.9581651041666668
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9749958333333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        },
        "0.01": null
      },
      "auroc": 0.9562401041666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 636,
          "fn": 164,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.96561796875
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.9629458333333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.9629458333333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": null
      },
      "auroc": 0.95579375
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": null
      },
      "auroc": 0.95579375
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        },
        "0.01": null
      },
      "auroc": 0.9593697916666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        },
        "0.01": null
      },
      "auroc": 0.9593697916666666
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": null
      },
      "auroc": 0.9336604166666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": null
      },
      "auroc": 0.9336604166666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        },
        "0.01": null
      },
      "auroc": 0.9280697916666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        },
        "0.01": null
      },
      "auroc": 0.9280697916666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 222,
          "accuracy": 0.445
        },
        "0.01": null
      },
      "auroc": 0.9308651041666666
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 222,
          "accuracy": 0.445
        },
        "0.01": null
      },
      "auroc": 0.9308651041666666
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.977928125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.977928125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9688197916666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9688197916666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": null
      },
      "auroc": 0.9733739583333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": null
      },
      "auroc": 0.9733739583333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9825302083333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9825302083333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.958609375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.958609375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": null
      },
      "auroc": 0.9705697916666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": null
      },
      "auroc": 0.9705697916666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": null
      },
      "auroc": 0.9585864583333332
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": null
      },
      "auroc": 0.9585864583333332
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": null
      },
      "auroc": 0.9433458333333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": null
      },
      "auroc": 0.9433458333333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        },
        "0.01": null
      },
      "auroc": 0.9509661458333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        },
        "0.01": null
      },
      "auroc": 0.9509661458333333
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1916,
          "fn": 284,
          "accuracy": 0.8709090909090909
        },
        "0.01": null
      },
      "auroc": 0.9714911931818182
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 929,
          "fn": 271,
          "accuracy": 0.7741666666666667
        },
        "0.01": null
      },
      "auroc": 0.9658328125
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2845,
          "fn": 555,
          "accuracy": 0.836764705882353
        },
        "0.01": null
      },
      "auroc": 0.9694941176470588
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1392,
          "fn": 808,
          "accuracy": 0.6327272727272727
        },
        "0.01": null
      },
      "auroc": 0.9489446969696969
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 556,
          "fn": 644,
          "accuracy": 0.4633333333333333
        },
        "0.01": null
      },
      "auroc": 0.9394432291666667
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1948,
          "fn": 1452,
          "accuracy": 0.5729411764705883
        },
        "0.01": null
      },
      "auroc": 0.9455912377450979
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3308,
          "fn": 1092,
          "accuracy": 0.7518181818181818
        },
        "0.01": null
      },
      "auroc": 0.9602179450757576
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1485,
          "fn": 915,
          "accuracy": 0.61875
        },
        "0.01": null
      },
      "auroc": 0.9526380208333334
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4793,
          "fn": 2007,
          "accuracy": 0.7048529411764706
        },
        "0.01": null
      },
      "auroc": 0.9575426776960786
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2384,
          "fn": 16,
          "accuracy": 0.9933333333333333
        },
        "0.01": null
      },
      "auroc": 0.9881241319444445
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2367,
          "fn": 33,
          "accuracy": 0.98625
        },
        "0.01": null
      },
      "auroc": 0.9872059895833334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4751,
          "fn": 49,
          "accuracy": 0.9897916666666666
        },
        "0.01": null
      },
      "auroc": 0.9876650607638889
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2377,
          "fn": 23,
          "accuracy": 0.9904166666666666
        },
        "0.01": null
      },
      "auroc": 0.9879258680555556
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2365,
          "fn": 35,
          "accuracy": 0.9854166666666667
        },
        "0.01": null
      },
      "auroc": 0.9866528645833332
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4742,
          "fn": 58,
          "accuracy": 0.9879166666666667
        },
        "0.01": null
      },
      "auroc": 0.9872893663194444
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4761,
          "fn": 39,
          "accuracy": 0.991875
        },
        "0.01": null
      },
      "auroc": 0.9880249999999999
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4732,
          "fn": 68,
          "accuracy": 0.9858333333333333
        },
        "0.01": null
      },
      "auroc": 0.9869294270833333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9493,
          "fn": 107,
          "accuracy": 0.9888541666666667
        },
        "0.01": null
      },
      "auroc": 0.9874772135416666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2349,
          "fn": 51,
          "accuracy": 0.97875
        },
        "0.01": null
      },
      "auroc": 0.9831299479166666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2342,
          "fn": 58,
          "accuracy": 0.9758333333333333
        },
        "0.01": null
      },
      "auroc": 0.9862940972222222
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4691,
          "fn": 109,
          "accuracy": 0.9772916666666667
        },
        "0.01": null
      },
      "auroc": 0.9847120225694445
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1822,
          "fn": 578,
          "accuracy": 0.7591666666666667
        },
        "0.01": null
      },
      "auroc": 0.95743359375
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2174,
          "fn": 226,
          "accuracy": 0.9058333333333334
        },
        "0.01": null
      },
      "auroc": 0.9774588541666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3996,
          "fn": 804,
          "accuracy": 0.8325
        },
        "0.01": null
      },
      "auroc": 0.9674462239583334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4171,
          "fn": 629,
          "accuracy": 0.8689583333333334
        },
        "0.01": null
      },
      "auroc": 0.9702817708333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4516,
          "fn": 284,
          "accuracy": 0.9408333333333333
        },
        "0.01": null
      },
      "auroc": 0.9818764756944445
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8687,
          "fn": 913,
          "accuracy": 0.9048958333333333
        },
        "0.01": null
      },
      "auroc": 0.9760791232638888
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2370,
          "fn": 30,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9869868055555555
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2089,
          "fn": 311,
          "accuracy": 0.8704166666666666
        },
        "0.01": null
      },
      "auroc": 0.9767110243055556
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4459,
          "fn": 341,
          "accuracy": 0.9289583333333333
        },
        "0.01": null
      },
      "auroc": 0.9818489149305555
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2335,
          "fn": 65,
          "accuracy": 0.9729166666666667
        },
        "0.01": null
      },
      "auroc": 0.9857238715277777
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1895,
          "fn": 505,
          "accuracy": 0.7895833333333333
        },
        "0.01": null
      },
      "auroc": 0.9639800347222223
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4230,
          "fn": 570,
          "accuracy": 0.88125
        },
        "0.01": null
      },
      "auroc": 0.9748519531250001
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4705,
          "fn": 95,
          "accuracy": 0.9802083333333333
        },
        "0.01": null
      },
      "auroc": 0.9863553385416666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3984,
          "fn": 816,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.9703455295138889
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8689,
          "fn": 911,
          "accuracy": 0.9051041666666667
        },
        "0.01": null
      },
      "auroc": 0.9783504340277778
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2347,
          "fn": 53,
          "accuracy": 0.9779166666666667
        },
        "0.01": null
      },
      "auroc": 0.9860908854166667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2390,
          "fn": 10,
          "accuracy": 0.9958333333333333
        },
        "0.01": null
      },
      "auroc": 0.9883237847222222
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4737,
          "fn": 63,
          "accuracy": 0.986875
        },
        "0.01": null
      },
      "auroc": 0.9872073350694444
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2089,
          "fn": 311,
          "accuracy": 0.8704166666666666
        },
        "0.01": null
      },
      "auroc": 0.9716387152777778
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2134,
          "fn": 266,
          "accuracy": 0.8891666666666667
        },
        "0.01": null
      },
      "auroc": 0.9775427083333332
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4223,
          "fn": 577,
          "accuracy": 0.8797916666666666
        },
        "0.01": null
      },
      "auroc": 0.9745907118055556
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4436,
          "fn": 364,
          "accuracy": 0.9241666666666667
        },
        "0.01": null
      },
      "auroc": 0.9788648003472222
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4524,
          "fn": 276,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9829332465277778
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8960,
          "fn": 640,
          "accuracy": 0.9333333333333333
        },
        "0.01": null
      },
      "auroc": 0.9808990234375
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2320,
          "fn": 80,
          "accuracy": 0.9666666666666667
        },
        "0.01": null
      },
      "auroc": 0.9803388020833333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2329,
          "fn": 71,
          "accuracy": 0.9704166666666667
        },
        "0.01": null
      },
      "auroc": 0.9815864583333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4649,
          "fn": 151,
          "accuracy": 0.9685416666666666
        },
        "0.01": null
      },
      "auroc": 0.9809626302083334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1606,
          "fn": 794,
          "accuracy": 0.6691666666666667
        },
        "0.01": null
      },
      "auroc": 0.9272073784722222
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2069,
          "fn": 331,
          "accuracy": 0.8620833333333333
        },
        "0.01": null
      },
      "auroc": 0.97339296875
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3675,
          "fn": 1125,
          "accuracy": 0.765625
        },
        "0.01": null
      },
      "auroc": 0.950300173611111
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3926,
          "fn": 874,
          "accuracy": 0.8179166666666666
        },
        "0.01": null
      },
      "auroc": 0.9537730902777778
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4398,
          "fn": 402,
          "accuracy": 0.91625
        },
        "0.01": null
      },
      "auroc": 0.9774897135416667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8324,
          "fn": 1276,
          "accuracy": 0.8670833333333333
        },
        "0.01": null
      },
      "auroc": 0.9656314019097222
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2393,
          "fn": 7,
          "accuracy": 0.9970833333333333
        },
        "0.01": null
      },
      "auroc": 0.9883488715277777
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2349,
          "fn": 51,
          "accuracy": 0.97875
        },
        "0.01": null
      },
      "auroc": 0.9864430555555556
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4742,
          "fn": 58,
          "accuracy": 0.9879166666666667
        },
        "0.01": null
      },
      "auroc": 0.9873959635416667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2356,
          "fn": 44,
          "accuracy": 0.9816666666666667
        },
        "0.01": null
      },
      "auroc": 0.9864151909722223
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2225,
          "fn": 175,
          "accuracy": 0.9270833333333334
        },
        "0.01": null
      },
      "auroc": 0.9815144097222223
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4581,
          "fn": 219,
          "accuracy": 0.954375
        },
        "0.01": null
      },
      "auroc": 0.9839648003472222
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4749,
          "fn": 51,
          "accuracy": 0.989375
        },
        "0.01": null
      },
      "auroc": 0.98738203125
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4574,
          "fn": 226,
          "accuracy": 0.9529166666666666
        },
        "0.01": null
      },
      "auroc": 0.9839787326388889
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9323,
          "fn": 277,
          "accuracy": 0.9711458333333334
        },
        "0.01": null
      },
      "auroc": 0.9856803819444444
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2286,
          "fn": 114,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.9807309895833334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2286,
          "fn": 114,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.9807309895833334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2252,
          "fn": 148,
          "accuracy": 0.9383333333333334
        },
        "0.01": null
      },
      "auroc": 0.9754457465277777
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2252,
          "fn": 148,
          "accuracy": 0.9383333333333334
        },
        "0.01": null
      },
      "auroc": 0.9754457465277777
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4538,
          "fn": 262,
          "accuracy": 0.9454166666666667
        },
        "0.01": null
      },
      "auroc": 0.9780883680555555
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4538,
          "fn": 262,
          "accuracy": 0.9454166666666667
        },
        "0.01": null
      },
      "auroc": 0.9780883680555555
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1928,
          "fn": 472,
          "accuracy": 0.8033333333333333
        },
        "0.01": null
      },
      "auroc": 0.9546681423611111
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1928,
          "fn": 472,
          "accuracy": 0.8033333333333333
        },
        "0.01": null
      },
      "auroc": 0.9546681423611111
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1605,
          "fn": 795,
          "accuracy": 0.66875
        },
        "0.01": null
      },
      "auroc": 0.9242436631944444
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1605,
          "fn": 795,
          "accuracy": 0.66875
        },
        "0.01": null
      },
      "auroc": 0.9242436631944444
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3533,
          "fn": 1267,
          "accuracy": 0.7360416666666667
        },
        "0.01": null
      },
      "auroc": 0.9394559027777777
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3533,
          "fn": 1267,
          "accuracy": 0.7360416666666667
        },
        "0.01": null
      },
      "auroc": 0.9394559027777777
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2345,
          "fn": 55,
          "accuracy": 0.9770833333333333
        },
        "0.01": null
      },
      "auroc": 0.97976796875
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2345,
          "fn": 55,
          "accuracy": 0.9770833333333333
        },
        "0.01": null
      },
      "auroc": 0.97976796875
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2301,
          "fn": 99,
          "accuracy": 0.95875
        },
        "0.01": null
      },
      "auroc": 0.9780138020833333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2301,
          "fn": 99,
          "accuracy": 0.95875
        },
        "0.01": null
      },
      "auroc": 0.9780138020833333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4646,
          "fn": 154,
          "accuracy": 0.9679166666666666
        },
        "0.01": null
      },
      "auroc": 0.9788908854166667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4646,
          "fn": 154,
          "accuracy": 0.9679166666666666
        },
        "0.01": null
      },
      "auroc": 0.9788908854166667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2390,
          "fn": 10,
          "accuracy": 0.9958333333333333
        },
        "0.01": null
      },
      "auroc": 0.988492013888889
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2390,
          "fn": 10,
          "accuracy": 0.9958333333333333
        },
        "0.01": null
      },
      "auroc": 0.988492013888889
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2315,
          "fn": 85,
          "accuracy": 0.9645833333333333
        },
        "0.01": null
      },
      "auroc": 0.9842024305555556
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2315,
          "fn": 85,
          "accuracy": 0.9645833333333333
        },
        "0.01": null
      },
      "auroc": 0.9842024305555556
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4705,
          "fn": 95,
          "accuracy": 0.9802083333333333
        },
        "0.01": null
      },
      "auroc": 0.9863472222222223
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4705,
          "fn": 95,
          "accuracy": 0.9802083333333333
        },
        "0.01": null
      },
      "auroc": 0.9863472222222223
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2289,
          "fn": 111,
          "accuracy": 0.95375
        },
        "0.01": null
      },
      "auroc": 0.9823921006944445
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2289,
          "fn": 111,
          "accuracy": 0.95375
        },
        "0.01": null
      },
      "auroc": 0.9823921006944445
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2143,
          "fn": 257,
          "accuracy": 0.8929166666666667
        },
        "0.01": null
      },
      "auroc": 0.9719993923611111
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2143,
          "fn": 257,
          "accuracy": 0.8929166666666667
        },
        "0.01": null
      },
      "auroc": 0.9719993923611111
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4432,
          "fn": 368,
          "accuracy": 0.9233333333333333
        },
        "0.01": null
      },
      "auroc": 0.9771957465277777
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4432,
          "fn": 368,
          "accuracy": 0.9233333333333333
        },
        "0.01": null
      },
      "auroc": 0.9771957465277777
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 25401,
          "fn": 999,
          "accuracy": 0.9621590909090909
        },
        "0.01": null
      },
      "auroc": 0.9817336963383838
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 13866,
          "fn": 534,
          "accuracy": 0.9629166666666666
        },
        "0.01": null
      },
      "auroc": 0.9844274016203705
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 39267,
          "fn": 1533,
          "accuracy": 0.9624264705882353
        },
        "0.01": null
      },
      "auroc": 0.9826844158496733
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 23201,
          "fn": 3199,
          "accuracy": 0.8788257575757575
        },
        "0.01": null
      },
      "auroc": 0.968204513888889
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 12862,
          "fn": 1538,
          "accuracy": 0.8931944444444444
        },
        "0.01": null
      },
      "auroc": 0.9767569733796297
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 36063,
          "fn": 4737,
          "accuracy": 0.8838970588235294
        },
        "0.01": null
      },
      "auroc": 0.971223029003268
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 48602,
          "fn": 4198,
          "accuracy": 0.9204924242424243
        },
        "0.01": null
      },
      "auroc": 0.9749691051136363
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 26728,
          "fn": 2072,
          "accuracy": 0.9280555555555555
        },
        "0.01": null
      },
      "auroc": 0.9805921875000001
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 75330,
          "fn": 6270,
          "accuracy": 0.9231617647058824
        },
        "0.01": null
      },
      "auroc": 0.9769537224264706
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993708333333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9987770833333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9990739583333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9875291666666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9987802083333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9931546875
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.99345
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9987786458333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 780,
          "fn": 20,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9961143229166667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9992791666666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993270833333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993270833333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993510416666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993666666666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993708333333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9870552083333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9974500000000001
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9922526041666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9932151041666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9984083333333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 775,
          "fn": 25,
          "accuracy": 0.96875
        }
      },
      "auroc": 0.99581171875
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9837885416666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9886791666666668
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9862338541666668
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9915817708333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9940270833333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 777,
          "fn": 23,
          "accuracy": 0.97125
        }
      },
      "auroc": 0.9928044270833334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993666666666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993666666666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9992802083333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9992802083333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993234375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993234375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999346875
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999346875
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9984052083333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9984052083333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9988760416666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9988760416666667
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993713068181818
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1197,
          "fn": 3,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9992739583333333
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 3397,
          "fn": 3,
          "accuracy": 0.9991176470588236
        }
      },
      "auroc": 0.9993369485294118
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2193,
          "fn": 7,
          "accuracy": 0.9968181818181818
        },
        "0.01": {
          "tp": 2148,
          "fn": 52,
          "accuracy": 0.9763636363636363
        }
      },
      "auroc": 0.9956643939393939
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        },
        "0.01": {
          "tp": 1186,
          "fn": 14,
          "accuracy": 0.9883333333333333
        }
      },
      "auroc": 0.9971564236111111
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3391,
          "fn": 9,
          "accuracy": 0.9973529411764706
        },
        "0.01": {
          "tp": 3334,
          "fn": 66,
          "accuracy": 0.9805882352941176
        }
      },
      "auroc": 0.9961909926470589
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 4393,
          "fn": 7,
          "accuracy": 0.9984090909090909
        },
        "0.01": {
          "tp": 4348,
          "fn": 52,
          "accuracy": 0.9881818181818182
        }
      },
      "auroc": 0.9975178503787878
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2398,
          "fn": 2,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 2383,
          "fn": 17,
          "accuracy": 0.9929166666666667
        }
      },
      "auroc": 0.9982151909722223
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 6791,
          "fn": 9,
          "accuracy": 0.9986764705882353
        },
        "0.01": {
          "tp": 6731,
          "fn": 69,
          "accuracy": 0.9898529411764706
        }
      },
      "auroc": 0.9977639705882353
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993708333333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9987770833333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9990739583333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9875291666666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9987802083333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9931546875
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.99345
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9987786458333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 780,
          "fn": 20,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9961143229166667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9992791666666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993270833333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993270833333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993510416666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993666666666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993708333333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9870552083333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9974500000000001
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9922526041666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9932151041666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9984083333333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 775,
          "fn": 25,
          "accuracy": 0.96875
        }
      },
      "auroc": 0.99581171875
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9837885416666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9886791666666668
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9862338541666668
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9915817708333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9940270833333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 777,
          "fn": 23,
          "accuracy": 0.97125
        }
      },
      "auroc": 0.9928044270833334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993666666666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993666666666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9992802083333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9992802083333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993234375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993234375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999346875
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999346875
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9984052083333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9984052083333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9988760416666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9988760416666667
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993713068181818
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1197,
          "fn": 3,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9992739583333333
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 3397,
          "fn": 3,
          "accuracy": 0.9991176470588236
        }
      },
      "auroc": 0.9993369485294118
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2193,
          "fn": 7,
          "accuracy": 0.9968181818181818
        },
        "0.01": {
          "tp": 2148,
          "fn": 52,
          "accuracy": 0.9763636363636363
        }
      },
      "auroc": 0.9956643939393939
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        },
        "0.01": {
          "tp": 1186,
          "fn": 14,
          "accuracy": 0.9883333333333333
        }
      },
      "auroc": 0.9971564236111111
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3391,
          "fn": 9,
          "accuracy": 0.9973529411764706
        },
        "0.01": {
          "tp": 3334,
          "fn": 66,
          "accuracy": 0.9805882352941176
        }
      },
      "auroc": 0.9961909926470589
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 4393,
          "fn": 7,
          "accuracy": 0.9984090909090909
        },
        "0.01": {
          "tp": 4348,
          "fn": 52,
          "accuracy": 0.9881818181818182
        }
      },
      "auroc": 0.9975178503787878
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2398,
          "fn": 2,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 2383,
          "fn": 17,
          "accuracy": 0.9929166666666667
        }
      },
      "auroc": 0.9982151909722223
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 6791,
          "fn": 9,
          "accuracy": 0.9986764705882353
        },
        "0.01": {
          "tp": 6731,
          "fn": 69,
          "accuracy": 0.9898529411764706
        }
      },
      "auroc": 0.9977639705882353
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993645833333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9988614583333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9991130208333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9880739583333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9987458333333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9934098958333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9937192708333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9988036458333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 781,
          "fn": 19,
          "accuracy": 0.97625
        }
      },
      "auroc": 0.9962614583333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9992687499999999
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993218749999999
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993218749999999
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993484375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9992864583333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993666666666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9993265625000001
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9890197916666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9979395833333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9934796875
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.994153125
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.998653125
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 780,
          "fn": 20,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9964031250000001
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993708333333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993729166666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9840718749999999
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9897104166666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9868911458333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9917234375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9945406250000001
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 777,
          "fn": 23,
          "accuracy": 0.97125
        }
      },
      "auroc": 0.99313203125
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993604166666666
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993604166666666
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993322916666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993322916666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993463541666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993463541666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993260416666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993260416666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9984739583333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9984739583333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9989
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9989
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2199,
          "fn": 1,
          "accuracy": 0.9995454545454545
        }
      },
      "auroc": 0.9993602272727272
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1197,
          "fn": 3,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9992873263888888
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 3396,
          "fn": 4,
          "accuracy": 0.9988235294117647
        }
      },
      "auroc": 0.9993344975490196
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2193,
          "fn": 7,
          "accuracy": 0.9968181818181818
        },
        "0.01": {
          "tp": 2155,
          "fn": 45,
          "accuracy": 0.9795454545454545
        }
      },
      "auroc": 0.9959292613636364
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 1,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 1186,
          "fn": 14,
          "accuracy": 0.9883333333333333
        }
      },
      "auroc": 0.9974024305555556
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3392,
          "fn": 8,
          "accuracy": 0.9976470588235294
        },
        "0.01": {
          "tp": 3341,
          "fn": 59,
          "accuracy": 0.9826470588235294
        }
      },
      "auroc": 0.9964492034313726
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 4393,
          "fn": 7,
          "accuracy": 0.9984090909090909
        },
        "0.01": {
          "tp": 4354,
          "fn": 46,
          "accuracy": 0.9895454545454545
        }
      },
      "auroc": 0.9976447443181818
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2399,
          "fn": 1,
          "accuracy": 0.9995833333333334
        },
        "0.01": {
          "tp": 2383,
          "fn": 17,
          "accuracy": 0.9929166666666667
        }
      },
      "auroc": 0.9983448784722222
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 6792,
          "fn": 8,
          "accuracy": 0.9988235294117647
        },
        "0.01": {
          "tp": 6737,
          "fn": 63,
          "accuracy": 0.990735294117647
        }
      },
      "auroc": 0.9978918504901961
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993052083333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9984395833333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9988723958333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9911895833333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9988656250000001
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9950276041666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9952473958333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9986526041666668
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 782,
          "fn": 18,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.99695
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9992927083333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993338541666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993338541666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993544270833333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99924375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993093749999999
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9872114583333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9978052083333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9925083333333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9932932291666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9985244791666668
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 778,
          "fn": 22,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9959088541666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9850822916666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9899572916666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9875197916666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9922286458333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9946661458333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 779,
          "fn": 21,
          "accuracy": 0.97375
        }
      },
      "auroc": 0.9934473958333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993052083333334
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993052083333334
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993401041666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993401041666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991739583333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991739583333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9992677083333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9992677083333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9992208333333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9992208333333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993354166666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993354166666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9988677083333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9988677083333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9991015624999999
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9991015624999999
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2199,
          "fn": 1,
          "accuracy": 0.9995454545454545
        }
      },
      "auroc": 0.9993467803030304
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1197,
          "fn": 3,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9991972222222222
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 3396,
          "fn": 4,
          "accuracy": 0.9988235294117647
        }
      },
      "auroc": 0.9992939950980392
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2192,
          "fn": 8,
          "accuracy": 0.9963636363636363
        },
        "0.01": {
          "tp": 2153,
          "fn": 47,
          "accuracy": 0.9786363636363636
        }
      },
      "auroc": 0.9961635416666667
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 1,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 1187,
          "fn": 13,
          "accuracy": 0.9891666666666666
        }
      },
      "auroc": 0.997445138888889
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3391,
          "fn": 9,
          "accuracy": 0.9973529411764706
        },
        "0.01": {
          "tp": 3340,
          "fn": 60,
          "accuracy": 0.9823529411764705
        }
      },
      "auroc": 0.9966158700980392
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 4392,
          "fn": 8,
          "accuracy": 0.9981818181818182
        },
        "0.01": {
          "tp": 4352,
          "fn": 48,
          "accuracy": 0.9890909090909091
        }
      },
      "auroc": 0.9977551609848485
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2399,
          "fn": 1,
          "accuracy": 0.9995833333333334
        },
        "0.01": {
          "tp": 2384,
          "fn": 16,
          "accuracy": 0.9933333333333333
        }
      },
      "auroc": 0.9983211805555555
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 6791,
          "fn": 9,
          "accuracy": 0.9986764705882353
        },
        "0.01": {
          "tp": 6736,
          "fn": 64,
          "accuracy": 0.9905882352941177
        }
      },
      "auroc": 0.9979549325980392
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993708333333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.998775
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9990729166666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9872916666666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.998775
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9930333333333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.99333125
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9987750000000001
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 782,
          "fn": 18,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9960531250000001
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99928125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999328125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999328125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993515625
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993624999999999
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993687499999999
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9867322916666668
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9976197916666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9921760416666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9930536458333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9984911458333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 778,
          "fn": 22,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9957723958333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9834291666666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9885302083333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9859796875
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9914020833333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9939526041666668
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 793,
          "fn": 7,
          "accuracy": 0.99125
        },
        "0.01": {
          "tp": 775,
          "fn": 25,
          "accuracy": 0.96875
        }
      },
      "auroc": 0.9926773437499999
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993708333333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993708333333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993114583333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993114583333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993411458333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993411458333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993624999999999
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993624999999999
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9984677083333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9984677083333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9989151041666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9989151041666666
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999373106060606
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1197,
          "fn": 3,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9992729166666666
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 3397,
          "fn": 3,
          "accuracy": 0.9991176470588236
        }
      },
      "auroc": 0.9993377450980392
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2193,
          "fn": 7,
          "accuracy": 0.9968181818181818
        },
        "0.01": {
          "tp": 2151,
          "fn": 49,
          "accuracy": 0.9777272727272728
        }
      },
      "auroc": 0.9955892992424242
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1197,
          "fn": 3,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 1186,
          "fn": 14,
          "accuracy": 0.9883333333333333
        }
      },
      "auroc": 0.997159375
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3390,
          "fn": 10,
          "accuracy": 0.9970588235294118
        },
        "0.01": {
          "tp": 3337,
          "fn": 63,
          "accuracy": 0.9814705882352941
        }
      },
      "auroc": 0.9961434436274509
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 4393,
          "fn": 7,
          "accuracy": 0.9984090909090909
        },
        "0.01": {
          "tp": 4351,
          "fn": 49,
          "accuracy": 0.9888636363636364
        }
      },
      "auroc": 0.9974812026515152
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2397,
          "fn": 3,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 2383,
          "fn": 17,
          "accuracy": 0.9929166666666667
        }
      },
      "auroc": 0.9982161458333333
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 6790,
          "fn": 10,
          "accuracy": 0.9985294117647059
        },
        "0.01": {
          "tp": 6734,
          "fn": 66,
          "accuracy": 0.9902941176470588
        }
      },
      "auroc": 0.9977405943627451
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9980947916666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9965479166666666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9973213541666668
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9975864583333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9973447916666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.997465625
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.997840625
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9969463541666668
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 770,
          "fn": 30,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9973934895833334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9966302083333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9792906250000001
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        }
      },
      "auroc": 0.9879604166666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9641145833333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9816322916666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9728734375000001
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9803723958333332
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        }
      },
      "auroc": 0.9804614583333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 793,
          "fn": 7,
          "accuracy": 0.99125
        },
        "0.01": {
          "tp": 683,
          "fn": 117,
          "accuracy": 0.85375
        }
      },
      "auroc": 0.9804169270833333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.99503125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.97904375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9870375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.99333125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9751510416666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        }
      },
      "auroc": 0.9842411458333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.99418125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9770973958333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 792,
          "fn": 8,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 707,
          "fn": 93,
          "accuracy": 0.88375
        }
      },
      "auroc": 0.9856393229166667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9958979166666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.994415625
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9951567708333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9785802083333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        }
      },
      "auroc": 0.968859375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        }
      },
      "auroc": 0.9737197916666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9872390625
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9816374999999999
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 690,
          "fn": 110,
          "accuracy": 0.8625
        }
      },
      "auroc": 0.9844382812500001
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9950291666666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9889833333333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        }
      },
      "auroc": 0.99200625
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9658260416666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.9413791666666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        }
      },
      "auroc": 0.9536026041666668
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9804276041666669
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        }
      },
      "auroc": 0.96518125
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 784,
          "fn": 16,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 641,
          "fn": 159,
          "accuracy": 0.80125
        }
      },
      "auroc": 0.9728044270833334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9977114583333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.99485625
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9962838541666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9947989583333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9877010416666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.99125
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9962552083333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9912786458333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 750,
          "fn": 50,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9937669270833333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9957427083333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9957427083333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9943031250000001
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9943031250000001
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9950229166666666
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9950229166666666
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9928583333333334
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9928583333333334
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9863635416666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9863635416666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9896109375000001
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9896109375000001
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9937354166666668
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9937354166666668
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.995109375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.995109375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9944223958333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9944223958333334
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9887479166666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9887479166666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.991203125
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.991203125
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9899755208333334
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9899755208333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9925458333333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9925458333333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9871458333333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9871458333333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9898458333333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9898458333333333
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2199,
          "fn": 1,
          "accuracy": 0.9995454545454545
        },
        "0.01": {
          "tp": 2079,
          "fn": 121,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9947295454545455
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1195,
          "fn": 5,
          "accuracy": 0.9958333333333333
        },
        "0.01": {
          "tp": 1073,
          "fn": 127,
          "accuracy": 0.8941666666666667
        }
      },
      "auroc": 0.98885625
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3394,
          "fn": 6,
          "accuracy": 0.9982352941176471
        },
        "0.01": {
          "tp": 3152,
          "fn": 248,
          "accuracy": 0.9270588235294117
        }
      },
      "auroc": 0.9926566176470588
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2187,
          "fn": 13,
          "accuracy": 0.9940909090909091
        },
        "0.01": {
          "tp": 1928,
          "fn": 272,
          "accuracy": 0.8763636363636363
        }
      },
      "auroc": 0.9862147727272728
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1178,
          "fn": 22,
          "accuracy": 0.9816666666666667
        },
        "0.01": {
          "tp": 991,
          "fn": 209,
          "accuracy": 0.8258333333333333
        }
      },
      "auroc": 0.9753446180555556
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3365,
          "fn": 35,
          "accuracy": 0.9897058823529412
        },
        "0.01": {
          "tp": 2919,
          "fn": 481,
          "accuracy": 0.8585294117647059
        }
      },
      "auroc": 0.9823782475490196
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 4386,
          "fn": 14,
          "accuracy": 0.9968181818181818
        },
        "0.01": {
          "tp": 4007,
          "fn": 393,
          "accuracy": 0.9106818181818181
        }
      },
      "auroc": 0.9904721590909091
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2373,
          "fn": 27,
          "accuracy": 0.98875
        },
        "0.01": {
          "tp": 2064,
          "fn": 336,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9821004340277777
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 6759,
          "fn": 41,
          "accuracy": 0.9939705882352942
        },
        "0.01": {
          "tp": 6071,
          "fn": 729,
          "accuracy": 0.8927941176470588
        }
      },
      "auroc": 0.9875174325980393
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991729166666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.998475
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9988239583333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9877552083333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9988927083333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9933239583333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9934640625
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9986838541666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 781,
          "fn": 19,
          "accuracy": 0.97625
        }
      },
      "auroc": 0.9960739583333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999303125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993390625
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993390625
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99935703125
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993552083333332
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993583333333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993567708333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9892229166666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9972208333333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        }
      },
      "auroc": 0.993221875
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9942890625
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9982895833333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 773,
          "fn": 27,
          "accuracy": 0.96625
        }
      },
      "auroc": 0.9962893229166667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9837083333333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9880333333333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9858708333333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9915416666666668
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9937041666666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 793,
          "fn": 7,
          "accuracy": 0.99125
        },
        "0.01": {
          "tp": 772,
          "fn": 28,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9926229166666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993708333333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993729166666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993729166666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993739583333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993604166666666
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993604166666666
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993010416666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993010416666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993307291666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993307291666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993666666666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993666666666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.998328125
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.998328125
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9988473958333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9988473958333334
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993527462121212
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1197,
          "fn": 3,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9992222222222222
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 3397,
          "fn": 3,
          "accuracy": 0.9991176470588236
        }
      },
      "auroc": 0.9993066789215687
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2194,
          "fn": 6,
          "accuracy": 0.9972727272727273
        },
        "0.01": {
          "tp": 2146,
          "fn": 54,
          "accuracy": 0.9754545454545455
        }
      },
      "auroc": 0.9958696022727273
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        },
        "0.01": {
          "tp": 1182,
          "fn": 18,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9970326388888888
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3392,
          "fn": 8,
          "accuracy": 0.9976470588235294
        },
        "0.01": {
          "tp": 3328,
          "fn": 72,
          "accuracy": 0.9788235294117648
        }
      },
      "auroc": 0.9962800857843138
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 4394,
          "fn": 6,
          "accuracy": 0.9986363636363637
        },
        "0.01": {
          "tp": 4346,
          "fn": 54,
          "accuracy": 0.9877272727272727
        }
      },
      "auroc": 0.9976111742424242
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2398,
          "fn": 2,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 2379,
          "fn": 21,
          "accuracy": 0.99125
        }
      },
      "auroc": 0.9981274305555554
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 6792,
          "fn": 8,
          "accuracy": 0.9988235294117647
        },
        "0.01": {
          "tp": 6725,
          "fn": 75,
          "accuracy": 0.9889705882352942
        }
      },
      "auroc": 0.9977933823529412
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993708333333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9987770833333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9990739583333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9875697916666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9987802083333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.993175
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9934703125000001
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9987786458333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 780,
          "fn": 20,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9961244791666668
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99928125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999328125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999328125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993515625
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993666666666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993708333333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9870552083333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9974500000000001
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9922526041666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9932151041666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9984083333333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 775,
          "fn": 25,
          "accuracy": 0.96875
        }
      },
      "auroc": 0.99581171875
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9837885416666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.988675
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9862317708333335
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9915817708333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9940250000000002
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 777,
          "fn": 23,
          "accuracy": 0.97125
        }
      },
      "auroc": 0.9928033854166667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993666666666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993666666666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9992802083333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9992802083333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993234375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993234375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999346875
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999346875
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9984052083333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9984052083333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9988760416666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9988760416666667
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993713068181818
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1197,
          "fn": 3,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9992739583333333
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 3397,
          "fn": 3,
          "accuracy": 0.9991176470588236
        }
      },
      "auroc": 0.9993369485294118
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2193,
          "fn": 7,
          "accuracy": 0.9968181818181818
        },
        "0.01": {
          "tp": 2148,
          "fn": 52,
          "accuracy": 0.9763636363636363
        }
      },
      "auroc": 0.995668087121212
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        },
        "0.01": {
          "tp": 1186,
          "fn": 14,
          "accuracy": 0.9883333333333333
        }
      },
      "auroc": 0.9971560763888889
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3391,
          "fn": 9,
          "accuracy": 0.9973529411764706
        },
        "0.01": {
          "tp": 3334,
          "fn": 66,
          "accuracy": 0.9805882352941176
        }
      },
      "auroc": 0.9961932598039216
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 4393,
          "fn": 7,
          "accuracy": 0.9984090909090909
        },
        "0.01": {
          "tp": 4348,
          "fn": 52,
          "accuracy": 0.9881818181818182
        }
      },
      "auroc": 0.9975196969696969
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2398,
          "fn": 2,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 2383,
          "fn": 17,
          "accuracy": 0.9929166666666667
        }
      },
      "auroc": 0.9982150173611111
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 6791,
          "fn": 9,
          "accuracy": 0.9986764705882353
        },
        "0.01": {
          "tp": 6731,
          "fn": 69,
          "accuracy": 0.9898529411764706
        }
      },
      "auroc": 0.9977651041666666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9979218750000001
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.99724375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9975828125
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        }
      },
      "auroc": 0.980040625
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9958416666666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9879411458333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9889812499999999
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9965427083333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 715,
          "fn": 85,
          "accuracy": 0.89375
        }
      },
      "auroc": 0.9927619791666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99935625
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999365625
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991645833333332
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9992697916666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9992604166666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9993177083333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9992281249999999
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9922260416666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9957270833333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        }
      },
      "auroc": 0.9752447916666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9851354166666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 287,
          "fn": 113,
          "accuracy": 0.7175
        }
      },
      "auroc": 0.9801901041666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9872364583333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9886807291666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 649,
          "fn": 151,
          "accuracy": 0.81125
        }
      },
      "auroc": 0.98795859375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993218749999999
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9991947916666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9992583333333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9845395833333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9869708333333332
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9857552083333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.9919307291666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        }
      },
      "auroc": 0.9930828125
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 724,
          "fn": 76,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9925067708333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993322916666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993536458333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993708333333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9990854166666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.999228125
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993729166666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9992088541666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9992908854166667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991802083333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991802083333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9973552083333334
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9973552083333334
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9982677083333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9982677083333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9973645833333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9973645833333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9968791666666666
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9968791666666666
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.997121875
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.997121875
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993322916666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993322916666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993536458333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993536458333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9992885416666666
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9992885416666666
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993317708333334
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993317708333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9987458333333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9987458333333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991104166666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991104166666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.998928125
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.998928125
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2188,
          "fn": 12,
          "accuracy": 0.9945454545454545
        }
      },
      "auroc": 0.9989670454545455
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1155,
          "fn": 45,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9977880208333334
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 3343,
          "fn": 57,
          "accuracy": 0.9832352941176471
        }
      },
      "auroc": 0.9985509191176472
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2019,
          "fn": 181,
          "accuracy": 0.9177272727272727
        }
      },
      "auroc": 0.9936283143939394
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1097,
          "fn": 103,
          "accuracy": 0.9141666666666667
        }
      },
      "auroc": 0.9942621527777777
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 3116,
          "fn": 284,
          "accuracy": 0.9164705882352941
        }
      },
      "auroc": 0.9938520220588235
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 4207,
          "fn": 193,
          "accuracy": 0.9561363636363637
        }
      },
      "auroc": 0.9962976799242425
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2252,
          "fn": 148,
          "accuracy": 0.9383333333333334
        }
      },
      "auroc": 0.9960250868055556
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 6800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 6459,
          "fn": 341,
          "accuracy": 0.9498529411764706
        }
      },
      "auroc": 0.9962014705882354
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993708333333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9987770833333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9990739583333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9899322916666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9987802083333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.99435625
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9946515625
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9987786458333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 782,
          "fn": 18,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9967151041666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9992833333333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993291666666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993291666666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993520833333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99936875
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999371875
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9863697916666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.998828125
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9925989583333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9928723958333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9990984374999999
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 778,
          "fn": 22,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9959854166666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9860312500000001
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9896885416666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9878598958333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9927031250000001
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9945317708333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 795,
          "fn": 5,
          "accuracy": 0.99375
        },
        "0.01": {
          "tp": 778,
          "fn": 22,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9936174479166667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993489583333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993489583333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9992697916666666
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9992697916666666
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9993093749999999
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9993093749999999
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993364583333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993364583333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9988697916666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9988697916666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.999103125
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.999103125
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99936875
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1197,
          "fn": 3,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9992743055555555
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 3397,
          "fn": 3,
          "accuracy": 0.9991176470588236
        }
      },
      "auroc": 0.9993354166666667
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2192,
          "fn": 8,
          "accuracy": 0.9963636363636363
        },
        "0.01": {
          "tp": 2151,
          "fn": 49,
          "accuracy": 0.9777272727272728
        }
      },
      "auroc": 0.9960657196969697
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 1,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 1188,
          "fn": 12,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9975550347222222
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3391,
          "fn": 9,
          "accuracy": 0.9973529411764706
        },
        "0.01": {
          "tp": 3339,
          "fn": 61,
          "accuracy": 0.9820588235294118
        }
      },
      "auroc": 0.9965913602941175
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 4392,
          "fn": 8,
          "accuracy": 0.9981818181818182
        },
        "0.01": {
          "tp": 4351,
          "fn": 49,
          "accuracy": 0.9888636363636364
        }
      },
      "auroc": 0.9977172348484848
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2399,
          "fn": 1,
          "accuracy": 0.9995833333333334
        },
        "0.01": {
          "tp": 2385,
          "fn": 15,
          "accuracy": 0.99375
        }
      },
      "auroc": 0.9984146701388888
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 6791,
          "fn": 9,
          "accuracy": 0.9986764705882353
        },
        "0.01": {
          "tp": 6736,
          "fn": 64,
          "accuracy": 0.9905882352941177
        }
      },
      "auroc": 0.9979633884803922
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993708333333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9988270833333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9990989583333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9876947916666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9987666666666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9932307291666668
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9935328125000001
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.998796875
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 779,
          "fn": 21,
          "accuracy": 0.97375
        }
      },
      "auroc": 0.9961648437500001
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993020833333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993385416666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993385416666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993567708333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993666666666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993708333333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9865510416666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9969416666666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        }
      },
      "auroc": 0.9917463541666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9929630208333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9981541666666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 773,
          "fn": 27,
          "accuracy": 0.96625
        }
      },
      "auroc": 0.99555859375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9837510416666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9888302083333332
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9862906250000001
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9915630208333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9941026041666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 793,
          "fn": 7,
          "accuracy": 0.99125
        },
        "0.01": {
          "tp": 777,
          "fn": 23,
          "accuracy": 0.97125
        }
      },
      "auroc": 0.9928328125
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993666666666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993666666666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9992802083333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9992802083333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993234375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993234375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999346875
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999346875
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9984052083333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9984052083333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9988760416666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9988760416666667
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993713068181818
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1197,
          "fn": 3,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9992822916666667
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 3397,
          "fn": 3,
          "accuracy": 0.9991176470588236
        }
      },
      "auroc": 0.9993398897058823
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2192,
          "fn": 8,
          "accuracy": 0.9963636363636363
        },
        "0.01": {
          "tp": 2146,
          "fn": 54,
          "accuracy": 0.9754545454545455
        }
      },
      "auroc": 0.9956302083333333
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        },
        "0.01": {
          "tp": 1185,
          "fn": 15,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9970984375
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3390,
          "fn": 10,
          "accuracy": 0.9970588235294118
        },
        "0.01": {
          "tp": 3331,
          "fn": 69,
          "accuracy": 0.9797058823529412
        }
      },
      "auroc": 0.9961484068627451
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 4392,
          "fn": 8,
          "accuracy": 0.9981818181818182
        },
        "0.01": {
          "tp": 4346,
          "fn": 54,
          "accuracy": 0.9877272727272727
        }
      },
      "auroc": 0.9975007575757575
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2398,
          "fn": 2,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 2382,
          "fn": 18,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9981903645833333
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 6790,
          "fn": 10,
          "accuracy": 0.9985294117647059
        },
        "0.01": {
          "tp": 6728,
          "fn": 72,
          "accuracy": 0.9894117647058823
        }
      },
      "auroc": 0.9977441482843137
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9992770833333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993145833333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9992958333333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993416666666666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9992583333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993093749999999
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9992864583333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9992979166666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9977052083333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9969
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9973026041666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        }
      },
      "auroc": 0.9828875
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.99181875
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        }
      },
      "auroc": 0.987353125
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9902963541666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        }
      },
      "auroc": 0.994359375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 688,
          "fn": 112,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9923278645833333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993624999999999
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993687499999999
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9990854166666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9992302083333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9992239583333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9992994791666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9987979166666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9918989583333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        }
      },
      "auroc": 0.9953484375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.9797864583333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        }
      },
      "auroc": 0.9884489583333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        }
      },
      "auroc": 0.9841177083333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        }
      },
      "auroc": 0.9892921875
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9901739583333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 625,
          "fn": 175,
          "accuracy": 0.78125
        }
      },
      "auroc": 0.9897330729166667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993479166666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9986791666666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9990135416666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9879447916666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        }
      },
      "auroc": 0.9889239583333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.988434375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9936463541666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9938015625
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 716,
          "fn": 84,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9937239583333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993666666666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993427083333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993546875
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993052083333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.998940625
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991229166666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993359374999999
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9991416666666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9992388020833333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.998996875
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.998996875
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9973333333333334
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9973333333333334
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9981651041666666
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9981651041666666
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9956979166666666
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9956979166666666
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9945437500000001
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9945437500000001
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9951208333333335
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9951208333333335
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9992958333333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9992958333333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993354166666666
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993354166666666
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999296875
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999296875
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993359374999999
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9993359374999999
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9987145833333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9987145833333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9988583333333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9988583333333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9987864583333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9987864583333333
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2174,
          "fn": 26,
          "accuracy": 0.9881818181818182
        }
      },
      "auroc": 0.9987299242424242
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1144,
          "fn": 56,
          "accuracy": 0.9533333333333334
        }
      },
      "auroc": 0.9975829861111111
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 3318,
          "fn": 82,
          "accuracy": 0.9758823529411764
        }
      },
      "auroc": 0.9983251225490196
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1994,
          "fn": 206,
          "accuracy": 0.9063636363636364
        }
      },
      "auroc": 0.9943607954545455
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1073,
          "fn": 127,
          "accuracy": 0.8941666666666667
        }
      },
      "auroc": 0.9944126736111111
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 3067,
          "fn": 333,
          "accuracy": 0.9020588235294118
        }
      },
      "auroc": 0.9943791053921569
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 4168,
          "fn": 232,
          "accuracy": 0.9472727272727273
        }
      },
      "auroc": 0.9965453598484848
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2217,
          "fn": 183,
          "accuracy": 0.92375
        }
      },
      "auroc": 0.9959978298611112
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 6800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 6385,
          "fn": 415,
          "accuracy": 0.9389705882352941
        }
      },
      "auroc": 0.9963521139705883
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2392,
          "fn": 8,
          "accuracy": 0.9966666666666667
        }
      },
      "auroc": 0.9992601562500001
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2390,
          "fn": 10,
          "accuracy": 0.9958333333333333
        }
      },
      "auroc": 0.999134375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 4782,
          "fn": 18,
          "accuracy": 0.99625
        }
      },
      "auroc": 0.999197265625
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2395,
          "fn": 5,
          "accuracy": 0.9979166666666667
        }
      },
      "auroc": 0.9992231770833334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2393,
          "fn": 7,
          "accuracy": 0.9970833333333333
        }
      },
      "auroc": 0.9991960937500001
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 4788,
          "fn": 12,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9992096354166666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 4787,
          "fn": 13,
          "accuracy": 0.9972916666666667
        }
      },
      "auroc": 0.9992416666666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 4783,
          "fn": 17,
          "accuracy": 0.9964583333333333
        }
      },
      "auroc": 0.999165234375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 9570,
          "fn": 30,
          "accuracy": 0.996875
        }
      },
      "auroc": 0.9992034505208334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2384,
          "fn": 16,
          "accuracy": 0.9933333333333333
        }
      },
      "auroc": 0.9988604166666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2398,
          "fn": 2,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 2320,
          "fn": 80,
          "accuracy": 0.9666666666666667
        }
      },
      "auroc": 0.9968267361111112
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4798,
          "fn": 2,
          "accuracy": 0.9995833333333334
        },
        "0.01": {
          "tp": 4704,
          "fn": 96,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9978435763888889
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2382,
          "fn": 18,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 2079,
          "fn": 321,
          "accuracy": 0.86625
        }
      },
      "auroc": 0.9851340277777778
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2398,
          "fn": 2,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 2330,
          "fn": 70,
          "accuracy": 0.9708333333333333
        }
      },
      "auroc": 0.99653828125
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4780,
          "fn": 20,
          "accuracy": 0.9958333333333333
        },
        "0.01": {
          "tp": 4409,
          "fn": 391,
          "accuracy": 0.9185416666666667
        }
      },
      "auroc": 0.9908361545138888
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4782,
          "fn": 18,
          "accuracy": 0.99625
        },
        "0.01": {
          "tp": 4463,
          "fn": 337,
          "accuracy": 0.9297916666666667
        }
      },
      "auroc": 0.9919972222222222
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4796,
          "fn": 4,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 4650,
          "fn": 150,
          "accuracy": 0.96875
        }
      },
      "auroc": 0.9966825086805556
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9578,
          "fn": 22,
          "accuracy": 0.9977083333333333
        },
        "0.01": {
          "tp": 9113,
          "fn": 487,
          "accuracy": 0.9492708333333333
        }
      },
      "auroc": 0.9943398654513889
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2391,
          "fn": 9,
          "accuracy": 0.99625
        }
      },
      "auroc": 0.9990130208333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2397,
          "fn": 3,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 2363,
          "fn": 37,
          "accuracy": 0.9845833333333334
        }
      },
      "auroc": 0.9976781250000001
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4797,
          "fn": 3,
          "accuracy": 0.999375
        },
        "0.01": {
          "tp": 4754,
          "fn": 46,
          "accuracy": 0.9904166666666666
        }
      },
      "auroc": 0.9983455729166667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2390,
          "fn": 10,
          "accuracy": 0.9958333333333333
        }
      },
      "auroc": 0.9988713541666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2395,
          "fn": 5,
          "accuracy": 0.9979166666666667
        },
        "0.01": {
          "tp": 2361,
          "fn": 39,
          "accuracy": 0.98375
        }
      },
      "auroc": 0.9972476562500001
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4795,
          "fn": 5,
          "accuracy": 0.9989583333333333
        },
        "0.01": {
          "tp": 4751,
          "fn": 49,
          "accuracy": 0.9897916666666666
        }
      },
      "auroc": 0.9980595052083333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 4781,
          "fn": 19,
          "accuracy": 0.9960416666666667
        }
      },
      "auroc": 0.9989421875000001
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4792,
          "fn": 8,
          "accuracy": 0.9983333333333333
        },
        "0.01": {
          "tp": 4724,
          "fn": 76,
          "accuracy": 0.9841666666666666
        }
      },
      "auroc": 0.997462890625
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9592,
          "fn": 8,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 9505,
          "fn": 95,
          "accuracy": 0.9901041666666667
        }
      },
      "auroc": 0.9982025390625
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2392,
          "fn": 8,
          "accuracy": 0.9966666666666667
        }
      },
      "auroc": 0.9990158854166666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2310,
          "fn": 90,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.997725607638889
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 4702,
          "fn": 98,
          "accuracy": 0.9795833333333334
        }
      },
      "auroc": 0.9983707465277778
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2388,
          "fn": 12,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 2032,
          "fn": 368,
          "accuracy": 0.8466666666666667
        }
      },
      "auroc": 0.9849903645833333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2398,
          "fn": 2,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 2215,
          "fn": 185,
          "accuracy": 0.9229166666666667
        }
      },
      "auroc": 0.9934290798611111
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4786,
          "fn": 14,
          "accuracy": 0.9970833333333333
        },
        "0.01": {
          "tp": 4247,
          "fn": 553,
          "accuracy": 0.8847916666666666
        }
      },
      "auroc": 0.9892097222222223
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4788,
          "fn": 12,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 4424,
          "fn": 376,
          "accuracy": 0.9216666666666666
        }
      },
      "auroc": 0.992003125
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4798,
          "fn": 2,
          "accuracy": 0.9995833333333334
        },
        "0.01": {
          "tp": 4525,
          "fn": 275,
          "accuracy": 0.9427083333333334
        }
      },
      "auroc": 0.99557734375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9586,
          "fn": 14,
          "accuracy": 0.9985416666666667
        },
        "0.01": {
          "tp": 8949,
          "fn": 651,
          "accuracy": 0.9321875
        }
      },
      "auroc": 0.993790234375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2387,
          "fn": 13,
          "accuracy": 0.9945833333333334
        }
      },
      "auroc": 0.9990061631944445
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2372,
          "fn": 28,
          "accuracy": 0.9883333333333333
        }
      },
      "auroc": 0.9984356770833334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 4759,
          "fn": 41,
          "accuracy": 0.9914583333333333
        }
      },
      "auroc": 0.9987209201388889
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2356,
          "fn": 44,
          "accuracy": 0.9816666666666667
        },
        "0.01": {
          "tp": 2152,
          "fn": 248,
          "accuracy": 0.8966666666666666
        }
      },
      "auroc": 0.9829791666666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2372,
          "fn": 28,
          "accuracy": 0.9883333333333333
        },
        "0.01": {
          "tp": 2159,
          "fn": 241,
          "accuracy": 0.8995833333333333
        }
      },
      "auroc": 0.984838107638889
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4728,
          "fn": 72,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 4311,
          "fn": 489,
          "accuracy": 0.898125
        }
      },
      "auroc": 0.9839086371527778
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4756,
          "fn": 44,
          "accuracy": 0.9908333333333333
        },
        "0.01": {
          "tp": 4539,
          "fn": 261,
          "accuracy": 0.945625
        }
      },
      "auroc": 0.9909926649305556
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4772,
          "fn": 28,
          "accuracy": 0.9941666666666666
        },
        "0.01": {
          "tp": 4531,
          "fn": 269,
          "accuracy": 0.9439583333333333
        }
      },
      "auroc": 0.9916368923611112
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9528,
          "fn": 72,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 9070,
          "fn": 530,
          "accuracy": 0.9447916666666667
        }
      },
      "auroc": 0.9913147786458333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2393,
          "fn": 7,
          "accuracy": 0.9970833333333333
        }
      },
      "auroc": 0.9992356770833333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2390,
          "fn": 10,
          "accuracy": 0.9958333333333333
        }
      },
      "auroc": 0.9989921875000001
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 4783,
          "fn": 17,
          "accuracy": 0.9964583333333333
        }
      },
      "auroc": 0.9991139322916667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2388,
          "fn": 12,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9989875
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2399,
          "fn": 1,
          "accuracy": 0.9995833333333334
        },
        "0.01": {
          "tp": 2375,
          "fn": 25,
          "accuracy": 0.9895833333333334
        }
      },
      "auroc": 0.9983414930555556
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4799,
          "fn": 1,
          "accuracy": 0.9997916666666666
        },
        "0.01": {
          "tp": 4763,
          "fn": 37,
          "accuracy": 0.9922916666666667
        }
      },
      "auroc": 0.9986644965277778
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 4781,
          "fn": 19,
          "accuracy": 0.9960416666666667
        }
      },
      "auroc": 0.9991115885416667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4799,
          "fn": 1,
          "accuracy": 0.9997916666666666
        },
        "0.01": {
          "tp": 4765,
          "fn": 35,
          "accuracy": 0.9927083333333333
        }
      },
      "auroc": 0.9986668402777779
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9599,
          "fn": 1,
          "accuracy": 0.9998958333333333
        },
        "0.01": {
          "tp": 9546,
          "fn": 54,
          "accuracy": 0.994375
        }
      },
      "auroc": 0.9988892144097222
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2390,
          "fn": 10,
          "accuracy": 0.9958333333333333
        }
      },
      "auroc": 0.9990245659722223
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2390,
          "fn": 10,
          "accuracy": 0.9958333333333333
        }
      },
      "auroc": 0.9990245659722223
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2371,
          "fn": 29,
          "accuracy": 0.9879166666666667
        }
      },
      "auroc": 0.9986080729166666
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2371,
          "fn": 29,
          "accuracy": 0.9879166666666667
        }
      },
      "auroc": 0.9986080729166666
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 4761,
          "fn": 39,
          "accuracy": 0.991875
        }
      },
      "auroc": 0.9988163194444444
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 4761,
          "fn": 39,
          "accuracy": 0.991875
        }
      },
      "auroc": 0.9988163194444444
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2367,
          "fn": 33,
          "accuracy": 0.98625
        }
      },
      "auroc": 0.9983335069444444
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2367,
          "fn": 33,
          "accuracy": 0.98625
        }
      },
      "auroc": 0.9983335069444444
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2398,
          "fn": 2,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 2344,
          "fn": 56,
          "accuracy": 0.9766666666666667
        }
      },
      "auroc": 0.997615798611111
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2398,
          "fn": 2,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 2344,
          "fn": 56,
          "accuracy": 0.9766666666666667
        }
      },
      "auroc": 0.997615798611111
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4798,
          "fn": 2,
          "accuracy": 0.9995833333333334
        },
        "0.01": {
          "tp": 4711,
          "fn": 89,
          "accuracy": 0.9814583333333333
        }
      },
      "auroc": 0.9979746527777779
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4798,
          "fn": 2,
          "accuracy": 0.9995833333333334
        },
        "0.01": {
          "tp": 4711,
          "fn": 89,
          "accuracy": 0.9814583333333333
        }
      },
      "auroc": 0.9979746527777779
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2384,
          "fn": 16,
          "accuracy": 0.9933333333333333
        }
      },
      "auroc": 0.9989050347222223
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2384,
          "fn": 16,
          "accuracy": 0.9933333333333333
        }
      },
      "auroc": 0.9989050347222223
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2392,
          "fn": 8,
          "accuracy": 0.9966666666666667
        }
      },
      "auroc": 0.999009375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2392,
          "fn": 8,
          "accuracy": 0.9966666666666667
        }
      },
      "auroc": 0.999009375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 4776,
          "fn": 24,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9989572048611111
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 4776,
          "fn": 24,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9989572048611111
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2399,
          "fn": 1,
          "accuracy": 0.9995833333333334
        },
        "0.01": {
          "tp": 2379,
          "fn": 21,
          "accuracy": 0.99125
        }
      },
      "auroc": 0.9984894097222222
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2399,
          "fn": 1,
          "accuracy": 0.9995833333333334
        },
        "0.01": {
          "tp": 2379,
          "fn": 21,
          "accuracy": 0.99125
        }
      },
      "auroc": 0.9984894097222222
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2379,
          "fn": 21,
          "accuracy": 0.99125
        }
      },
      "auroc": 0.9986802951388889
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2379,
          "fn": 21,
          "accuracy": 0.99125
        }
      },
      "auroc": 0.9986802951388889
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4799,
          "fn": 1,
          "accuracy": 0.9997916666666666
        },
        "0.01": {
          "tp": 4758,
          "fn": 42,
          "accuracy": 0.99125
        }
      },
      "auroc": 0.9985848524305556
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4799,
          "fn": 1,
          "accuracy": 0.9997916666666666
        },
        "0.01": {
          "tp": 4758,
          "fn": 42,
          "accuracy": 0.99125
        }
      },
      "auroc": 0.9985848524305556
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2380,
          "fn": 20,
          "accuracy": 0.9916666666666667
        }
      },
      "auroc": 0.9986767361111111
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2380,
          "fn": 20,
          "accuracy": 0.9916666666666667
        }
      },
      "auroc": 0.9986767361111111
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2398,
          "fn": 2,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 2365,
          "fn": 35,
          "accuracy": 0.9854166666666667
        }
      },
      "auroc": 0.9976452256944444
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2398,
          "fn": 2,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 2365,
          "fn": 35,
          "accuracy": 0.9854166666666667
        }
      },
      "auroc": 0.9976452256944444
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4798,
          "fn": 2,
          "accuracy": 0.9995833333333334
        },
        "0.01": {
          "tp": 4745,
          "fn": 55,
          "accuracy": 0.9885416666666667
        }
      },
      "auroc": 0.9981609809027777
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4798,
          "fn": 2,
          "accuracy": 0.9995833333333334
        },
        "0.01": {
          "tp": 4745,
          "fn": 55,
          "accuracy": 0.9885416666666667
        }
      },
      "auroc": 0.9981609809027777
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 26399,
          "fn": 1,
          "accuracy": 0.9999621212121212
        },
        "0.01": {
          "tp": 26239,
          "fn": 161,
          "accuracy": 0.9939015151515151
        }
      },
      "auroc": 0.9988927793560607
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 14395,
          "fn": 5,
          "accuracy": 0.9996527777777777
        },
        "0.01": {
          "tp": 14145,
          "fn": 255,
          "accuracy": 0.9822916666666667
        }
      },
      "auroc": 0.9981321180555556
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 40794,
          "fn": 6,
          "accuracy": 0.9998529411764706
        },
        "0.01": {
          "tp": 40384,
          "fn": 416,
          "accuracy": 0.9898039215686274
        }
      },
      "auroc": 0.9986243106617648
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 26322,
          "fn": 78,
          "accuracy": 0.9970454545454546
        },
        "0.01": {
          "tp": 25287,
          "fn": 1113,
          "accuracy": 0.9578409090909091
        }
      },
      "auroc": 0.9947040325126263
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 14362,
          "fn": 38,
          "accuracy": 0.9973611111111111
        },
        "0.01": {
          "tp": 13833,
          "fn": 567,
          "accuracy": 0.960625
        }
      },
      "auroc": 0.9949317853009259
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 40684,
          "fn": 116,
          "accuracy": 0.9971568627450981
        },
        "0.01": {
          "tp": 39120,
          "fn": 1680,
          "accuracy": 0.9588235294117647
        }
      },
      "auroc": 0.9947844158496733
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 52721,
          "fn": 79,
          "accuracy": 0.9985037878787879
        },
        "0.01": {
          "tp": 51526,
          "fn": 1274,
          "accuracy": 0.9758712121212121
        }
      },
      "auroc": 0.9967984059343434
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 28757,
          "fn": 43,
          "accuracy": 0.9985069444444444
        },
        "0.01": {
          "tp": 27978,
          "fn": 822,
          "accuracy": 0.9714583333333333
        }
      },
      "auroc": 0.9965319516782408
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 81478,
          "fn": 122,
          "accuracy": 0.9985049019607843
        },
        "0.01": {
          "tp": 79504,
          "fn": 2096,
          "accuracy": 0.9743137254901961
        }
      },
      "auroc": 0.9967043632557189
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.995175
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9951916666666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9951916666666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9952000000000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9950385416666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9951416666666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9950901041666668
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.9616197916666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.99359375
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9776067708333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9783291666666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9943677083333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 780,
          "fn": 20,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 706,
          "fn": 94,
          "accuracy": 0.8825
        }
      },
      "auroc": 0.9863484375
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9948812499999999
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.993384375
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9941328125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9950447916666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9942963541666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9946705729166667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9876947916666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9951416666666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9914182291666668
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9914515625000001
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.995175
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 760,
          "fn": 40,
          "accuracy": 0.95
        }
      },
      "auroc": 0.99331328125
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.995175
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9951625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9951687499999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9559385416666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9945583333333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9752484375
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9755567708333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9948604166666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 781,
          "fn": 19,
          "accuracy": 0.97625
        },
        "0.01": {
          "tp": 732,
          "fn": 68,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9852085937500001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.995075
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9951625
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9951187499999999
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9925072916666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9945583333333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9935328125
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9937911458333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9948604166666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 787,
          "fn": 13,
          "accuracy": 0.98375
        }
      },
      "auroc": 0.9943257812499999
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9941156250000001
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9941156250000001
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9939145833333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9939145833333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9940151041666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9940151041666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9677281249999999
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9677281249999999
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.942890625
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.942890625
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        }
      },
      "auroc": 0.955309375
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        }
      },
      "auroc": 0.955309375
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.99356875
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.99356875
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943885416666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943885416666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9817343749999999
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9817343749999999
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9721885416666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9721885416666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9769614583333335
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9769614583333335
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2181,
          "fn": 19,
          "accuracy": 0.9913636363636363
        },
        "0.01": {
          "tp": 2104,
          "fn": 96,
          "accuracy": 0.9563636363636364
        }
      },
      "auroc": 0.9913553030303031
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1196,
          "fn": 4,
          "accuracy": 0.9966666666666667
        }
      },
      "auroc": 0.9951819444444445
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3381,
          "fn": 19,
          "accuracy": 0.9944117647058823
        },
        "0.01": {
          "tp": 3300,
          "fn": 100,
          "accuracy": 0.9705882352941176
        }
      },
      "auroc": 0.9927058823529411
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2117,
          "fn": 83,
          "accuracy": 0.9622727272727273
        },
        "0.01": {
          "tp": 1906,
          "fn": 294,
          "accuracy": 0.8663636363636363
        }
      },
      "auroc": 0.9805109848484848
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        },
        "0.01": {
          "tp": 1167,
          "fn": 33,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9944019097222223
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3315,
          "fn": 85,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 3073,
          "fn": 327,
          "accuracy": 0.9038235294117647
        }
      },
      "auroc": 0.9854136642156862
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 4298,
          "fn": 102,
          "accuracy": 0.9768181818181818
        },
        "0.01": {
          "tp": 4010,
          "fn": 390,
          "accuracy": 0.9113636363636364
        }
      },
      "auroc": 0.9859331439393939
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2398,
          "fn": 2,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 2363,
          "fn": 37,
          "accuracy": 0.9845833333333334
        }
      },
      "auroc": 0.9947919270833333
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 6696,
          "fn": 104,
          "accuracy": 0.9847058823529412
        },
        "0.01": {
          "tp": 6373,
          "fn": 427,
          "accuracy": 0.9372058823529412
        }
      },
      "auroc": 0.9890597732843137
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.995175
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9951916666666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9951916666666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9952000000000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9950385416666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9951416666666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9950901041666668
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.9616197916666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.99359375
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9776067708333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9783291666666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9943677083333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 780,
          "fn": 20,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 706,
          "fn": 94,
          "accuracy": 0.8825
        }
      },
      "auroc": 0.9863484375
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9948812499999999
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.993384375
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9941328125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9950447916666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9942963541666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9946705729166667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9876947916666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9951416666666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9914182291666668
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9914515625000001
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.995175
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 760,
          "fn": 40,
          "accuracy": 0.95
        }
      },
      "auroc": 0.99331328125
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.995175
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9951625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9951687499999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9559385416666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9945583333333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9752484375
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9755567708333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9948604166666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 781,
          "fn": 19,
          "accuracy": 0.97625
        },
        "0.01": {
          "tp": 732,
          "fn": 68,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9852085937500001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.995075
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9951625
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9951187499999999
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9925072916666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9945583333333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9935328125
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9937911458333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9948604166666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 787,
          "fn": 13,
          "accuracy": 0.98375
        }
      },
      "auroc": 0.9943257812499999
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9941156250000001
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9941156250000001
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9939145833333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9939145833333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9940151041666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9940151041666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9677281249999999
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9677281249999999
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.942890625
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.942890625
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        }
      },
      "auroc": 0.955309375
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        }
      },
      "auroc": 0.955309375
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.99356875
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.99356875
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943885416666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943885416666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9817343749999999
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9817343749999999
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9721885416666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9721885416666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9769614583333335
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9769614583333335
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2181,
          "fn": 19,
          "accuracy": 0.9913636363636363
        },
        "0.01": {
          "tp": 2104,
          "fn": 96,
          "accuracy": 0.9563636363636364
        }
      },
      "auroc": 0.9913553030303031
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1196,
          "fn": 4,
          "accuracy": 0.9966666666666667
        }
      },
      "auroc": 0.9951819444444445
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3381,
          "fn": 19,
          "accuracy": 0.9944117647058823
        },
        "0.01": {
          "tp": 3300,
          "fn": 100,
          "accuracy": 0.9705882352941176
        }
      },
      "auroc": 0.9927058823529411
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2117,
          "fn": 83,
          "accuracy": 0.9622727272727273
        },
        "0.01": {
          "tp": 1906,
          "fn": 294,
          "accuracy": 0.8663636363636363
        }
      },
      "auroc": 0.9805109848484848
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        },
        "0.01": {
          "tp": 1167,
          "fn": 33,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9944019097222223
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3315,
          "fn": 85,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 3073,
          "fn": 327,
          "accuracy": 0.9038235294117647
        }
      },
      "auroc": 0.9854136642156862
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 4298,
          "fn": 102,
          "accuracy": 0.9768181818181818
        },
        "0.01": {
          "tp": 4010,
          "fn": 390,
          "accuracy": 0.9113636363636364
        }
      },
      "auroc": 0.9859331439393939
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2398,
          "fn": 2,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 2363,
          "fn": 37,
          "accuracy": 0.9845833333333334
        }
      },
      "auroc": 0.9947919270833333
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 6696,
          "fn": 104,
          "accuracy": 0.9847058823529412
        },
        "0.01": {
          "tp": 6373,
          "fn": 427,
          "accuracy": 0.9372058823529412
        }
      },
      "auroc": 0.9890597732843137
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9949989583333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9951416666666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9950703125
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9680875
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9935979166666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        }
      },
      "auroc": 0.9808427083333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9815432291666668
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9943697916666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 786,
          "fn": 14,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 726,
          "fn": 74,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.9879565104166668
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9948864583333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9935489583333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9942177083333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9950473958333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9943786458333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 791,
          "fn": 9,
          "accuracy": 0.98875
        }
      },
      "auroc": 0.9947130208333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.989059375
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9921338541666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9921338541666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": {
          "tp": 775,
          "fn": 25,
          "accuracy": 0.96875
        }
      },
      "auroc": 0.99367109375
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9951541666666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.99518125
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9624427083333332
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9948875
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9786651041666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9788255208333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9950208333333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 785,
          "fn": 15,
          "accuracy": 0.98125
        },
        "0.01": {
          "tp": 753,
          "fn": 47,
          "accuracy": 0.94125
        }
      },
      "auroc": 0.9869231770833333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9951083333333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9951583333333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9951333333333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9927197916666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9947739583333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.993746875
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9939140625
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9949661458333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 788,
          "fn": 12,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9944401041666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.99405625
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.99405625
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9940604166666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9940604166666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9940583333333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9940583333333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9694375
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9694375
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        }
      },
      "auroc": 0.9478239583333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        }
      },
      "auroc": 0.9478239583333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9586307291666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9586307291666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9932666666666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9932666666666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9942375
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9942375
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.983675
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.983675
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9746177083333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9746177083333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        }
      },
      "auroc": 0.9791463541666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        }
      },
      "auroc": 0.9791463541666667
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2180,
          "fn": 20,
          "accuracy": 0.990909090909091
        },
        "0.01": {
          "tp": 2118,
          "fn": 82,
          "accuracy": 0.9627272727272728
        }
      },
      "auroc": 0.9916841856060606
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1196,
          "fn": 4,
          "accuracy": 0.9966666666666667
        }
      },
      "auroc": 0.9951798611111111
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3380,
          "fn": 20,
          "accuracy": 0.9941176470588236
        },
        "0.01": {
          "tp": 3314,
          "fn": 86,
          "accuracy": 0.9747058823529412
        }
      },
      "auroc": 0.9929179534313726
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2131,
          "fn": 69,
          "accuracy": 0.9686363636363636
        },
        "0.01": {
          "tp": 1970,
          "fn": 230,
          "accuracy": 0.8954545454545455
        }
      },
      "auroc": 0.9824892045454545
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 1,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 1173,
          "fn": 27,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9945375000000001
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3330,
          "fn": 70,
          "accuracy": 0.9794117647058823
        },
        "0.01": {
          "tp": 3143,
          "fn": 257,
          "accuracy": 0.9244117647058824
        }
      },
      "auroc": 0.986741544117647
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 4311,
          "fn": 89,
          "accuracy": 0.9797727272727272
        },
        "0.01": {
          "tp": 4088,
          "fn": 312,
          "accuracy": 0.9290909090909091
        }
      },
      "auroc": 0.9870866950757577
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2399,
          "fn": 1,
          "accuracy": 0.9995833333333334
        },
        "0.01": {
          "tp": 2369,
          "fn": 31,
          "accuracy": 0.9870833333333333
        }
      },
      "auroc": 0.9948586805555555
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 6710,
          "fn": 90,
          "accuracy": 0.986764705882353
        },
        "0.01": {
          "tp": 6457,
          "fn": 343,
          "accuracy": 0.9495588235294118
        }
      },
      "auroc": 0.9898297487745098
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.99514375
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9951760416666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9950947916666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9951515625
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9951192708333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        }
      },
      "auroc": 0.9951638020833333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9948552083333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9951291666666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9949921875
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        }
      },
      "auroc": 0.974309375
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9935447916666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9839270833333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        }
      },
      "auroc": 0.9845822916666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9943369791666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 789,
          "fn": 11,
          "accuracy": 0.98625
        },
        "0.01": {
          "tp": 710,
          "fn": 90,
          "accuracy": 0.8875
        }
      },
      "auroc": 0.9894596354166667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.995003125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.994975
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9949890625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9944364583333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9909687500000001
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9927026041666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9947197916666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9929718750000001
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 773,
          "fn": 27,
          "accuracy": 0.96625
        }
      },
      "auroc": 0.9938458333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9951416666666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.995175
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9951583333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        }
      },
      "auroc": 0.9878677083333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9950843749999999
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.9914760416666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9915046875
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9951296875000001
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 748,
          "fn": 52,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9933171875000001
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9951458333333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.995090625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9951182291666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9644208333333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9951291666666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9797750000000001
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        }
      },
      "auroc": 0.9797833333333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9951098958333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 783,
          "fn": 17,
          "accuracy": 0.97875
        },
        "0.01": {
          "tp": 735,
          "fn": 65,
          "accuracy": 0.91875
        }
      },
      "auroc": 0.9874466145833334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9947072916666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9951291666666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9949182291666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.993875
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9932885416666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9935817708333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9942911458333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9942088541666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 770,
          "fn": 30,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9942500000000001
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.993146875
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.993146875
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9919791666666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9919791666666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.9925630208333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.9925630208333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9419916666666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9419916666666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.9194562500000001
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.9194562500000001
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        }
      },
      "auroc": 0.9307239583333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        }
      },
      "auroc": 0.9307239583333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9950843749999999
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9950843749999999
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9951458333333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9951458333333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9951151041666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9951151041666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9950458333333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9950458333333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9910822916666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9910822916666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9930640625
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9930640625
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9696479166666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9696479166666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        }
      },
      "auroc": 0.9514010416666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        }
      },
      "auroc": 0.9514010416666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        }
      },
      "auroc": 0.9605244791666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        }
      },
      "auroc": 0.9605244791666666
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2154,
          "fn": 46,
          "accuracy": 0.9790909090909091
        },
        "0.01": {
          "tp": 2022,
          "fn": 178,
          "accuracy": 0.9190909090909091
        }
      },
      "auroc": 0.987725284090909
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1187,
          "fn": 13,
          "accuracy": 0.9891666666666666
        }
      },
      "auroc": 0.9951071180555555
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3354,
          "fn": 46,
          "accuracy": 0.9864705882352941
        },
        "0.01": {
          "tp": 3209,
          "fn": 191,
          "accuracy": 0.9438235294117647
        }
      },
      "auroc": 0.9903306372549021
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2097,
          "fn": 103,
          "accuracy": 0.9531818181818181
        },
        "0.01": {
          "tp": 1789,
          "fn": 411,
          "accuracy": 0.8131818181818182
        }
      },
      "auroc": 0.9781074810606061
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1197,
          "fn": 3,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 1159,
          "fn": 41,
          "accuracy": 0.9658333333333333
        }
      },
      "auroc": 0.9938517361111111
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3294,
          "fn": 106,
          "accuracy": 0.9688235294117648
        },
        "0.01": {
          "tp": 2948,
          "fn": 452,
          "accuracy": 0.8670588235294118
        }
      },
      "auroc": 0.9836642769607843
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 4251,
          "fn": 149,
          "accuracy": 0.9661363636363637
        },
        "0.01": {
          "tp": 3811,
          "fn": 589,
          "accuracy": 0.8661363636363636
        }
      },
      "auroc": 0.9829163825757576
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2397,
          "fn": 3,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 2346,
          "fn": 54,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9944794270833334
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 6648,
          "fn": 152,
          "accuracy": 0.9776470588235294
        },
        "0.01": {
          "tp": 6157,
          "fn": 643,
          "accuracy": 0.9054411764705882
        }
      },
      "auroc": 0.9869974571078431
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9950458333333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.995175
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9951104166666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9611083333333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.993525
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        }
      },
      "auroc": 0.9773166666666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        }
      },
      "auroc": 0.9780770833333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9943500000000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 780,
          "fn": 20,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 716,
          "fn": 84,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9862135416666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9948895833333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9932802083333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9940848958333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9950489583333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9942442708333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9946466145833334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9865333333333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.995175
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9908541666666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.9908708333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9951916666666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 766,
          "fn": 34,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.99303125
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9951625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9951625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9951625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.9588770833333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9948447916666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        }
      },
      "auroc": 0.9768609374999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        }
      },
      "auroc": 0.9770197916666665
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9950036458333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 786,
          "fn": 14,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 739,
          "fn": 61,
          "accuracy": 0.92375
        }
      },
      "auroc": 0.9860117187499999
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.995075
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9951625
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9951187499999999
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9926520833333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9942885416666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9934703125
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9938635416666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9947255208333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": {
          "tp": 786,
          "fn": 14,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.99429453125
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9940822916666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9940822916666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9938416666666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9938416666666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9939619791666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9939619791666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9697510416666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9697510416666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9456895833333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9456895833333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        }
      },
      "auroc": 0.9577203125
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        }
      },
      "auroc": 0.9577203125
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9940760416666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9940760416666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9946421875
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9946421875
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9829874999999999
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9829874999999999
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9733739583333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9733739583333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9781807291666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9781807291666667
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2182,
          "fn": 18,
          "accuracy": 0.9918181818181818
        },
        "0.01": {
          "tp": 2105,
          "fn": 95,
          "accuracy": 0.9568181818181818
        }
      },
      "auroc": 0.9916496212121213
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1197,
          "fn": 3,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9951875
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3382,
          "fn": 18,
          "accuracy": 0.9947058823529412
        },
        "0.01": {
          "tp": 3302,
          "fn": 98,
          "accuracy": 0.9711764705882353
        }
      },
      "auroc": 0.9928982843137255
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2124,
          "fn": 76,
          "accuracy": 0.9654545454545455
        },
        "0.01": {
          "tp": 1934,
          "fn": 266,
          "accuracy": 0.8790909090909091
        }
      },
      "auroc": 0.9810416666666666
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1197,
          "fn": 3,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 1168,
          "fn": 32,
          "accuracy": 0.9733333333333334
        }
      },
      "auroc": 0.9943869791666666
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3321,
          "fn": 79,
          "accuracy": 0.976764705882353
        },
        "0.01": {
          "tp": 3102,
          "fn": 298,
          "accuracy": 0.9123529411764706
        }
      },
      "auroc": 0.9857517769607844
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 4306,
          "fn": 94,
          "accuracy": 0.9786363636363636
        },
        "0.01": {
          "tp": 4039,
          "fn": 361,
          "accuracy": 0.9179545454545455
        }
      },
      "auroc": 0.9863456439393941
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2397,
          "fn": 3,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 2365,
          "fn": 35,
          "accuracy": 0.9854166666666667
        }
      },
      "auroc": 0.9947872395833333
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 6703,
          "fn": 97,
          "accuracy": 0.985735294117647
        },
        "0.01": {
          "tp": 6404,
          "fn": 396,
          "accuracy": 0.941764705882353
        }
      },
      "auroc": 0.989325030637255
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.994975
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9941322916666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9945536458333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.99475625
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9937614583333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9942588541666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.994865625
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.993946875
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 771,
          "fn": 29,
          "accuracy": 0.96375
        }
      },
      "auroc": 0.9944062499999999
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9943989583333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9889114583333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9916552083333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.951625
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.9770052083333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        }
      },
      "auroc": 0.9643151041666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": {
          "tp": 261,
          "fn": 139,
          "accuracy": 0.6525
        }
      },
      "auroc": 0.9730119791666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9829583333333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 761,
          "fn": 39,
          "accuracy": 0.95125
        },
        "0.01": {
          "tp": 545,
          "fn": 255,
          "accuracy": 0.68125
        }
      },
      "auroc": 0.9779851562499999
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9942427083333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9921989583333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9932208333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9913177083333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9894541666666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        }
      },
      "auroc": 0.9903859375
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9927802083333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        }
      },
      "auroc": 0.9908265625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 695,
          "fn": 105,
          "accuracy": 0.86875
        }
      },
      "auroc": 0.9918033854166667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9941625000000001
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.99305625
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        }
      },
      "auroc": 0.993609375
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.9769552083333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        }
      },
      "auroc": 0.99014375
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        }
      },
      "auroc": 0.9835494791666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        }
      },
      "auroc": 0.9855588541666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        }
      },
      "auroc": 0.9916
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 788,
          "fn": 12,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 622,
          "fn": 178,
          "accuracy": 0.7775
        }
      },
      "auroc": 0.9885794270833334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9937010416666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9860749999999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9898880208333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.9388072916666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9769395833333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        }
      },
      "auroc": 0.9578734375
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9662541666666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 291,
          "fn": 109,
          "accuracy": 0.7275
        }
      },
      "auroc": 0.9815072916666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 754,
          "fn": 46,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 539,
          "fn": 261,
          "accuracy": 0.67375
        }
      },
      "auroc": 0.9738807291666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9944354166666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9945177083333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9944765625
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9909479166666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.98761875
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        }
      },
      "auroc": 0.9892833333333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9926916666666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9910682291666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 793,
          "fn": 7,
          "accuracy": 0.99125
        },
        "0.01": {
          "tp": 726,
          "fn": 74,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.9918799479166667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9901239583333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9901239583333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9928885416666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9928885416666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        }
      },
      "auroc": 0.99150625
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        }
      },
      "auroc": 0.99150625
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        }
      },
      "auroc": 0.9205822916666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        }
      },
      "auroc": 0.9205822916666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.9054333333333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.9054333333333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        },
        "0.01": {
          "tp": 164,
          "fn": 236,
          "accuracy": 0.41
        }
      },
      "auroc": 0.9130078125
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        },
        "0.01": {
          "tp": 164,
          "fn": 236,
          "accuracy": 0.41
        }
      },
      "auroc": 0.9130078125
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9948291666666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9948291666666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9946906249999999
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9946906249999999
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9947598958333332
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9947598958333332
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9930177083333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9930177083333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9830125000000001
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9830125000000001
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9880151041666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9880151041666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.9609958333333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.9609958333333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.9452375
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.9452375
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        }
      },
      "auroc": 0.9531166666666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        }
      },
      "auroc": 0.9531166666666667
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2131,
          "fn": 69,
          "accuracy": 0.9686363636363636
        },
        "0.01": {
          "tp": 1876,
          "fn": 324,
          "accuracy": 0.8527272727272728
        }
      },
      "auroc": 0.984133143939394
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1191,
          "fn": 9,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 1075,
          "fn": 125,
          "accuracy": 0.8958333333333334
        }
      },
      "auroc": 0.9914819444444445
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3322,
          "fn": 78,
          "accuracy": 0.9770588235294118
        },
        "0.01": {
          "tp": 2951,
          "fn": 449,
          "accuracy": 0.8679411764705882
        }
      },
      "auroc": 0.9867268382352941
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2034,
          "fn": 166,
          "accuracy": 0.9245454545454546
        },
        "0.01": {
          "tp": 1465,
          "fn": 735,
          "accuracy": 0.6659090909090909
        }
      },
      "auroc": 0.969606534090909
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1174,
          "fn": 26,
          "accuracy": 0.9783333333333334
        },
        "0.01": {
          "tp": 913,
          "fn": 287,
          "accuracy": 0.7608333333333334
        }
      },
      "auroc": 0.9858204861111111
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3208,
          "fn": 192,
          "accuracy": 0.9435294117647058
        },
        "0.01": {
          "tp": 2378,
          "fn": 1022,
          "accuracy": 0.6994117647058824
        }
      },
      "auroc": 0.9753291053921569
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 4165,
          "fn": 235,
          "accuracy": 0.946590909090909
        },
        "0.01": {
          "tp": 3341,
          "fn": 1059,
          "accuracy": 0.7593181818181818
        }
      },
      "auroc": 0.9768698390151516
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2365,
          "fn": 35,
          "accuracy": 0.9854166666666667
        },
        "0.01": {
          "tp": 1988,
          "fn": 412,
          "accuracy": 0.8283333333333334
        }
      },
      "auroc": 0.9886512152777779
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 6530,
          "fn": 270,
          "accuracy": 0.9602941176470589
        },
        "0.01": {
          "tp": 5329,
          "fn": 1471,
          "accuracy": 0.7836764705882353
        }
      },
      "auroc": 0.9810279718137256
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.995175
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9951916666666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9951916666666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9952000000000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9948635416666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.995125
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9949942708333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        }
      },
      "auroc": 0.962075
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.993578125
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9778265625
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        }
      },
      "auroc": 0.9784692708333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9943515625
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 780,
          "fn": 20,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 705,
          "fn": 95,
          "accuracy": 0.88125
        }
      },
      "auroc": 0.9864104166666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9948812499999999
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.993396875
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9941390625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9950447916666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9943026041666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9946736979166667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9877374999999999
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9951416666666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        }
      },
      "auroc": 0.9914395833333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.9914729166666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.995175
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 761,
          "fn": 39,
          "accuracy": 0.95125
        }
      },
      "auroc": 0.9933239583333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9951625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9951854166666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9580947916666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9945708333333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9763328125
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        }
      },
      "auroc": 0.9766515625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9948666666666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 781,
          "fn": 19,
          "accuracy": 0.97625
        },
        "0.01": {
          "tp": 733,
          "fn": 67,
          "accuracy": 0.91625
        }
      },
      "auroc": 0.9857591145833333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.995075
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9951625
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9951187499999999
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9925072916666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9945583333333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9935328125
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9937911458333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9948604166666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 787,
          "fn": 13,
          "accuracy": 0.98375
        }
      },
      "auroc": 0.9943257812499999
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9941072916666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9941072916666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9939125
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9939125
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9940098958333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9940098958333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9679854166666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9679854166666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9440291666666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9440291666666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        }
      },
      "auroc": 0.9560072916666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        }
      },
      "auroc": 0.9560072916666666
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.99356875
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.99356875
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943885416666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943885416666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9819395833333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9819395833333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9726989583333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9726989583333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        }
      },
      "auroc": 0.9773192708333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        }
      },
      "auroc": 0.9773192708333334
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2181,
          "fn": 19,
          "accuracy": 0.9913636363636363
        },
        "0.01": {
          "tp": 2105,
          "fn": 95,
          "accuracy": 0.9568181818181818
        }
      },
      "auroc": 0.9913837121212121
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1196,
          "fn": 4,
          "accuracy": 0.9966666666666667
        }
      },
      "auroc": 0.9951791666666667
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3381,
          "fn": 19,
          "accuracy": 0.9944117647058823
        },
        "0.01": {
          "tp": 3301,
          "fn": 99,
          "accuracy": 0.9708823529411764
        }
      },
      "auroc": 0.9927232843137255
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2118,
          "fn": 82,
          "accuracy": 0.9627272727272728
        },
        "0.01": {
          "tp": 1906,
          "fn": 294,
          "accuracy": 0.8663636363636363
        }
      },
      "auroc": 0.9809019886363637
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        },
        "0.01": {
          "tp": 1169,
          "fn": 31,
          "accuracy": 0.9741666666666666
        }
      },
      "auroc": 0.9944034722222221
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3316,
          "fn": 84,
          "accuracy": 0.9752941176470589
        },
        "0.01": {
          "tp": 3075,
          "fn": 325,
          "accuracy": 0.9044117647058824
        }
      },
      "auroc": 0.9856672181372548
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 4299,
          "fn": 101,
          "accuracy": 0.9770454545454546
        },
        "0.01": {
          "tp": 4011,
          "fn": 389,
          "accuracy": 0.9115909090909091
        }
      },
      "auroc": 0.9861428503787879
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2398,
          "fn": 2,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 2365,
          "fn": 35,
          "accuracy": 0.9854166666666667
        }
      },
      "auroc": 0.9947913194444444
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 6697,
          "fn": 103,
          "accuracy": 0.9848529411764706
        },
        "0.01": {
          "tp": 6376,
          "fn": 424,
          "accuracy": 0.9376470588235294
        }
      },
      "auroc": 0.9891952512254902
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.995175
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9951916666666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9951916666666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9952000000000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9950385416666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9951416666666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9950901041666668
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.9616197916666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9935895833333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9776046875
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9783291666666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.994365625
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 780,
          "fn": 20,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 706,
          "fn": 94,
          "accuracy": 0.8825
        }
      },
      "auroc": 0.9863473958333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9948812499999999
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.993384375
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9941328125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9950447916666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9942963541666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9946705729166667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9876947916666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9951416666666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9914182291666668
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9914515625000001
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.995175
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 760,
          "fn": 40,
          "accuracy": 0.95
        }
      },
      "auroc": 0.99331328125
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.995175
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9951625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9951687499999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9559145833333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9945583333333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9752364583333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9755447916666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9948604166666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 780,
          "fn": 20,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 732,
          "fn": 68,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9852026041666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.995075
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9951625
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9951187499999999
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9925072916666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9945583333333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9935328125
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9937911458333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9948604166666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 787,
          "fn": 13,
          "accuracy": 0.98375
        }
      },
      "auroc": 0.9943257812499999
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9941156250000001
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9941156250000001
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9939145833333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9939145833333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9940151041666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9940151041666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9677281249999999
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9677281249999999
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.942890625
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.942890625
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        }
      },
      "auroc": 0.955309375
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        }
      },
      "auroc": 0.955309375
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.99356875
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.99356875
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943885416666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943885416666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9817385416666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9817385416666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9719041666666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9719041666666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9768213541666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9768213541666666
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2181,
          "fn": 19,
          "accuracy": 0.9913636363636363
        },
        "0.01": {
          "tp": 2104,
          "fn": 96,
          "accuracy": 0.9563636363636364
        }
      },
      "auroc": 0.9913556818181818
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1196,
          "fn": 4,
          "accuracy": 0.9966666666666667
        }
      },
      "auroc": 0.9951819444444445
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3381,
          "fn": 19,
          "accuracy": 0.9944117647058823
        },
        "0.01": {
          "tp": 3300,
          "fn": 100,
          "accuracy": 0.9705882352941176
        }
      },
      "auroc": 0.9927061274509803
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2116,
          "fn": 84,
          "accuracy": 0.9618181818181818
        },
        "0.01": {
          "tp": 1906,
          "fn": 294,
          "accuracy": 0.8663636363636363
        }
      },
      "auroc": 0.9804829545454545
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        },
        "0.01": {
          "tp": 1167,
          "fn": 33,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9944012152777777
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3314,
          "fn": 86,
          "accuracy": 0.9747058823529412
        },
        "0.01": {
          "tp": 3073,
          "fn": 327,
          "accuracy": 0.9038235294117647
        }
      },
      "auroc": 0.9853952818627452
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 4297,
          "fn": 103,
          "accuracy": 0.9765909090909091
        },
        "0.01": {
          "tp": 4010,
          "fn": 390,
          "accuracy": 0.9113636363636364
        }
      },
      "auroc": 0.985919318181818
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2398,
          "fn": 2,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 2363,
          "fn": 37,
          "accuracy": 0.9845833333333334
        }
      },
      "auroc": 0.9947915798611111
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 6695,
          "fn": 105,
          "accuracy": 0.9845588235294118
        },
        "0.01": {
          "tp": 6373,
          "fn": 427,
          "accuracy": 0.9372058823529412
        }
      },
      "auroc": 0.9890507046568627
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        }
      },
      "auroc": 0.9915322916666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9896791666666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        }
      },
      "auroc": 0.9906057291666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.98965625
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.98531875
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 278,
          "fn": 122,
          "accuracy": 0.695
        }
      },
      "auroc": 0.9874875
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        }
      },
      "auroc": 0.9905942708333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        }
      },
      "auroc": 0.9874989583333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 791,
          "fn": 9,
          "accuracy": 0.98875
        },
        "0.01": {
          "tp": 575,
          "fn": 225,
          "accuracy": 0.71875
        }
      },
      "auroc": 0.9890466145833333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.99240625
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9924447916666668
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        }
      },
      "auroc": 0.9924255208333335
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        }
      },
      "auroc": 0.9346833333333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.9875666666666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        }
      },
      "auroc": 0.961125
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        }
      },
      "auroc": 0.9635447916666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9900057291666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 745,
          "fn": 55,
          "accuracy": 0.93125
        },
        "0.01": {
          "tp": 544,
          "fn": 256,
          "accuracy": 0.68
        }
      },
      "auroc": 0.9767752604166666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        }
      },
      "auroc": 0.988240625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        }
      },
      "auroc": 0.9851708333333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9867057291666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        }
      },
      "auroc": 0.984715625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.9791614583333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        }
      },
      "auroc": 0.9819385416666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 255,
          "fn": 145,
          "accuracy": 0.6375
        }
      },
      "auroc": 0.9864781250000001
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        }
      },
      "auroc": 0.9821661458333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 775,
          "fn": 25,
          "accuracy": 0.96875
        },
        "0.01": {
          "tp": 472,
          "fn": 328,
          "accuracy": 0.59
        }
      },
      "auroc": 0.9843221354166667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9944770833333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.99348125
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9939791666666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.9368916666666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.9711322916666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": {
          "tp": 89,
          "fn": 311,
          "accuracy": 0.2225
        }
      },
      "auroc": 0.9540119791666668
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        }
      },
      "auroc": 0.965684375
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 235,
          "fn": 165,
          "accuracy": 0.5875
        }
      },
      "auroc": 0.9823067708333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 741,
          "fn": 59,
          "accuracy": 0.92625
        },
        "0.01": {
          "tp": 449,
          "fn": 351,
          "accuracy": 0.56125
        }
      },
      "auroc": 0.9739955729166667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.99463125
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9938177083333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9942244791666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        }
      },
      "auroc": 0.9407354166666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        }
      },
      "auroc": 0.9810447916666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        }
      },
      "auroc": 0.9608901041666668
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 280,
          "fn": 120,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9676833333333332
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        }
      },
      "auroc": 0.98743125
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 740,
          "fn": 60,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 577,
          "fn": 223,
          "accuracy": 0.72125
        }
      },
      "auroc": 0.9775572916666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        }
      },
      "auroc": 0.9912624999999999
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.9849708333333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        }
      },
      "auroc": 0.9881166666666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        }
      },
      "auroc": 0.9825812500000001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9756750000000001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        }
      },
      "auroc": 0.9791281250000001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        }
      },
      "auroc": 0.9869218750000001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        }
      },
      "auroc": 0.9803229166666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 777,
          "fn": 23,
          "accuracy": 0.97125
        },
        "0.01": {
          "tp": 463,
          "fn": 337,
          "accuracy": 0.57875
        }
      },
      "auroc": 0.9836223958333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        }
      },
      "auroc": 0.9870541666666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        }
      },
      "auroc": 0.9870541666666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        }
      },
      "auroc": 0.98293125
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        }
      },
      "auroc": 0.98293125
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        }
      },
      "auroc": 0.9849927083333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        }
      },
      "auroc": 0.9849927083333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.9316697916666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.9316697916666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.9153291666666665
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.9153291666666665
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        },
        "0.01": {
          "tp": 63,
          "fn": 337,
          "accuracy": 0.1575
        }
      },
      "auroc": 0.9234994791666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        },
        "0.01": {
          "tp": 63,
          "fn": 337,
          "accuracy": 0.1575
        }
      },
      "auroc": 0.9234994791666666
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.9887302083333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.9887302083333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        }
      },
      "auroc": 0.9802270833333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        }
      },
      "auroc": 0.9802270833333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        }
      },
      "auroc": 0.9844786458333332
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        }
      },
      "auroc": 0.9844786458333332
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        }
      },
      "auroc": 0.98545
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        }
      },
      "auroc": 0.98545
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.9619312499999999
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.9619312499999999
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        }
      },
      "auroc": 0.973690625
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        }
      },
      "auroc": 0.973690625
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        }
      },
      "auroc": 0.9450875
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        }
      },
      "auroc": 0.9450875
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        }
      },
      "auroc": 0.93960625
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        }
      },
      "auroc": 0.93960625
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        },
        "0.01": {
          "tp": 94,
          "fn": 306,
          "accuracy": 0.235
        }
      },
      "auroc": 0.9423468749999999
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        },
        "0.01": {
          "tp": 94,
          "fn": 306,
          "accuracy": 0.235
        }
      },
      "auroc": 0.9423468749999999
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2094,
          "fn": 106,
          "accuracy": 0.9518181818181818
        },
        "0.01": {
          "tp": 1474,
          "fn": 726,
          "accuracy": 0.67
        }
      },
      "auroc": 0.9809583333333333
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1187,
          "fn": 13,
          "accuracy": 0.9891666666666666
        },
        "0.01": {
          "tp": 918,
          "fn": 282,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9899274305555557
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3281,
          "fn": 119,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 2392,
          "fn": 1008,
          "accuracy": 0.7035294117647058
        }
      },
      "auroc": 0.9841238970588235
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1895,
          "fn": 305,
          "accuracy": 0.8613636363636363
        },
        "0.01": {
          "tp": 834,
          "fn": 1366,
          "accuracy": 0.3790909090909091
        }
      },
      "auroc": 0.959026231060606
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1148,
          "fn": 52,
          "accuracy": 0.9566666666666667
        },
        "0.01": {
          "tp": 625,
          "fn": 575,
          "accuracy": 0.5208333333333334
        }
      },
      "auroc": 0.9799831597222223
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3043,
          "fn": 357,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 1459,
          "fn": 1941,
          "accuracy": 0.42911764705882355
        }
      },
      "auroc": 0.9664227941176471
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3989,
          "fn": 411,
          "accuracy": 0.9065909090909091
        },
        "0.01": {
          "tp": 2308,
          "fn": 2092,
          "accuracy": 0.5245454545454545
        }
      },
      "auroc": 0.9699922821969696
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2335,
          "fn": 65,
          "accuracy": 0.9729166666666667
        },
        "0.01": {
          "tp": 1543,
          "fn": 857,
          "accuracy": 0.6429166666666667
        }
      },
      "auroc": 0.9849552951388888
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 6324,
          "fn": 476,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 3851,
          "fn": 2949,
          "accuracy": 0.5663235294117647
        }
      },
      "auroc": 0.9752733455882353
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.995175
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9951916666666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9951916666666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9952000000000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9949895833333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9951416666666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.995065625
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9680145833333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.99359375
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        }
      },
      "auroc": 0.9808041666666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9815020833333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9943677083333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 787,
          "fn": 13,
          "accuracy": 0.98375
        },
        "0.01": {
          "tp": 722,
          "fn": 78,
          "accuracy": 0.9025
        }
      },
      "auroc": 0.9879348958333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9948812499999999
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9933875
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9941343750000001
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9950447916666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9942979166666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9946713541666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.98926875
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9951416666666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9922052083333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9922385416666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.995175
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 775,
          "fn": 25,
          "accuracy": 0.96875
        }
      },
      "auroc": 0.9937067708333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9951625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9951625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9951625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        }
      },
      "auroc": 0.9640458333333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9949520833333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9794989583333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9796041666666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9950572916666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 787,
          "fn": 13,
          "accuracy": 0.98375
        },
        "0.01": {
          "tp": 746,
          "fn": 54,
          "accuracy": 0.9325
        }
      },
      "auroc": 0.9873307291666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9950791666666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9951625
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9951208333333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.992440625
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9947729166666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9936067708333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9937598958333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9949677083333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 786,
          "fn": 14,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9943638020833333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9940229166666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9940229166666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9933645833333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9933645833333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        }
      },
      "auroc": 0.99369375
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        }
      },
      "auroc": 0.99369375
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        }
      },
      "auroc": 0.970078125
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        }
      },
      "auroc": 0.970078125
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9496760416666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9496760416666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        }
      },
      "auroc": 0.9598770833333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        }
      },
      "auroc": 0.9598770833333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9933208333333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9933208333333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9942645833333335
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9942645833333335
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9820125
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9820125
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9713999999999999
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9713999999999999
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.97670625
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.97670625
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2180,
          "fn": 20,
          "accuracy": 0.990909090909091
        },
        "0.01": {
          "tp": 2102,
          "fn": 98,
          "accuracy": 0.9554545454545454
        }
      },
      "auroc": 0.9915805871212121
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1196,
          "fn": 4,
          "accuracy": 0.9966666666666667
        }
      },
      "auroc": 0.9951819444444445
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3380,
          "fn": 20,
          "accuracy": 0.9941176470588236
        },
        "0.01": {
          "tp": 3298,
          "fn": 102,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9928516544117647
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2133,
          "fn": 67,
          "accuracy": 0.9695454545454546
        },
        "0.01": {
          "tp": 1952,
          "fn": 248,
          "accuracy": 0.8872727272727273
        }
      },
      "auroc": 0.9824390151515151
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 1,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 1166,
          "fn": 34,
          "accuracy": 0.9716666666666667
        }
      },
      "auroc": 0.9945038194444444
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3332,
          "fn": 68,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 3118,
          "fn": 282,
          "accuracy": 0.9170588235294118
        }
      },
      "auroc": 0.986697181372549
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 4313,
          "fn": 87,
          "accuracy": 0.9802272727272727
        },
        "0.01": {
          "tp": 4054,
          "fn": 346,
          "accuracy": 0.9213636363636364
        }
      },
      "auroc": 0.9870098011363637
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2399,
          "fn": 1,
          "accuracy": 0.9995833333333334
        },
        "0.01": {
          "tp": 2362,
          "fn": 38,
          "accuracy": 0.9841666666666666
        }
      },
      "auroc": 0.9948428819444445
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 6712,
          "fn": 88,
          "accuracy": 0.9870588235294118
        },
        "0.01": {
          "tp": 6416,
          "fn": 384,
          "accuracy": 0.9435294117647058
        }
      },
      "auroc": 0.9897744178921568
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.995175
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9951916666666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9951916666666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9952000000000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9950625
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9951416666666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9951020833333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.9617166666666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9937145833333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        }
      },
      "auroc": 0.9777156250000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        }
      },
      "auroc": 0.9783895833333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.994428125
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 780,
          "fn": 20,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 707,
          "fn": 93,
          "accuracy": 0.88375
        }
      },
      "auroc": 0.9864088541666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9949072916666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9933875
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9941473958333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9950578125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9942979166666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9946778645833333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.987378125
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9951416666666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        }
      },
      "auroc": 0.9912598958333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.9912932291666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.995175
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 761,
          "fn": 39,
          "accuracy": 0.95125
        }
      },
      "auroc": 0.9932341145833333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.995175
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9951625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9951687499999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9527937499999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9944291666666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9736114583333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        }
      },
      "auroc": 0.973984375
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9947958333333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 778,
          "fn": 22,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 732,
          "fn": 68,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9843901041666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.995075
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9951625
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9951187499999999
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9931635416666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9945427083333332
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.993853125
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9941192708333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9948526041666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 786,
          "fn": 14,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9944859375
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9941614583333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9941614583333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9939020833333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9939020833333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9940317708333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9940317708333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9670197916666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9670197916666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        }
      },
      "auroc": 0.9425114583333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        }
      },
      "auroc": 0.9425114583333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": {
          "tp": 291,
          "fn": 109,
          "accuracy": 0.7275
        }
      },
      "auroc": 0.9547656250000001
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": {
          "tp": 291,
          "fn": 109,
          "accuracy": 0.7275
        }
      },
      "auroc": 0.9547656250000001
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9934822916666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9934822916666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943453125
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9943453125
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9824645833333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9824645833333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.971915625
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.971915625
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        }
      },
      "auroc": 0.9771901041666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        }
      },
      "auroc": 0.9771901041666666
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2181,
          "fn": 19,
          "accuracy": 0.9913636363636363
        },
        "0.01": {
          "tp": 2103,
          "fn": 97,
          "accuracy": 0.9559090909090909
        }
      },
      "auroc": 0.9913636363636364
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1196,
          "fn": 4,
          "accuracy": 0.9966666666666667
        }
      },
      "auroc": 0.9951819444444445
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3381,
          "fn": 19,
          "accuracy": 0.9944117647058823
        },
        "0.01": {
          "tp": 3299,
          "fn": 101,
          "accuracy": 0.9702941176470589
        }
      },
      "auroc": 0.9927112745098039
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2115,
          "fn": 85,
          "accuracy": 0.9613636363636363
        },
        "0.01": {
          "tp": 1908,
          "fn": 292,
          "accuracy": 0.8672727272727273
        }
      },
      "auroc": 0.9801988636363635
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        },
        "0.01": {
          "tp": 1166,
          "fn": 34,
          "accuracy": 0.9716666666666667
        }
      },
      "auroc": 0.9943984375
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3313,
          "fn": 87,
          "accuracy": 0.9744117647058823
        },
        "0.01": {
          "tp": 3074,
          "fn": 326,
          "accuracy": 0.9041176470588236
        }
      },
      "auroc": 0.9852104779411763
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 4296,
          "fn": 104,
          "accuracy": 0.9763636363636363
        },
        "0.01": {
          "tp": 4011,
          "fn": 389,
          "accuracy": 0.9115909090909091
        }
      },
      "auroc": 0.98578125
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2398,
          "fn": 2,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 2362,
          "fn": 38,
          "accuracy": 0.9841666666666666
        }
      },
      "auroc": 0.9947901909722222
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 6694,
          "fn": 106,
          "accuracy": 0.9844117647058823
        },
        "0.01": {
          "tp": 6373,
          "fn": 427,
          "accuracy": 0.9372058823529412
        }
      },
      "auroc": 0.9889608762254902
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.9785458333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        }
      },
      "auroc": 0.9770052083333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 113,
          "fn": 287,
          "accuracy": 0.2825
        }
      },
      "auroc": 0.9777755208333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.9759354166666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 47,
          "fn": 153,
          "accuracy": 0.235
        }
      },
      "auroc": 0.9763541666666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 109,
          "fn": 291,
          "accuracy": 0.2725
        }
      },
      "auroc": 0.9761447916666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 120,
          "fn": 280,
          "accuracy": 0.3
        }
      },
      "auroc": 0.977240625
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 102,
          "fn": 298,
          "accuracy": 0.255
        }
      },
      "auroc": 0.9766796875
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 786,
          "fn": 14,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 222,
          "fn": 578,
          "accuracy": 0.2775
        }
      },
      "auroc": 0.97696015625
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9872000000000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.990146875
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9886734375
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        }
      },
      "auroc": 0.9400385416666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        }
      },
      "auroc": 0.978440625
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 97,
          "fn": 303,
          "accuracy": 0.2425
        }
      },
      "auroc": 0.9592395833333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 158,
          "fn": 242,
          "accuracy": 0.395
        }
      },
      "auroc": 0.9636192708333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        }
      },
      "auroc": 0.98429375
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 741,
          "fn": 59,
          "accuracy": 0.92625
        },
        "0.01": {
          "tp": 345,
          "fn": 455,
          "accuracy": 0.43125
        }
      },
      "auroc": 0.9739565104166666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9737010416666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.9711125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 89,
          "fn": 311,
          "accuracy": 0.2225
        }
      },
      "auroc": 0.9724067708333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        }
      },
      "auroc": 0.969265625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        }
      },
      "auroc": 0.964784375
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 75,
          "fn": 325,
          "accuracy": 0.1875
        }
      },
      "auroc": 0.967025
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 98,
          "fn": 302,
          "accuracy": 0.245
        }
      },
      "auroc": 0.9714833333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 66,
          "fn": 334,
          "accuracy": 0.165
        }
      },
      "auroc": 0.9679484375
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 761,
          "fn": 39,
          "accuracy": 0.95125
        },
        "0.01": {
          "tp": 164,
          "fn": 636,
          "accuracy": 0.205
        }
      },
      "auroc": 0.9697158854166666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9925270833333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9877416666666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        }
      },
      "auroc": 0.990134375
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.9403416666666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.964071875
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 42,
          "fn": 358,
          "accuracy": 0.105
        }
      },
      "auroc": 0.9522067708333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        }
      },
      "auroc": 0.9664343750000001
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 148,
          "fn": 252,
          "accuracy": 0.37
        }
      },
      "auroc": 0.9759067708333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 745,
          "fn": 55,
          "accuracy": 0.93125
        },
        "0.01": {
          "tp": 328,
          "fn": 472,
          "accuracy": 0.41
        }
      },
      "auroc": 0.9711705729166666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9939177083333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9916395833333332
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.9927786458333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.948715625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        }
      },
      "auroc": 0.9734364583333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": {
          "tp": 164,
          "fn": 236,
          "accuracy": 0.41
        }
      },
      "auroc": 0.9610760416666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 261,
          "fn": 139,
          "accuracy": 0.6525
        }
      },
      "auroc": 0.9713166666666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        }
      },
      "auroc": 0.9825380208333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 747,
          "fn": 53,
          "accuracy": 0.93375
        },
        "0.01": {
          "tp": 527,
          "fn": 273,
          "accuracy": 0.65875
        }
      },
      "auroc": 0.97692734375
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.9780781249999999
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.969103125
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 92,
          "fn": 308,
          "accuracy": 0.23
        }
      },
      "auroc": 0.973590625
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.9659177083333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        }
      },
      "auroc": 0.9629010416666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 69,
          "fn": 331,
          "accuracy": 0.1725
        }
      },
      "auroc": 0.9644093749999999
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 100,
          "fn": 300,
          "accuracy": 0.25
        }
      },
      "auroc": 0.9719979166666668
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 61,
          "fn": 339,
          "accuracy": 0.1525
        }
      },
      "auroc": 0.9660020833333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 758,
          "fn": 42,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 161,
          "fn": 639,
          "accuracy": 0.20125
        }
      },
      "auroc": 0.969
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.9763552083333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.9763552083333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        }
      },
      "auroc": 0.9661739583333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        }
      },
      "auroc": 0.9661739583333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 74,
          "fn": 326,
          "accuracy": 0.185
        }
      },
      "auroc": 0.9712645833333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 74,
          "fn": 326,
          "accuracy": 0.185
        }
      },
      "auroc": 0.9712645833333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.931925
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.931925
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.9264302083333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.9264302083333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 22,
          "fn": 378,
          "accuracy": 0.055
        }
      },
      "auroc": 0.9291776041666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 22,
          "fn": 378,
          "accuracy": 0.055
        }
      },
      "auroc": 0.9291776041666666
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.9681833333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.9681833333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.95994375
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.95994375
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 43,
          "fn": 357,
          "accuracy": 0.1075
        }
      },
      "auroc": 0.9640635416666665
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 43,
          "fn": 357,
          "accuracy": 0.1075
        }
      },
      "auroc": 0.9640635416666665
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.9728802083333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.9728802083333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.9524333333333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.9524333333333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 45,
          "fn": 355,
          "accuracy": 0.1125
        }
      },
      "auroc": 0.9626567708333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 45,
          "fn": 355,
          "accuracy": 0.1125
        }
      },
      "auroc": 0.9626567708333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        }
      },
      "auroc": 0.9438677083333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        }
      },
      "auroc": 0.9438677083333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.9394802083333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.9394802083333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 32,
          "fn": 368,
          "accuracy": 0.08
        }
      },
      "auroc": 0.9416739583333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 32,
          "fn": 368,
          "accuracy": 0.08
        }
      },
      "auroc": 0.9416739583333333
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2064,
          "fn": 136,
          "accuracy": 0.9381818181818182
        },
        "0.01": {
          "tp": 781,
          "fn": 1419,
          "accuracy": 0.355
        }
      },
      "auroc": 0.9724710227272728
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1184,
          "fn": 16,
          "accuracy": 0.9866666666666667
        },
        "0.01": {
          "tp": 546,
          "fn": 654,
          "accuracy": 0.455
        }
      },
      "auroc": 0.981124826388889
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3248,
          "fn": 152,
          "accuracy": 0.9552941176470588
        },
        "0.01": {
          "tp": 1327,
          "fn": 2073,
          "accuracy": 0.39029411764705885
        }
      },
      "auroc": 0.975525306372549
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1869,
          "fn": 331,
          "accuracy": 0.8495454545454545
        },
        "0.01": {
          "tp": 352,
          "fn": 1848,
          "accuracy": 0.16
        }
      },
      "auroc": 0.9531523674242424
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1138,
          "fn": 62,
          "accuracy": 0.9483333333333334
        },
        "0.01": {
          "tp": 284,
          "fn": 916,
          "accuracy": 0.23666666666666666
        }
      },
      "auroc": 0.9699980902777777
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3007,
          "fn": 393,
          "accuracy": 0.8844117647058823
        },
        "0.01": {
          "tp": 636,
          "fn": 2764,
          "accuracy": 0.18705882352941178
        }
      },
      "auroc": 0.9590979166666666
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3933,
          "fn": 467,
          "accuracy": 0.8938636363636364
        },
        "0.01": {
          "tp": 1133,
          "fn": 3267,
          "accuracy": 0.2575
        }
      },
      "auroc": 0.9628116950757577
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2322,
          "fn": 78,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 830,
          "fn": 1570,
          "accuracy": 0.3458333333333333
        }
      },
      "auroc": 0.9755614583333334
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 6255,
          "fn": 545,
          "accuracy": 0.9198529411764705
        },
        "0.01": {
          "tp": 1963,
          "fn": 4837,
          "accuracy": 0.2886764705882353
        }
      },
      "auroc": 0.9673116115196078
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2399,
          "fn": 1,
          "accuracy": 0.9995833333333334
        },
        "0.01": {
          "tp": 2208,
          "fn": 192,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9934940104166666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2396,
          "fn": 4,
          "accuracy": 0.9983333333333333
        },
        "0.01": {
          "tp": 2192,
          "fn": 208,
          "accuracy": 0.9133333333333333
        }
      },
      "auroc": 0.9931355902777778
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4795,
          "fn": 5,
          "accuracy": 0.9989583333333333
        },
        "0.01": {
          "tp": 4400,
          "fn": 400,
          "accuracy": 0.9166666666666666
        }
      },
      "auroc": 0.9933148003472222
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2391,
          "fn": 9,
          "accuracy": 0.99625
        },
        "0.01": {
          "tp": 2205,
          "fn": 195,
          "accuracy": 0.91875
        }
      },
      "auroc": 0.9931019097222222
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2390,
          "fn": 10,
          "accuracy": 0.9958333333333333
        },
        "0.01": {
          "tp": 2154,
          "fn": 246,
          "accuracy": 0.8975
        }
      },
      "auroc": 0.9926663194444444
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4781,
          "fn": 19,
          "accuracy": 0.9960416666666667
        },
        "0.01": {
          "tp": 4359,
          "fn": 441,
          "accuracy": 0.908125
        }
      },
      "auroc": 0.9928841145833334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4790,
          "fn": 10,
          "accuracy": 0.9979166666666667
        },
        "0.01": {
          "tp": 4413,
          "fn": 387,
          "accuracy": 0.919375
        }
      },
      "auroc": 0.9932979600694445
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4786,
          "fn": 14,
          "accuracy": 0.9970833333333333
        },
        "0.01": {
          "tp": 4346,
          "fn": 454,
          "accuracy": 0.9054166666666666
        }
      },
      "auroc": 0.9929009548611111
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9576,
          "fn": 24,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 8759,
          "fn": 841,
          "accuracy": 0.9123958333333333
        }
      },
      "auroc": 0.9930994574652778
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2394,
          "fn": 6,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 2248,
          "fn": 152,
          "accuracy": 0.9366666666666666
        }
      },
      "auroc": 0.9940780381944444
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2397,
          "fn": 3,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 2252,
          "fn": 148,
          "accuracy": 0.9383333333333334
        }
      },
      "auroc": 0.9939818576388889
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4791,
          "fn": 9,
          "accuracy": 0.998125
        },
        "0.01": {
          "tp": 4500,
          "fn": 300,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9940299479166668
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2117,
          "fn": 283,
          "accuracy": 0.8820833333333333
        },
        "0.01": {
          "tp": 1360,
          "fn": 1040,
          "accuracy": 0.5666666666666667
        }
      },
      "auroc": 0.9588764756944446
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2381,
          "fn": 19,
          "accuracy": 0.9920833333333333
        },
        "0.01": {
          "tp": 1978,
          "fn": 422,
          "accuracy": 0.8241666666666667
        }
      },
      "auroc": 0.9904453125
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4498,
          "fn": 302,
          "accuracy": 0.9370833333333334
        },
        "0.01": {
          "tp": 3338,
          "fn": 1462,
          "accuracy": 0.6954166666666667
        }
      },
      "auroc": 0.9746608940972222
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4511,
          "fn": 289,
          "accuracy": 0.9397916666666667
        },
        "0.01": {
          "tp": 3608,
          "fn": 1192,
          "accuracy": 0.7516666666666667
        }
      },
      "auroc": 0.9764772569444444
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4778,
          "fn": 22,
          "accuracy": 0.9954166666666666
        },
        "0.01": {
          "tp": 4230,
          "fn": 570,
          "accuracy": 0.88125
        }
      },
      "auroc": 0.9922135850694445
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9289,
          "fn": 311,
          "accuracy": 0.9676041666666667
        },
        "0.01": {
          "tp": 7838,
          "fn": 1762,
          "accuracy": 0.8164583333333333
        }
      },
      "auroc": 0.9843454210069444
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2387,
          "fn": 13,
          "accuracy": 0.9945833333333334
        },
        "0.01": {
          "tp": 2170,
          "fn": 230,
          "accuracy": 0.9041666666666667
        }
      },
      "auroc": 0.9927378472222224
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2387,
          "fn": 13,
          "accuracy": 0.9945833333333334
        },
        "0.01": {
          "tp": 2123,
          "fn": 277,
          "accuracy": 0.8845833333333334
        }
      },
      "auroc": 0.9920936631944445
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4774,
          "fn": 26,
          "accuracy": 0.9945833333333334
        },
        "0.01": {
          "tp": 4293,
          "fn": 507,
          "accuracy": 0.894375
        }
      },
      "auroc": 0.9924157552083334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2381,
          "fn": 19,
          "accuracy": 0.9920833333333333
        },
        "0.01": {
          "tp": 2093,
          "fn": 307,
          "accuracy": 0.8720833333333333
        }
      },
      "auroc": 0.9915687499999999
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2366,
          "fn": 34,
          "accuracy": 0.9858333333333333
        },
        "0.01": {
          "tp": 2039,
          "fn": 361,
          "accuracy": 0.8495833333333334
        }
      },
      "auroc": 0.989293576388889
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4747,
          "fn": 53,
          "accuracy": 0.9889583333333334
        },
        "0.01": {
          "tp": 4132,
          "fn": 668,
          "accuracy": 0.8608333333333333
        }
      },
      "auroc": 0.9904311631944445
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4768,
          "fn": 32,
          "accuracy": 0.9933333333333333
        },
        "0.01": {
          "tp": 4263,
          "fn": 537,
          "accuracy": 0.888125
        }
      },
      "auroc": 0.9921532986111111
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4753,
          "fn": 47,
          "accuracy": 0.9902083333333334
        },
        "0.01": {
          "tp": 4162,
          "fn": 638,
          "accuracy": 0.8670833333333333
        }
      },
      "auroc": 0.9906936197916667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9521,
          "fn": 79,
          "accuracy": 0.9917708333333334
        },
        "0.01": {
          "tp": 8425,
          "fn": 1175,
          "accuracy": 0.8776041666666666
        }
      },
      "auroc": 0.9914234592013889
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2332,
          "fn": 68,
          "accuracy": 0.9716666666666667
        }
      },
      "auroc": 0.99483125
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2398,
          "fn": 2,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 2282,
          "fn": 118,
          "accuracy": 0.9508333333333333
        }
      },
      "auroc": 0.9942600694444444
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4798,
          "fn": 2,
          "accuracy": 0.9995833333333334
        },
        "0.01": {
          "tp": 4614,
          "fn": 186,
          "accuracy": 0.96125
        }
      },
      "auroc": 0.9945456597222222
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2270,
          "fn": 130,
          "accuracy": 0.9458333333333333
        },
        "0.01": {
          "tp": 1626,
          "fn": 774,
          "accuracy": 0.6775
        }
      },
      "auroc": 0.9787598090277778
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2371,
          "fn": 29,
          "accuracy": 0.9879166666666667
        },
        "0.01": {
          "tp": 2025,
          "fn": 375,
          "accuracy": 0.84375
        }
      },
      "auroc": 0.9901388020833334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4641,
          "fn": 159,
          "accuracy": 0.966875
        },
        "0.01": {
          "tp": 3651,
          "fn": 1149,
          "accuracy": 0.760625
        }
      },
      "auroc": 0.9844493055555555
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4670,
          "fn": 130,
          "accuracy": 0.9729166666666667
        },
        "0.01": {
          "tp": 3958,
          "fn": 842,
          "accuracy": 0.8245833333333333
        }
      },
      "auroc": 0.9867955295138888
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4769,
          "fn": 31,
          "accuracy": 0.9935416666666667
        },
        "0.01": {
          "tp": 4307,
          "fn": 493,
          "accuracy": 0.8972916666666667
        }
      },
      "auroc": 0.9921994357638888
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9439,
          "fn": 161,
          "accuracy": 0.9832291666666667
        },
        "0.01": {
          "tp": 8265,
          "fn": 1335,
          "accuracy": 0.8609375
        }
      },
      "auroc": 0.9894974826388889
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2355,
          "fn": 45,
          "accuracy": 0.98125
        }
      },
      "auroc": 0.994903125
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2395,
          "fn": 5,
          "accuracy": 0.9979166666666667
        },
        "0.01": {
          "tp": 2322,
          "fn": 78,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9939928819444445
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4795,
          "fn": 5,
          "accuracy": 0.9989583333333333
        },
        "0.01": {
          "tp": 4677,
          "fn": 123,
          "accuracy": 0.974375
        }
      },
      "auroc": 0.9944480034722223
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2121,
          "fn": 279,
          "accuracy": 0.88375
        },
        "0.01": {
          "tp": 1503,
          "fn": 897,
          "accuracy": 0.62625
        }
      },
      "auroc": 0.9547270833333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2367,
          "fn": 33,
          "accuracy": 0.98625
        },
        "0.01": {
          "tp": 2097,
          "fn": 303,
          "accuracy": 0.87375
        }
      },
      "auroc": 0.99032578125
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4488,
          "fn": 312,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 3600,
          "fn": 1200,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9725264322916667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4521,
          "fn": 279,
          "accuracy": 0.941875
        },
        "0.01": {
          "tp": 3858,
          "fn": 942,
          "accuracy": 0.80375
        }
      },
      "auroc": 0.9748151041666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4762,
          "fn": 38,
          "accuracy": 0.9920833333333333
        },
        "0.01": {
          "tp": 4419,
          "fn": 381,
          "accuracy": 0.920625
        }
      },
      "auroc": 0.9921593315972221
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9283,
          "fn": 317,
          "accuracy": 0.9669791666666666
        },
        "0.01": {
          "tp": 8277,
          "fn": 1323,
          "accuracy": 0.8621875
        }
      },
      "auroc": 0.9834872178819445
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2395,
          "fn": 5,
          "accuracy": 0.9979166666666667
        },
        "0.01": {
          "tp": 2177,
          "fn": 223,
          "accuracy": 0.9070833333333334
        }
      },
      "auroc": 0.9932600694444444
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2389,
          "fn": 11,
          "accuracy": 0.9954166666666666
        },
        "0.01": {
          "tp": 2124,
          "fn": 276,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9920847222222223
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4784,
          "fn": 16,
          "accuracy": 0.9966666666666667
        },
        "0.01": {
          "tp": 4301,
          "fn": 499,
          "accuracy": 0.8960416666666666
        }
      },
      "auroc": 0.9926723958333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2358,
          "fn": 42,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 2082,
          "fn": 318,
          "accuracy": 0.8675
        }
      },
      "auroc": 0.9895272569444444
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2367,
          "fn": 33,
          "accuracy": 0.98625
        },
        "0.01": {
          "tp": 2031,
          "fn": 369,
          "accuracy": 0.84625
        }
      },
      "auroc": 0.9896745659722223
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4725,
          "fn": 75,
          "accuracy": 0.984375
        },
        "0.01": {
          "tp": 4113,
          "fn": 687,
          "accuracy": 0.856875
        }
      },
      "auroc": 0.9896009114583333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4753,
          "fn": 47,
          "accuracy": 0.9902083333333334
        },
        "0.01": {
          "tp": 4259,
          "fn": 541,
          "accuracy": 0.8872916666666667
        }
      },
      "auroc": 0.9913936631944444
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4756,
          "fn": 44,
          "accuracy": 0.9908333333333333
        },
        "0.01": {
          "tp": 4155,
          "fn": 645,
          "accuracy": 0.865625
        }
      },
      "auroc": 0.9908796440972222
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9509,
          "fn": 91,
          "accuracy": 0.9905208333333333
        },
        "0.01": {
          "tp": 8414,
          "fn": 1186,
          "accuracy": 0.8764583333333333
        }
      },
      "auroc": 0.9911366536458333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2387,
          "fn": 13,
          "accuracy": 0.9945833333333334
        },
        "0.01": {
          "tp": 2007,
          "fn": 393,
          "accuracy": 0.83625
        }
      },
      "auroc": 0.9916214409722222
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2387,
          "fn": 13,
          "accuracy": 0.9945833333333334
        },
        "0.01": {
          "tp": 2007,
          "fn": 393,
          "accuracy": 0.83625
        }
      },
      "auroc": 0.9916214409722222
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2379,
          "fn": 21,
          "accuracy": 0.99125
        },
        "0.01": {
          "tp": 1957,
          "fn": 443,
          "accuracy": 0.8154166666666667
        }
      },
      "auroc": 0.9903998263888889
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2379,
          "fn": 21,
          "accuracy": 0.99125
        },
        "0.01": {
          "tp": 1957,
          "fn": 443,
          "accuracy": 0.8154166666666667
        }
      },
      "auroc": 0.9903998263888889
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4766,
          "fn": 34,
          "accuracy": 0.9929166666666667
        },
        "0.01": {
          "tp": 3964,
          "fn": 836,
          "accuracy": 0.8258333333333333
        }
      },
      "auroc": 0.9910106336805555
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4766,
          "fn": 34,
          "accuracy": 0.9929166666666667
        },
        "0.01": {
          "tp": 3964,
          "fn": 836,
          "accuracy": 0.8258333333333333
        }
      },
      "auroc": 0.9910106336805555
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2116,
          "fn": 284,
          "accuracy": 0.8816666666666667
        },
        "0.01": {
          "tp": 1510,
          "fn": 890,
          "accuracy": 0.6291666666666667
        }
      },
      "auroc": 0.9561354166666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2116,
          "fn": 284,
          "accuracy": 0.8816666666666667
        },
        "0.01": {
          "tp": 1510,
          "fn": 890,
          "accuracy": 0.6291666666666667
        }
      },
      "auroc": 0.9561354166666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1966,
          "fn": 434,
          "accuracy": 0.8191666666666667
        },
        "0.01": {
          "tp": 1342,
          "fn": 1058,
          "accuracy": 0.5591666666666667
        }
      },
      "auroc": 0.9354209201388889
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1966,
          "fn": 434,
          "accuracy": 0.8191666666666667
        },
        "0.01": {
          "tp": 1342,
          "fn": 1058,
          "accuracy": 0.5591666666666667
        }
      },
      "auroc": 0.9354209201388889
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4082,
          "fn": 718,
          "accuracy": 0.8504166666666667
        },
        "0.01": {
          "tp": 2852,
          "fn": 1948,
          "accuracy": 0.5941666666666666
        }
      },
      "auroc": 0.9457781684027777
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4082,
          "fn": 718,
          "accuracy": 0.8504166666666667
        },
        "0.01": {
          "tp": 2852,
          "fn": 1948,
          "accuracy": 0.5941666666666666
        }
      },
      "auroc": 0.9457781684027777
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2384,
          "fn": 16,
          "accuracy": 0.9933333333333333
        },
        "0.01": {
          "tp": 2153,
          "fn": 247,
          "accuracy": 0.8970833333333333
        }
      },
      "auroc": 0.9923744791666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2384,
          "fn": 16,
          "accuracy": 0.9933333333333333
        },
        "0.01": {
          "tp": 2153,
          "fn": 247,
          "accuracy": 0.8970833333333333
        }
      },
      "auroc": 0.9923744791666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2375,
          "fn": 25,
          "accuracy": 0.9895833333333334
        },
        "0.01": {
          "tp": 2095,
          "fn": 305,
          "accuracy": 0.8729166666666667
        }
      },
      "auroc": 0.9909728298611111
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2375,
          "fn": 25,
          "accuracy": 0.9895833333333334
        },
        "0.01": {
          "tp": 2095,
          "fn": 305,
          "accuracy": 0.8729166666666667
        }
      },
      "auroc": 0.9909728298611111
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4759,
          "fn": 41,
          "accuracy": 0.9914583333333333
        },
        "0.01": {
          "tp": 4248,
          "fn": 552,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9916736545138889
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4759,
          "fn": 41,
          "accuracy": 0.9914583333333333
        },
        "0.01": {
          "tp": 4248,
          "fn": 552,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9916736545138889
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2389,
          "fn": 11,
          "accuracy": 0.9954166666666666
        },
        "0.01": {
          "tp": 2160,
          "fn": 240,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9923383680555555
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2389,
          "fn": 11,
          "accuracy": 0.9954166666666666
        },
        "0.01": {
          "tp": 2160,
          "fn": 240,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9923383680555555
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2350,
          "fn": 50,
          "accuracy": 0.9791666666666666
        },
        "0.01": {
          "tp": 1960,
          "fn": 440,
          "accuracy": 0.8166666666666667
        }
      },
      "auroc": 0.9864066840277776
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2350,
          "fn": 50,
          "accuracy": 0.9791666666666666
        },
        "0.01": {
          "tp": 1960,
          "fn": 440,
          "accuracy": 0.8166666666666667
        }
      },
      "auroc": 0.9864066840277776
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4739,
          "fn": 61,
          "accuracy": 0.9872916666666667
        },
        "0.01": {
          "tp": 4120,
          "fn": 680,
          "accuracy": 0.8583333333333333
        }
      },
      "auroc": 0.9893725260416667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4739,
          "fn": 61,
          "accuracy": 0.9872916666666667
        },
        "0.01": {
          "tp": 4120,
          "fn": 680,
          "accuracy": 0.8583333333333333
        }
      },
      "auroc": 0.9893725260416667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2239,
          "fn": 161,
          "accuracy": 0.9329166666666666
        },
        "0.01": {
          "tp": 1678,
          "fn": 722,
          "accuracy": 0.6991666666666667
        }
      },
      "auroc": 0.9731571180555556
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2239,
          "fn": 161,
          "accuracy": 0.9329166666666666
        },
        "0.01": {
          "tp": 1678,
          "fn": 722,
          "accuracy": 0.6991666666666667
        }
      },
      "auroc": 0.9731571180555556
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2158,
          "fn": 242,
          "accuracy": 0.8991666666666667
        },
        "0.01": {
          "tp": 1605,
          "fn": 795,
          "accuracy": 0.66875
        }
      },
      "auroc": 0.9630010416666668
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2158,
          "fn": 242,
          "accuracy": 0.8991666666666667
        },
        "0.01": {
          "tp": 1605,
          "fn": 795,
          "accuracy": 0.66875
        }
      },
      "auroc": 0.9630010416666668
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4397,
          "fn": 403,
          "accuracy": 0.9160416666666666
        },
        "0.01": {
          "tp": 3283,
          "fn": 1517,
          "accuracy": 0.6839583333333333
        }
      },
      "auroc": 0.968079079861111
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4397,
          "fn": 403,
          "accuracy": 0.9160416666666666
        },
        "0.01": {
          "tp": 3283,
          "fn": 1517,
          "accuracy": 0.6839583333333333
        }
      },
      "auroc": 0.968079079861111
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 25890,
          "fn": 510,
          "accuracy": 0.9806818181818182
        },
        "0.01": {
          "tp": 22998,
          "fn": 3402,
          "accuracy": 0.8711363636363636
        }
      },
      "auroc": 0.988084651199495
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 14362,
          "fn": 38,
          "accuracy": 0.9973611111111111
        },
        "0.01": {
          "tp": 13295,
          "fn": 1105,
          "accuracy": 0.9232638888888889
        }
      },
      "auroc": 0.993258130787037
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 40252,
          "fn": 548,
          "accuracy": 0.9865686274509804
        },
        "0.01": {
          "tp": 36293,
          "fn": 4507,
          "accuracy": 0.8895343137254902
        }
      },
      "auroc": 0.9899105851715686
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 24866,
          "fn": 1534,
          "accuracy": 0.9418939393939394
        },
        "0.01": {
          "tp": 19828,
          "fn": 6572,
          "accuracy": 0.7510606060606061
        }
      },
      "auroc": 0.975705689709596
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 14242,
          "fn": 158,
          "accuracy": 0.9890277777777777
        },
        "0.01": {
          "tp": 12324,
          "fn": 2076,
          "accuracy": 0.8558333333333333
        }
      },
      "auroc": 0.9904240596064815
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 39108,
          "fn": 1692,
          "accuracy": 0.9585294117647059
        },
        "0.01": {
          "tp": 32152,
          "fn": 8648,
          "accuracy": 0.7880392156862746
        }
      },
      "auroc": 0.9809004084967321
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 50756,
          "fn": 2044,
          "accuracy": 0.9612878787878788
        },
        "0.01": {
          "tp": 42826,
          "fn": 9974,
          "accuracy": 0.8110984848484849
        }
      },
      "auroc": 0.9818951704545456
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 28604,
          "fn": 196,
          "accuracy": 0.9931944444444445
        },
        "0.01": {
          "tp": 25619,
          "fn": 3181,
          "accuracy": 0.8895486111111112
        }
      },
      "auroc": 0.9918410951967592
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 79360,
          "fn": 2240,
          "accuracy": 0.9725490196078431
        },
        "0.01": {
          "tp": 68445,
          "fn": 13155,
          "accuracy": 0.8387867647058823
        }
      },
      "auroc": 0.9854054968341502
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9897916666666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9903125
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9654322916666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9892802083333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.97735625
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        }
      },
      "auroc": 0.9781328125000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9895359374999999
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 781,
          "fn": 19,
          "accuracy": 0.97625
        },
        "0.01": {
          "tp": 715,
          "fn": 85,
          "accuracy": 0.89375
        }
      },
      "auroc": 0.983834375
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9847072916666668
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9877703125
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9877703125
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 793,
          "fn": 7,
          "accuracy": 0.99125
        }
      },
      "auroc": 0.9893018229166667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9861500000000001
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.98955
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9878500000000001
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9884916666666668
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9901916666666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 766,
          "fn": 34,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9893416666666668
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9907395833333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9907864583333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.9535552083333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.98578125
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9696682291666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        }
      },
      "auroc": 0.9721942708333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9882604166666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 762,
          "fn": 38,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 685,
          "fn": 115,
          "accuracy": 0.85625
        }
      },
      "auroc": 0.98022734375
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9907666666666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9908
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9908
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9908166666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9906802083333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9906802083333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9904739583333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9904739583333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9905770833333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9905770833333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9828072916666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9828072916666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9512520833333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9512520833333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9670296875
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9670296875
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9895864583333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9895864583333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9791562500000001
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9791562500000001
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9843713541666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9843713541666667
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2193,
          "fn": 7,
          "accuracy": 0.9968181818181818
        },
        "0.01": {
          "tp": 2176,
          "fn": 24,
          "accuracy": 0.9890909090909091
        }
      },
      "auroc": 0.9899764204545455
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 1,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        }
      },
      "auroc": 0.9906440972222222
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3392,
          "fn": 8,
          "accuracy": 0.9976470588235294
        },
        "0.01": {
          "tp": 3374,
          "fn": 26,
          "accuracy": 0.9923529411764705
        }
      },
      "auroc": 0.9902120710784313
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2108,
          "fn": 92,
          "accuracy": 0.9581818181818181
        },
        "0.01": {
          "tp": 1921,
          "fn": 279,
          "accuracy": 0.8731818181818182
        }
      },
      "auroc": 0.9800169507575757
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1190,
          "fn": 10,
          "accuracy": 0.9916666666666667
        },
        "0.01": {
          "tp": 1154,
          "fn": 46,
          "accuracy": 0.9616666666666667
        }
      },
      "auroc": 0.9884864583333335
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3298,
          "fn": 102,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 3075,
          "fn": 325,
          "accuracy": 0.9044117647058824
        }
      },
      "auroc": 0.9830061887254902
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 4301,
          "fn": 99,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 4097,
          "fn": 303,
          "accuracy": 0.9311363636363637
        }
      },
      "auroc": 0.9849966856060607
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2389,
          "fn": 11,
          "accuracy": 0.9954166666666666
        },
        "0.01": {
          "tp": 2352,
          "fn": 48,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9895652777777777
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 6690,
          "fn": 110,
          "accuracy": 0.9838235294117647
        },
        "0.01": {
          "tp": 6449,
          "fn": 351,
          "accuracy": 0.9483823529411765
        }
      },
      "auroc": 0.9866091299019608
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9897916666666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9903125
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9654322916666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9892802083333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.97735625
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        }
      },
      "auroc": 0.9781328125000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9895359374999999
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 781,
          "fn": 19,
          "accuracy": 0.97625
        },
        "0.01": {
          "tp": 715,
          "fn": 85,
          "accuracy": 0.89375
        }
      },
      "auroc": 0.983834375
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9847072916666668
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9877703125
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9877703125
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 793,
          "fn": 7,
          "accuracy": 0.99125
        }
      },
      "auroc": 0.9893018229166667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9861500000000001
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.98955
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9878500000000001
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9884916666666668
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9901916666666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 766,
          "fn": 34,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9893416666666668
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9907395833333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9907864583333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.9535552083333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.98578125
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9696682291666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        }
      },
      "auroc": 0.9721942708333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9882604166666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 762,
          "fn": 38,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 685,
          "fn": 115,
          "accuracy": 0.85625
        }
      },
      "auroc": 0.98022734375
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9907666666666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9908
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9908
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9908166666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9906802083333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9906802083333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9904739583333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9904739583333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9905770833333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9905770833333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9828072916666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9828072916666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9512520833333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9512520833333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9670296875
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9670296875
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9895864583333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9895864583333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9791562500000001
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9791562500000001
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9843713541666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9843713541666667
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2193,
          "fn": 7,
          "accuracy": 0.9968181818181818
        },
        "0.01": {
          "tp": 2176,
          "fn": 24,
          "accuracy": 0.9890909090909091
        }
      },
      "auroc": 0.9899764204545455
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 1,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        }
      },
      "auroc": 0.9906440972222222
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3392,
          "fn": 8,
          "accuracy": 0.9976470588235294
        },
        "0.01": {
          "tp": 3374,
          "fn": 26,
          "accuracy": 0.9923529411764705
        }
      },
      "auroc": 0.9902120710784313
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2108,
          "fn": 92,
          "accuracy": 0.9581818181818181
        },
        "0.01": {
          "tp": 1921,
          "fn": 279,
          "accuracy": 0.8731818181818182
        }
      },
      "auroc": 0.9800169507575757
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1190,
          "fn": 10,
          "accuracy": 0.9916666666666667
        },
        "0.01": {
          "tp": 1154,
          "fn": 46,
          "accuracy": 0.9616666666666667
        }
      },
      "auroc": 0.9884864583333335
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3298,
          "fn": 102,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 3075,
          "fn": 325,
          "accuracy": 0.9044117647058824
        }
      },
      "auroc": 0.9830061887254902
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 4301,
          "fn": 99,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 4097,
          "fn": 303,
          "accuracy": 0.9311363636363637
        }
      },
      "auroc": 0.9849966856060607
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2389,
          "fn": 11,
          "accuracy": 0.9954166666666666
        },
        "0.01": {
          "tp": 2352,
          "fn": 48,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9895652777777777
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 6690,
          "fn": 110,
          "accuracy": 0.9838235294117647
        },
        "0.01": {
          "tp": 6449,
          "fn": 351,
          "accuracy": 0.9483823529411765
        }
      },
      "auroc": 0.9866091299019608
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9901249999999999
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9904791666666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        }
      },
      "auroc": 0.97400625
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9893020833333332
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9816541666666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9824197916666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9897135416666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 787,
          "fn": 13,
          "accuracy": 0.98375
        },
        "0.01": {
          "tp": 743,
          "fn": 57,
          "accuracy": 0.92875
        }
      },
      "auroc": 0.9860666666666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.985425
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9881291666666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9881291666666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.98948125
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9878260416666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9897729166666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9887994791666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9893296875
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.990303125
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 783,
          "fn": 17,
          "accuracy": 0.97875
        }
      },
      "auroc": 0.9898164062499999
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.9646229166666668
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9867322916666668
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 320,
          "fn": 80,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9756776041666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9777281250000001
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9887828125000001
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 777,
          "fn": 23,
          "accuracy": 0.97125
        },
        "0.01": {
          "tp": 720,
          "fn": 80,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9832554687499999
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9907666666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9907666666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9904572916666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9904572916666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9906119791666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9906119791666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9848885416666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9848885416666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.9559364583333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.9559364583333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        }
      },
      "auroc": 0.9704125000000001
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        }
      },
      "auroc": 0.9704125000000001
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9902
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9902
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9797437499999999
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9797437499999999
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        }
      },
      "auroc": 0.984971875
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        }
      },
      "auroc": 0.984971875
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2194,
          "fn": 6,
          "accuracy": 0.9972727272727273
        },
        "0.01": {
          "tp": 2181,
          "fn": 19,
          "accuracy": 0.9913636363636363
        }
      },
      "auroc": 0.9902292613636363
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 1,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 1199,
          "fn": 1,
          "accuracy": 0.9991666666666666
        }
      },
      "auroc": 0.9907152777777778
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3393,
          "fn": 7,
          "accuracy": 0.9979411764705882
        },
        "0.01": {
          "tp": 3380,
          "fn": 20,
          "accuracy": 0.9941176470588236
        }
      },
      "auroc": 0.9904007965686275
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2131,
          "fn": 69,
          "accuracy": 0.9686363636363636
        },
        "0.01": {
          "tp": 1993,
          "fn": 207,
          "accuracy": 0.9059090909090909
        }
      },
      "auroc": 0.9824326704545454
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1192,
          "fn": 8,
          "accuracy": 0.9933333333333333
        },
        "0.01": {
          "tp": 1165,
          "fn": 35,
          "accuracy": 0.9708333333333333
        }
      },
      "auroc": 0.9888164930555556
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3323,
          "fn": 77,
          "accuracy": 0.9773529411764705
        },
        "0.01": {
          "tp": 3158,
          "fn": 242,
          "accuracy": 0.9288235294117647
        }
      },
      "auroc": 0.9846857843137256
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 4325,
          "fn": 75,
          "accuracy": 0.9829545454545454
        },
        "0.01": {
          "tp": 4174,
          "fn": 226,
          "accuracy": 0.9486363636363636
        }
      },
      "auroc": 0.986330965909091
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2391,
          "fn": 9,
          "accuracy": 0.99625
        },
        "0.01": {
          "tp": 2364,
          "fn": 36,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9897658854166667
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 6716,
          "fn": 84,
          "accuracy": 0.9876470588235294
        },
        "0.01": {
          "tp": 6538,
          "fn": 262,
          "accuracy": 0.9614705882352941
        }
      },
      "auroc": 0.9875432904411764
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9901208333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9904770833333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9897354166666668
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.990284375
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.989928125
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        }
      },
      "auroc": 0.9903807291666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9904999999999999
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9906666666666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        }
      },
      "auroc": 0.9712322916666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9894322916666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        }
      },
      "auroc": 0.9803322916666668
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        }
      },
      "auroc": 0.9810328125
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9899661458333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 786,
          "fn": 14,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 724,
          "fn": 76,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9854994791666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.990690625
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9907619791666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9846885416666668
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9877609375000002
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9876895833333335
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 793,
          "fn": 7,
          "accuracy": 0.99125
        }
      },
      "auroc": 0.9892614583333335
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9850958333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9901229166666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        }
      },
      "auroc": 0.987609375
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.9879645833333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.990478125
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": {
          "tp": 761,
          "fn": 39,
          "accuracy": 0.95125
        }
      },
      "auroc": 0.9892213541666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.99049375
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9906635416666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        }
      },
      "auroc": 0.9631916666666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9879729166666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9755822916666668
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        }
      },
      "auroc": 0.9770125000000001
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9892333333333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 775,
          "fn": 25,
          "accuracy": 0.96875
        },
        "0.01": {
          "tp": 697,
          "fn": 103,
          "accuracy": 0.87125
        }
      },
      "auroc": 0.9831229166666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9906999999999999
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9907666666666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9907666666666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9908
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.990484375
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.990484375
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9896666666666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9896666666666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9900755208333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9900755208333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9750114583333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9750114583333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.9367687499999999
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.9367687499999999
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        }
      },
      "auroc": 0.9558901041666669
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        }
      },
      "auroc": 0.9558901041666669
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9872916666666668
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9872916666666668
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.966890625
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.966890625
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        }
      },
      "auroc": 0.9770911458333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        }
      },
      "auroc": 0.9770911458333333
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2184,
          "fn": 16,
          "accuracy": 0.9927272727272727
        },
        "0.01": {
          "tp": 2148,
          "fn": 52,
          "accuracy": 0.9763636363636363
        }
      },
      "auroc": 0.9890412878787879
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 1,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 1194,
          "fn": 6,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9905786458333333
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3383,
          "fn": 17,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 3342,
          "fn": 58,
          "accuracy": 0.9829411764705882
        }
      },
      "auroc": 0.9895838848039216
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2097,
          "fn": 103,
          "accuracy": 0.9531818181818181
        },
        "0.01": {
          "tp": 1884,
          "fn": 316,
          "accuracy": 0.8563636363636363
        }
      },
      "auroc": 0.9788193181818181
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1193,
          "fn": 7,
          "accuracy": 0.9941666666666666
        },
        "0.01": {
          "tp": 1159,
          "fn": 41,
          "accuracy": 0.9658333333333333
        }
      },
      "auroc": 0.9887753472222223
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3290,
          "fn": 110,
          "accuracy": 0.9676470588235294
        },
        "0.01": {
          "tp": 3043,
          "fn": 357,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9823332107843137
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 4281,
          "fn": 119,
          "accuracy": 0.9729545454545454
        },
        "0.01": {
          "tp": 4032,
          "fn": 368,
          "accuracy": 0.9163636363636364
        }
      },
      "auroc": 0.983930303030303
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2392,
          "fn": 8,
          "accuracy": 0.9966666666666667
        },
        "0.01": {
          "tp": 2353,
          "fn": 47,
          "accuracy": 0.9804166666666667
        }
      },
      "auroc": 0.9896769965277779
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 6673,
          "fn": 127,
          "accuracy": 0.9813235294117647
        },
        "0.01": {
          "tp": 6385,
          "fn": 415,
          "accuracy": 0.9389705882352941
        }
      },
      "auroc": 0.9859585477941177
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9899625
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9903979166666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        }
      },
      "auroc": 0.9629125000000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.98935625
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        }
      },
      "auroc": 0.976134375
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9768729166666668
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.989659375
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 777,
          "fn": 23,
          "accuracy": 0.97125
        },
        "0.01": {
          "tp": 722,
          "fn": 78,
          "accuracy": 0.9025
        }
      },
      "auroc": 0.9832661458333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9844968749999999
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9876651041666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9876651041666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 792,
          "fn": 8,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9892492187499999
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9858374999999999
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9895760416666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9877067708333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9883354166666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9902046875
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 766,
          "fn": 34,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9892700520833333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9907395833333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9907864583333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        }
      },
      "auroc": 0.9524239583333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9854104166666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": {
          "tp": 287,
          "fn": 113,
          "accuracy": 0.7175
        }
      },
      "auroc": 0.9689171875
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        }
      },
      "auroc": 0.9716286458333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.988075
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 763,
          "fn": 37,
          "accuracy": 0.95375
        },
        "0.01": {
          "tp": 686,
          "fn": 114,
          "accuracy": 0.8575
        }
      },
      "auroc": 0.9798518229166667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9907666666666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9908
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9908
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9908166666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9907666666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9907666666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.990346875
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.990346875
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9905567708333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9905567708333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9822760416666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9822760416666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9503010416666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9503010416666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9662885416666668
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9662885416666668
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.989525
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.989525
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9790781249999999
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9790781249999999
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9843015625
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9843015625
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2193,
          "fn": 7,
          "accuracy": 0.9968181818181818
        },
        "0.01": {
          "tp": 2174,
          "fn": 26,
          "accuracy": 0.9881818181818182
        }
      },
      "auroc": 0.9899303977272728
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 1,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        }
      },
      "auroc": 0.9906725694444445
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3392,
          "fn": 8,
          "accuracy": 0.9976470588235294
        },
        "0.01": {
          "tp": 3372,
          "fn": 28,
          "accuracy": 0.991764705882353
        }
      },
      "auroc": 0.9901923406862745
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2104,
          "fn": 96,
          "accuracy": 0.9563636363636364
        },
        "0.01": {
          "tp": 1928,
          "fn": 272,
          "accuracy": 0.8763636363636363
        }
      },
      "auroc": 0.9795515151515152
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1190,
          "fn": 10,
          "accuracy": 0.9916666666666667
        },
        "0.01": {
          "tp": 1153,
          "fn": 47,
          "accuracy": 0.9608333333333333
        }
      },
      "auroc": 0.9884065972222221
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3294,
          "fn": 106,
          "accuracy": 0.9688235294117648
        },
        "0.01": {
          "tp": 3081,
          "fn": 319,
          "accuracy": 0.9061764705882352
        }
      },
      "auroc": 0.9826768382352942
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 4297,
          "fn": 103,
          "accuracy": 0.9765909090909091
        },
        "0.01": {
          "tp": 4102,
          "fn": 298,
          "accuracy": 0.9322727272727273
        }
      },
      "auroc": 0.9847409564393939
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2389,
          "fn": 11,
          "accuracy": 0.9954166666666666
        },
        "0.01": {
          "tp": 2351,
          "fn": 49,
          "accuracy": 0.9795833333333334
        }
      },
      "auroc": 0.9895395833333334
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 6686,
          "fn": 114,
          "accuracy": 0.9832352941176471
        },
        "0.01": {
          "tp": 6453,
          "fn": 347,
          "accuracy": 0.9489705882352941
        }
      },
      "auroc": 0.9864345894607843
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9906729166666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9895333333333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9901031250000001
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9898760416666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.98236875
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9861223958333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9902744791666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9859510416666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 792,
          "fn": 8,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 770,
          "fn": 30,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9881127604166667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9906635416666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        }
      },
      "auroc": 0.9763833333333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        }
      },
      "auroc": 0.9835234374999999
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        }
      },
      "auroc": 0.9303416666666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        }
      },
      "auroc": 0.9619729166666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 178,
          "fn": 222,
          "accuracy": 0.445
        }
      },
      "auroc": 0.9461572916666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 259,
          "fn": 141,
          "accuracy": 0.6475
        }
      },
      "auroc": 0.9605026041666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        }
      },
      "auroc": 0.969178125
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 713,
          "fn": 87,
          "accuracy": 0.89125
        },
        "0.01": {
          "tp": 525,
          "fn": 275,
          "accuracy": 0.65625
        }
      },
      "auroc": 0.9648403645833332
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9901010416666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9781114583333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.98410625
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9890833333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9691406250000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        }
      },
      "auroc": 0.9791119791666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9895921875
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 287,
          "fn": 113,
          "accuracy": 0.7175
        }
      },
      "auroc": 0.9736260416666668
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 771,
          "fn": 29,
          "accuracy": 0.96375
        },
        "0.01": {
          "tp": 675,
          "fn": 125,
          "accuracy": 0.84375
        }
      },
      "auroc": 0.9816091145833334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.98725625
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9893958333333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9883260416666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        }
      },
      "auroc": 0.9707229166666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.972709375
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 229,
          "fn": 171,
          "accuracy": 0.5725
        }
      },
      "auroc": 0.9717161458333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        }
      },
      "auroc": 0.9789895833333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9810526041666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 764,
          "fn": 36,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 604,
          "fn": 196,
          "accuracy": 0.755
        }
      },
      "auroc": 0.98002109375
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9827552083333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9867942708333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        }
      },
      "auroc": 0.9226604166666665
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.9229520833333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 115,
          "fn": 285,
          "accuracy": 0.2875
        }
      },
      "auroc": 0.9228062499999999
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 256,
          "fn": 144,
          "accuracy": 0.64
        }
      },
      "auroc": 0.9567468749999999
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": {
          "tp": 221,
          "fn": 179,
          "accuracy": 0.5525
        }
      },
      "auroc": 0.9528536458333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 663,
          "fn": 137,
          "accuracy": 0.82875
        },
        "0.01": {
          "tp": 477,
          "fn": 323,
          "accuracy": 0.59625
        }
      },
      "auroc": 0.9548002604166665
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.989578125
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9902057291666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9904520833333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9869354166666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.98869375
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9906427083333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9882567708333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 779,
          "fn": 21,
          "accuracy": 0.97375
        }
      },
      "auroc": 0.9894497395833334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9885604166666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9885604166666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9839562500000001
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9839562500000001
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9862583333333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9862583333333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        }
      },
      "auroc": 0.9505520833333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        }
      },
      "auroc": 0.9505520833333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        }
      },
      "auroc": 0.908828125
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        }
      },
      "auroc": 0.908828125
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        }
      },
      "auroc": 0.9296901041666668
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        }
      },
      "auroc": 0.9296901041666668
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9907302083333334
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9907302083333334
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9900802083333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9900802083333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9904052083333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9904052083333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9903822916666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9903822916666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9857322916666666
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9857322916666666
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9880572916666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9880572916666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9772052083333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9772052083333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9509072916666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9509072916666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9640562500000001
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9640562500000001
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2156,
          "fn": 44,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 2034,
          "fn": 166,
          "accuracy": 0.9245454545454546
        }
      },
      "auroc": 0.9852536931818181
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1170,
          "fn": 30,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 1057,
          "fn": 143,
          "accuracy": 0.8808333333333334
        }
      },
      "auroc": 0.9842928819444445
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3326,
          "fn": 74,
          "accuracy": 0.9782352941176471
        },
        "0.01": {
          "tp": 3091,
          "fn": 309,
          "accuracy": 0.9091176470588235
        }
      },
      "auroc": 0.9849145833333333
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1939,
          "fn": 261,
          "accuracy": 0.8813636363636363
        },
        "0.01": {
          "tp": 1567,
          "fn": 633,
          "accuracy": 0.7122727272727273
        }
      },
      "auroc": 0.9647855113636363
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1070,
          "fn": 130,
          "accuracy": 0.8916666666666667
        },
        "0.01": {
          "tp": 786,
          "fn": 414,
          "accuracy": 0.655
        }
      },
      "auroc": 0.9660131944444444
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3009,
          "fn": 391,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 2353,
          "fn": 1047,
          "accuracy": 0.6920588235294117
        }
      },
      "auroc": 0.9652188112745098
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 4095,
          "fn": 305,
          "accuracy": 0.9306818181818182
        },
        "0.01": {
          "tp": 3601,
          "fn": 799,
          "accuracy": 0.8184090909090909
        }
      },
      "auroc": 0.9750196022727272
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2240,
          "fn": 160,
          "accuracy": 0.9333333333333333
        },
        "0.01": {
          "tp": 1843,
          "fn": 557,
          "accuracy": 0.7679166666666667
        }
      },
      "auroc": 0.9751530381944444
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 6335,
          "fn": 465,
          "accuracy": 0.9316176470588236
        },
        "0.01": {
          "tp": 5444,
          "fn": 1356,
          "accuracy": 0.8005882352941176
        }
      },
      "auroc": 0.9750666973039217
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9897916666666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9903125
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.9667333333333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9895697916666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9781515624999999
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        }
      },
      "auroc": 0.9787833333333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9896807291666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 780,
          "fn": 20,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 723,
          "fn": 77,
          "accuracy": 0.90375
        }
      },
      "auroc": 0.98423203125
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9847135416666668
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9877734375
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9877734375
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 793,
          "fn": 7,
          "accuracy": 0.99125
        }
      },
      "auroc": 0.9893033854166666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9863802083333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9896520833333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9880161458333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        }
      },
      "auroc": 0.9886067708333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9902427083333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 766,
          "fn": 34,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9894247395833333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9907395833333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9907864583333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.9549739583333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9859489583333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9704614583333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        }
      },
      "auroc": 0.9729036458333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9883442708333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 765,
          "fn": 35,
          "accuracy": 0.95625
        },
        "0.01": {
          "tp": 689,
          "fn": 111,
          "accuracy": 0.86125
        }
      },
      "auroc": 0.9806239583333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9907666666666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9908
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9908
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9908166666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9907166666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9907166666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9904739583333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9904739583333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9905953125
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9905953125
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9839385416666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9839385416666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        }
      },
      "auroc": 0.9521437500000001
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        }
      },
      "auroc": 0.9521437500000001
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9680411458333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9680411458333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9895864583333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9895864583333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9792354166666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9792354166666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9844109375000001
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9844109375000001
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2193,
          "fn": 7,
          "accuracy": 0.9968181818181818
        },
        "0.01": {
          "tp": 2176,
          "fn": 24,
          "accuracy": 0.9890909090909091
        }
      },
      "auroc": 0.9900825757575759
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 1,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        }
      },
      "auroc": 0.9906440972222222
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3392,
          "fn": 8,
          "accuracy": 0.9976470588235294
        },
        "0.01": {
          "tp": 3374,
          "fn": 26,
          "accuracy": 0.9923529411764705
        }
      },
      "auroc": 0.9902807598039216
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2111,
          "fn": 89,
          "accuracy": 0.9595454545454546
        },
        "0.01": {
          "tp": 1931,
          "fn": 269,
          "accuracy": 0.8777272727272727
        }
      },
      "auroc": 0.9803733901515151
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1190,
          "fn": 10,
          "accuracy": 0.9916666666666667
        },
        "0.01": {
          "tp": 1157,
          "fn": 43,
          "accuracy": 0.9641666666666666
        }
      },
      "auroc": 0.9885807291666666
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3301,
          "fn": 99,
          "accuracy": 0.9708823529411764
        },
        "0.01": {
          "tp": 3088,
          "fn": 312,
          "accuracy": 0.908235294117647
        }
      },
      "auroc": 0.9832700980392157
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 4304,
          "fn": 96,
          "accuracy": 0.9781818181818182
        },
        "0.01": {
          "tp": 4107,
          "fn": 293,
          "accuracy": 0.9334090909090909
        }
      },
      "auroc": 0.9852279829545454
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2389,
          "fn": 11,
          "accuracy": 0.9954166666666666
        },
        "0.01": {
          "tp": 2355,
          "fn": 45,
          "accuracy": 0.98125
        }
      },
      "auroc": 0.9896124131944446
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 6693,
          "fn": 107,
          "accuracy": 0.9842647058823529
        },
        "0.01": {
          "tp": 6462,
          "fn": 338,
          "accuracy": 0.9502941176470588
        }
      },
      "auroc": 0.9867754289215687
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9897916666666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9903125
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9655052083333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9892802083333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9773927083333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        }
      },
      "auroc": 0.9781692708333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9895359374999999
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 781,
          "fn": 19,
          "accuracy": 0.97625
        },
        "0.01": {
          "tp": 715,
          "fn": 85,
          "accuracy": 0.89375
        }
      },
      "auroc": 0.9838526041666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9846343750000002
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9877338541666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9877338541666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 793,
          "fn": 7,
          "accuracy": 0.99125
        }
      },
      "auroc": 0.98928359375
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9861500000000001
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.98955
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9878500000000001
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9884916666666668
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9901916666666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 766,
          "fn": 34,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9893416666666668
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9907395833333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9907864583333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.9535552083333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9857541666666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9696546875
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        }
      },
      "auroc": 0.9721942708333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9882468750000001
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 762,
          "fn": 38,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 685,
          "fn": 115,
          "accuracy": 0.85625
        }
      },
      "auroc": 0.9802205729166666
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9907666666666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9908
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9908
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9908166666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9906802083333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9906802083333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9904739583333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9904739583333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9905770833333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9905770833333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9828072916666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9828072916666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9512520833333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9512520833333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9670296875
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9670296875
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9895864583333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9895864583333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9791166666666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9791166666666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9843515625
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9843515625
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2193,
          "fn": 7,
          "accuracy": 0.9968181818181818
        },
        "0.01": {
          "tp": 2176,
          "fn": 24,
          "accuracy": 0.9890909090909091
        }
      },
      "auroc": 0.9899764204545455
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 1,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        }
      },
      "auroc": 0.9906440972222222
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3392,
          "fn": 8,
          "accuracy": 0.9976470588235294
        },
        "0.01": {
          "tp": 3374,
          "fn": 26,
          "accuracy": 0.9923529411764705
        }
      },
      "auroc": 0.9902120710784313
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2108,
          "fn": 92,
          "accuracy": 0.9581818181818181
        },
        "0.01": {
          "tp": 1921,
          "fn": 279,
          "accuracy": 0.8731818181818182
        }
      },
      "auroc": 0.9800199810606061
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1190,
          "fn": 10,
          "accuracy": 0.9916666666666667
        },
        "0.01": {
          "tp": 1154,
          "fn": 46,
          "accuracy": 0.9616666666666667
        }
      },
      "auroc": 0.9884697916666666
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3298,
          "fn": 102,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 3075,
          "fn": 325,
          "accuracy": 0.9044117647058824
        }
      },
      "auroc": 0.9830022671568628
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 4301,
          "fn": 99,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 4097,
          "fn": 303,
          "accuracy": 0.9311363636363637
        }
      },
      "auroc": 0.9849982007575757
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2389,
          "fn": 11,
          "accuracy": 0.9954166666666666
        },
        "0.01": {
          "tp": 2352,
          "fn": 48,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9895569444444444
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 6690,
          "fn": 110,
          "accuracy": 0.9838235294117647
        },
        "0.01": {
          "tp": 6449,
          "fn": 351,
          "accuracy": 0.9483823529411765
        }
      },
      "auroc": 0.986607169117647
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9820708333333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.9717239583333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9768973958333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.980125
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        }
      },
      "auroc": 0.969509375
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9748171875
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 289,
          "fn": 111,
          "accuracy": 0.7225
        }
      },
      "auroc": 0.9810979166666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        }
      },
      "auroc": 0.9706166666666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 760,
          "fn": 40,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 500,
          "fn": 300,
          "accuracy": 0.625
        }
      },
      "auroc": 0.9758572916666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9888395833333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        }
      },
      "auroc": 0.98163125
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        }
      },
      "auroc": 0.9852354166666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        }
      },
      "auroc": 0.9233677083333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        }
      },
      "auroc": 0.972546875
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": {
          "tp": 115,
          "fn": 285,
          "accuracy": 0.2875
        }
      },
      "auroc": 0.9479572916666665
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 228,
          "fn": 172,
          "accuracy": 0.57
        }
      },
      "auroc": 0.9561036458333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.9770890625
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 721,
          "fn": 79,
          "accuracy": 0.90125
        },
        "0.01": {
          "tp": 419,
          "fn": 381,
          "accuracy": 0.52375
        }
      },
      "auroc": 0.9665963541666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9879302083333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9774572916666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        }
      },
      "auroc": 0.98269375
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        }
      },
      "auroc": 0.981478125
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.9541906250000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        }
      },
      "auroc": 0.967834375
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9847041666666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        }
      },
      "auroc": 0.9658239583333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 754,
          "fn": 46,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 516,
          "fn": 284,
          "accuracy": 0.645
        }
      },
      "auroc": 0.9752640625000001
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9906999999999999
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9860302083333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        }
      },
      "auroc": 0.9883651041666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        }
      },
      "auroc": 0.9355489583333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        }
      },
      "auroc": 0.9309739583333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 274,
          "fn": 126,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 65,
          "fn": 335,
          "accuracy": 0.1625
        }
      },
      "auroc": 0.9332614583333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 232,
          "fn": 168,
          "accuracy": 0.58
        }
      },
      "auroc": 0.9631244791666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        }
      },
      "auroc": 0.9585020833333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 673,
          "fn": 127,
          "accuracy": 0.84125
        },
        "0.01": {
          "tp": 426,
          "fn": 374,
          "accuracy": 0.5325
        }
      },
      "auroc": 0.9608132812499999
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9895031249999999
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        }
      },
      "auroc": 0.9727447916666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 295,
          "fn": 105,
          "accuracy": 0.7375
        }
      },
      "auroc": 0.9811239583333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        }
      },
      "auroc": 0.91590625
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        }
      },
      "auroc": 0.9367427083333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        },
        "0.01": {
          "tp": 63,
          "fn": 337,
          "accuracy": 0.1575
        }
      },
      "auroc": 0.9263244791666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        }
      },
      "auroc": 0.9527046875
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 135,
          "fn": 265,
          "accuracy": 0.3375
        }
      },
      "auroc": 0.95474375
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 655,
          "fn": 145,
          "accuracy": 0.81875
        },
        "0.01": {
          "tp": 358,
          "fn": 442,
          "accuracy": 0.4475
        }
      },
      "auroc": 0.95372421875
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9902614583333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.9799291666666666
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        }
      },
      "auroc": 0.9850953125
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9858510416666666
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.9621489583333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        }
      },
      "auroc": 0.974
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.98805625
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        }
      },
      "auroc": 0.9710390625
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 767,
          "fn": 33,
          "accuracy": 0.95875
        },
        "0.01": {
          "tp": 579,
          "fn": 221,
          "accuracy": 0.72375
        }
      },
      "auroc": 0.97954765625
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.9768197916666668
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.9768197916666668
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        }
      },
      "auroc": 0.961846875
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        }
      },
      "auroc": 0.961846875
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 232,
          "fn": 168,
          "accuracy": 0.58
        }
      },
      "auroc": 0.9693333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 232,
          "fn": 168,
          "accuracy": 0.58
        }
      },
      "auroc": 0.9693333333333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.9133010416666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.9133010416666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.8888541666666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.8888541666666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 214,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 39,
          "fn": 361,
          "accuracy": 0.0975
        }
      },
      "auroc": 0.9010776041666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 214,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 39,
          "fn": 361,
          "accuracy": 0.0975
        }
      },
      "auroc": 0.9010776041666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9816947916666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9816947916666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.9753614583333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.9753614583333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 253,
          "fn": 147,
          "accuracy": 0.6325
        }
      },
      "auroc": 0.9785281250000001
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 253,
          "fn": 147,
          "accuracy": 0.6325
        }
      },
      "auroc": 0.9785281250000001
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9901187499999999
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9901187499999999
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        }
      },
      "auroc": 0.9704927083333332
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        }
      },
      "auroc": 0.9704927083333332
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9803057291666666
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9803057291666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        }
      },
      "auroc": 0.9565052083333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        }
      },
      "auroc": 0.9565052083333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9403406249999999
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9403406249999999
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        }
      },
      "auroc": 0.9484229166666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        }
      },
      "auroc": 0.9484229166666667
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2051,
          "fn": 149,
          "accuracy": 0.9322727272727273
        },
        "0.01": {
          "tp": 1693,
          "fn": 507,
          "accuracy": 0.7695454545454545
        }
      },
      "auroc": 0.9770677083333332
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1162,
          "fn": 38,
          "accuracy": 0.9683333333333334
        },
        "0.01": {
          "tp": 748,
          "fn": 452,
          "accuracy": 0.6233333333333333
        }
      },
      "auroc": 0.9782527777777776
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3213,
          "fn": 187,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 2441,
          "fn": 959,
          "accuracy": 0.7179411764705882
        }
      },
      "auroc": 0.9774859681372549
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1740,
          "fn": 460,
          "accuracy": 0.7909090909090909
        },
        "0.01": {
          "tp": 976,
          "fn": 1224,
          "accuracy": 0.44363636363636366
        }
      },
      "auroc": 0.9508339015151516
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1011,
          "fn": 189,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 389,
          "fn": 811,
          "accuracy": 0.32416666666666666
        }
      },
      "auroc": 0.9543520833333333
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2751,
          "fn": 649,
          "accuracy": 0.8091176470588235
        },
        "0.01": {
          "tp": 1365,
          "fn": 2035,
          "accuracy": 0.40147058823529413
        }
      },
      "auroc": 0.952075612745098
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3791,
          "fn": 609,
          "accuracy": 0.8615909090909091
        },
        "0.01": {
          "tp": 2669,
          "fn": 1731,
          "accuracy": 0.6065909090909091
        }
      },
      "auroc": 0.9639508049242425
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2173,
          "fn": 227,
          "accuracy": 0.9054166666666666
        },
        "0.01": {
          "tp": 1137,
          "fn": 1263,
          "accuracy": 0.47375
        }
      },
      "auroc": 0.9663024305555555
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5964,
          "fn": 836,
          "accuracy": 0.8770588235294118
        },
        "0.01": {
          "tp": 3806,
          "fn": 2994,
          "accuracy": 0.5597058823529412
        }
      },
      "auroc": 0.9647807904411765
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9897916666666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9903125
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9723
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9892802083333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9807901041666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        }
      },
      "auroc": 0.9815666666666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9895359374999999
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 785,
          "fn": 15,
          "accuracy": 0.98125
        },
        "0.01": {
          "tp": 733,
          "fn": 67,
          "accuracy": 0.91625
        }
      },
      "auroc": 0.9855513020833334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9846333333333335
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9877333333333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9877333333333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 793,
          "fn": 7,
          "accuracy": 0.99125
        }
      },
      "auroc": 0.9892833333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.98801875
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9897750000000001
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.988896875
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9894260416666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9903041666666668
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": {
          "tp": 781,
          "fn": 19,
          "accuracy": 0.97625
        }
      },
      "auroc": 0.9898651041666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        }
      },
      "auroc": 0.96445
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9868802083333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9756651041666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        }
      },
      "auroc": 0.9776416666666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9888567708333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 775,
          "fn": 25,
          "accuracy": 0.96875
        },
        "0.01": {
          "tp": 717,
          "fn": 83,
          "accuracy": 0.89625
        }
      },
      "auroc": 0.9832492187499999
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9905729166666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9905729166666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9904177083333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9904177083333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9904953125
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9904953125
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.98439375
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.98439375
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9536604166666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9536604166666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9690270833333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9690270833333334
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9897822916666668
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9897822916666668
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9783510416666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9783510416666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9840666666666668
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9840666666666668
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2194,
          "fn": 6,
          "accuracy": 0.9972727272727273
        },
        "0.01": {
          "tp": 2173,
          "fn": 27,
          "accuracy": 0.9877272727272727
        }
      },
      "auroc": 0.9901286931818183
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 1,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 1199,
          "fn": 1,
          "accuracy": 0.9991666666666666
        }
      },
      "auroc": 0.9906597222222222
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3393,
          "fn": 7,
          "accuracy": 0.9979411764705882
        },
        "0.01": {
          "tp": 3372,
          "fn": 28,
          "accuracy": 0.991764705882353
        }
      },
      "auroc": 0.9903161151960784
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2127,
          "fn": 73,
          "accuracy": 0.9668181818181818
        },
        "0.01": {
          "tp": 1983,
          "fn": 217,
          "accuracy": 0.9013636363636364
        }
      },
      "auroc": 0.9819422348484848
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1192,
          "fn": 8,
          "accuracy": 0.9933333333333333
        },
        "0.01": {
          "tp": 1161,
          "fn": 39,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9887059027777777
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3319,
          "fn": 81,
          "accuracy": 0.9761764705882353
        },
        "0.01": {
          "tp": 3144,
          "fn": 256,
          "accuracy": 0.9247058823529412
        }
      },
      "auroc": 0.9843294117647059
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 4321,
          "fn": 79,
          "accuracy": 0.9820454545454546
        },
        "0.01": {
          "tp": 4156,
          "fn": 244,
          "accuracy": 0.9445454545454546
        }
      },
      "auroc": 0.9860354640151516
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2391,
          "fn": 9,
          "accuracy": 0.99625
        },
        "0.01": {
          "tp": 2360,
          "fn": 40,
          "accuracy": 0.9833333333333333
        }
      },
      "auroc": 0.9896828125
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 6712,
          "fn": 88,
          "accuracy": 0.9870588235294118
        },
        "0.01": {
          "tp": 6516,
          "fn": 284,
          "accuracy": 0.9582352941176471
        }
      },
      "auroc": 0.987322763480392
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9897645833333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9902989583333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.9643552083333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9892531250000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9768041666666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9775942708333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9895088541666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 778,
          "fn": 22,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 716,
          "fn": 84,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9835515625
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9844479166666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.987640625
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.987640625
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 792,
          "fn": 8,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9892369791666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9859239583333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9896833333333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9878036458333335
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9883786458333335
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9902583333333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 769,
          "fn": 31,
          "accuracy": 0.96125
        }
      },
      "auroc": 0.9893184895833333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9907395833333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9907864583333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        }
      },
      "auroc": 0.9538645833333332
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9856114583333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": {
          "tp": 287,
          "fn": 113,
          "accuracy": 0.7175
        }
      },
      "auroc": 0.9697380208333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        }
      },
      "auroc": 0.9723489583333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9881755208333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 763,
          "fn": 37,
          "accuracy": 0.95375
        },
        "0.01": {
          "tp": 686,
          "fn": 114,
          "accuracy": 0.8575
        }
      },
      "auroc": 0.9802622395833333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9907666666666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9908
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9908
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9908166666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9904739583333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9904739583333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9906536458333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9906536458333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9819760416666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9819760416666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        }
      },
      "auroc": 0.9497729166666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        }
      },
      "auroc": 0.9497729166666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        }
      },
      "auroc": 0.9658744791666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        }
      },
      "auroc": 0.9658744791666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9908333333333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.989128125
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.989128125
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9786854166666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9786854166666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9839067708333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9839067708333333
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2193,
          "fn": 7,
          "accuracy": 0.9968181818181818
        },
        "0.01": {
          "tp": 2175,
          "fn": 25,
          "accuracy": 0.9886363636363636
        }
      },
      "auroc": 0.989873106060606
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 1,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        }
      },
      "auroc": 0.9906395833333334
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3392,
          "fn": 8,
          "accuracy": 0.9976470588235294
        },
        "0.01": {
          "tp": 3373,
          "fn": 27,
          "accuracy": 0.9920588235294118
        }
      },
      "auroc": 0.9901436274509805
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2103,
          "fn": 97,
          "accuracy": 0.9559090909090909
        },
        "0.01": {
          "tp": 1918,
          "fn": 282,
          "accuracy": 0.8718181818181818
        }
      },
      "auroc": 0.9797493371212121
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1190,
          "fn": 10,
          "accuracy": 0.9916666666666667
        },
        "0.01": {
          "tp": 1154,
          "fn": 46,
          "accuracy": 0.9616666666666667
        }
      },
      "auroc": 0.9884326388888889
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3293,
          "fn": 107,
          "accuracy": 0.9685294117647059
        },
        "0.01": {
          "tp": 3072,
          "fn": 328,
          "accuracy": 0.9035294117647059
        }
      },
      "auroc": 0.9828140318627452
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 4296,
          "fn": 104,
          "accuracy": 0.9763636363636363
        },
        "0.01": {
          "tp": 4093,
          "fn": 307,
          "accuracy": 0.9302272727272727
        }
      },
      "auroc": 0.9848112215909092
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2389,
          "fn": 11,
          "accuracy": 0.9954166666666666
        },
        "0.01": {
          "tp": 2352,
          "fn": 48,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9895361111111112
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 6685,
          "fn": 115,
          "accuracy": 0.9830882352941176
        },
        "0.01": {
          "tp": 6445,
          "fn": 355,
          "accuracy": 0.9477941176470588
        }
      },
      "auroc": 0.9864788296568627
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.9594604166666666
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.9397125
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 74,
          "fn": 326,
          "accuracy": 0.185
        }
      },
      "auroc": 0.9495864583333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.9586604166666666
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.9454791666666666
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 106,
          "fn": 294,
          "accuracy": 0.265
        }
      },
      "auroc": 0.9520697916666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": {
          "tp": 125,
          "fn": 275,
          "accuracy": 0.3125
        }
      },
      "auroc": 0.9590604166666666
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        },
        "0.01": {
          "tp": 55,
          "fn": 345,
          "accuracy": 0.1375
        }
      },
      "auroc": 0.9425958333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 662,
          "fn": 138,
          "accuracy": 0.8275
        },
        "0.01": {
          "tp": 180,
          "fn": 620,
          "accuracy": 0.225
        }
      },
      "auroc": 0.9508281249999999
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9860187499999999
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.9584708333333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        }
      },
      "auroc": 0.9722447916666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.9244052083333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.9451291666666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        },
        "0.01": {
          "tp": 37,
          "fn": 363,
          "accuracy": 0.0925
        }
      },
      "auroc": 0.9347671875
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        }
      },
      "auroc": 0.9552119791666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 40,
          "fn": 360,
          "accuracy": 0.1
        }
      },
      "auroc": 0.9518
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 669,
          "fn": 131,
          "accuracy": 0.83625
        },
        "0.01": {
          "tp": 220,
          "fn": 580,
          "accuracy": 0.275
        }
      },
      "auroc": 0.9535059895833333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.9698677083333332
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        }
      },
      "auroc": 0.9666989583333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 158,
          "fn": 242,
          "accuracy": 0.395
        }
      },
      "auroc": 0.9682833333333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.9613197916666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.9455916666666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 97,
          "fn": 303,
          "accuracy": 0.2425
        }
      },
      "auroc": 0.9534557291666668
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 168,
          "fn": 232,
          "accuracy": 0.42
        }
      },
      "auroc": 0.9655937499999999
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 87,
          "fn": 313,
          "accuracy": 0.2175
        }
      },
      "auroc": 0.9561453125
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 708,
          "fn": 92,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 255,
          "fn": 545,
          "accuracy": 0.31875
        }
      },
      "auroc": 0.96086953125
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9902927083333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.9697239583333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 256,
          "fn": 144,
          "accuracy": 0.64
        }
      },
      "auroc": 0.9800083333333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.931725
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.9261
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 27,
          "fn": 373,
          "accuracy": 0.0675
        }
      },
      "auroc": 0.9289125
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.9610088541666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 75,
          "fn": 325,
          "accuracy": 0.1875
        }
      },
      "auroc": 0.9479119791666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 642,
          "fn": 158,
          "accuracy": 0.8025
        },
        "0.01": {
          "tp": 283,
          "fn": 517,
          "accuracy": 0.35375
        }
      },
      "auroc": 0.9544604166666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9876989583333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.9624489583333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        }
      },
      "auroc": 0.9750739583333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.9216385416666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        }
      },
      "auroc": 0.923040625
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        },
        "0.01": {
          "tp": 40,
          "fn": 360,
          "accuracy": 0.1
        }
      },
      "auroc": 0.9223395833333332
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        },
        "0.01": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        }
      },
      "auroc": 0.95466875
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 289,
          "fn": 111,
          "accuracy": 0.7225
        },
        "0.01": {
          "tp": 86,
          "fn": 314,
          "accuracy": 0.215
        }
      },
      "auroc": 0.9427447916666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 606,
          "fn": 194,
          "accuracy": 0.7575
        },
        "0.01": {
          "tp": 287,
          "fn": 513,
          "accuracy": 0.35875
        }
      },
      "auroc": 0.9487067708333332
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9827447916666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.944109375
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        }
      },
      "auroc": 0.9634270833333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.9698572916666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.93328125
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        },
        "0.01": {
          "tp": 125,
          "fn": 275,
          "accuracy": 0.3125
        }
      },
      "auroc": 0.9515692708333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        }
      },
      "auroc": 0.9763010416666666
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 46,
          "fn": 354,
          "accuracy": 0.115
        }
      },
      "auroc": 0.9386953124999999
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 672,
          "fn": 128,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 304,
          "fn": 496,
          "accuracy": 0.38
        }
      },
      "auroc": 0.9574981770833333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        }
      },
      "auroc": 0.9593666666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        }
      },
      "auroc": 0.9593666666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        }
      },
      "auroc": 0.9483166666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        }
      },
      "auroc": 0.9483166666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 132,
          "fn": 268,
          "accuracy": 0.33
        }
      },
      "auroc": 0.9538416666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 132,
          "fn": 268,
          "accuracy": 0.33
        }
      },
      "auroc": 0.9538416666666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        }
      },
      "auroc": 0.907225
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        }
      },
      "auroc": 0.907225
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.8966041666666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.8966041666666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 268,
          "accuracy": 0.33
        },
        "0.01": {
          "tp": 12,
          "fn": 388,
          "accuracy": 0.03
        }
      },
      "auroc": 0.9019145833333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 268,
          "accuracy": 0.33
        },
        "0.01": {
          "tp": 12,
          "fn": 388,
          "accuracy": 0.03
        }
      },
      "auroc": 0.9019145833333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.9532177083333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.9532177083333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        }
      },
      "auroc": 0.9482510416666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        }
      },
      "auroc": 0.9482510416666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 85,
          "fn": 315,
          "accuracy": 0.2125
        }
      },
      "auroc": 0.9507343749999999
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 85,
          "fn": 315,
          "accuracy": 0.2125
        }
      },
      "auroc": 0.9507343749999999
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.98534375
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.98534375
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        }
      },
      "auroc": 0.9567229166666666
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        }
      },
      "auroc": 0.9567229166666666
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        }
      },
      "auroc": 0.9710333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        }
      },
      "auroc": 0.9710333333333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.9446052083333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.9446052083333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.9318916666666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.9318916666666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 271,
          "fn": 129,
          "accuracy": 0.6775
        },
        "0.01": {
          "tp": 84,
          "fn": 316,
          "accuracy": 0.21
        }
      },
      "auroc": 0.9382484375
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 271,
          "fn": 129,
          "accuracy": 0.6775
        },
        "0.01": {
          "tp": 84,
          "fn": 316,
          "accuracy": 0.21
        }
      },
      "auroc": 0.9382484375
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1935,
          "fn": 265,
          "accuracy": 0.8795454545454545
        },
        "0.01": {
          "tp": 1190,
          "fn": 1010,
          "accuracy": 0.5409090909090909
        }
      },
      "auroc": 0.965985606060606
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1069,
          "fn": 131,
          "accuracy": 0.8908333333333334
        },
        "0.01": {
          "tp": 251,
          "fn": 949,
          "accuracy": 0.20916666666666667
        }
      },
      "auroc": 0.9568607638888889
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3004,
          "fn": 396,
          "accuracy": 0.8835294117647059
        },
        "0.01": {
          "tp": 1441,
          "fn": 1959,
          "accuracy": 0.4238235294117647
        }
      },
      "auroc": 0.9627650735294118
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1562,
          "fn": 638,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 464,
          "fn": 1736,
          "accuracy": 0.2109090909090909
        }
      },
      "auroc": 0.9408538825757575
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 831,
          "fn": 369,
          "accuracy": 0.6925
        },
        "0.01": {
          "tp": 138,
          "fn": 1062,
          "accuracy": 0.115
        }
      },
      "auroc": 0.9364369791666667
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2393,
          "fn": 1007,
          "accuracy": 0.7038235294117647
        },
        "0.01": {
          "tp": 602,
          "fn": 2798,
          "accuracy": 0.17705882352941177
        }
      },
      "auroc": 0.939294975490196
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3497,
          "fn": 903,
          "accuracy": 0.7947727272727273
        },
        "0.01": {
          "tp": 1654,
          "fn": 2746,
          "accuracy": 0.3759090909090909
        }
      },
      "auroc": 0.9534197443181819
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1900,
          "fn": 500,
          "accuracy": 0.7916666666666666
        },
        "0.01": {
          "tp": 389,
          "fn": 2011,
          "accuracy": 0.16208333333333333
        }
      },
      "auroc": 0.9466488715277777
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 5397,
          "fn": 1403,
          "accuracy": 0.7936764705882353
        },
        "0.01": {
          "tp": 2043,
          "fn": 4757,
          "accuracy": 0.3004411764705882
        }
      },
      "auroc": 0.951030024509804
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2378,
          "fn": 22,
          "accuracy": 0.9908333333333333
        },
        "0.01": {
          "tp": 2205,
          "fn": 195,
          "accuracy": 0.91875
        }
      },
      "auroc": 0.9874753472222223
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2338,
          "fn": 62,
          "accuracy": 0.9741666666666666
        },
        "0.01": {
          "tp": 2117,
          "fn": 283,
          "accuracy": 0.8820833333333333
        }
      },
      "auroc": 0.9848131076388889
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4716,
          "fn": 84,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 4322,
          "fn": 478,
          "accuracy": 0.9004166666666666
        }
      },
      "auroc": 0.9861442274305555
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2363,
          "fn": 37,
          "accuracy": 0.9845833333333334
        },
        "0.01": {
          "tp": 2204,
          "fn": 196,
          "accuracy": 0.9183333333333333
        }
      },
      "auroc": 0.9871801215277778
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2333,
          "fn": 67,
          "accuracy": 0.9720833333333333
        },
        "0.01": {
          "tp": 2121,
          "fn": 279,
          "accuracy": 0.88375
        }
      },
      "auroc": 0.9844799479166666
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4696,
          "fn": 104,
          "accuracy": 0.9783333333333334
        },
        "0.01": {
          "tp": 4325,
          "fn": 475,
          "accuracy": 0.9010416666666666
        }
      },
      "auroc": 0.9858300347222222
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4741,
          "fn": 59,
          "accuracy": 0.9877083333333333
        },
        "0.01": {
          "tp": 4409,
          "fn": 391,
          "accuracy": 0.9185416666666667
        }
      },
      "auroc": 0.987327734375
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4671,
          "fn": 129,
          "accuracy": 0.973125
        },
        "0.01": {
          "tp": 4238,
          "fn": 562,
          "accuracy": 0.8829166666666667
        }
      },
      "auroc": 0.9846465277777777
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9412,
          "fn": 188,
          "accuracy": 0.9804166666666667
        },
        "0.01": {
          "tp": 8647,
          "fn": 953,
          "accuracy": 0.9007291666666667
        }
      },
      "auroc": 0.9859871310763888
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2399,
          "fn": 1,
          "accuracy": 0.9995833333333334
        },
        "0.01": {
          "tp": 2349,
          "fn": 51,
          "accuracy": 0.97875
        }
      },
      "auroc": 0.9902518229166667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2372,
          "fn": 28,
          "accuracy": 0.9883333333333333
        },
        "0.01": {
          "tp": 2076,
          "fn": 324,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9854829861111111
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4771,
          "fn": 29,
          "accuracy": 0.9939583333333334
        },
        "0.01": {
          "tp": 4425,
          "fn": 375,
          "accuracy": 0.921875
        }
      },
      "auroc": 0.9878674045138889
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2030,
          "fn": 370,
          "accuracy": 0.8458333333333333
        },
        "0.01": {
          "tp": 1326,
          "fn": 1074,
          "accuracy": 0.5525
        }
      },
      "auroc": 0.9571686631944444
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2338,
          "fn": 62,
          "accuracy": 0.9741666666666666
        },
        "0.01": {
          "tp": 1919,
          "fn": 481,
          "accuracy": 0.7995833333333333
        }
      },
      "auroc": 0.9819736111111111
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4368,
          "fn": 432,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 3245,
          "fn": 1555,
          "accuracy": 0.6760416666666667
        }
      },
      "auroc": 0.9695711371527778
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4429,
          "fn": 371,
          "accuracy": 0.9227083333333334
        },
        "0.01": {
          "tp": 3675,
          "fn": 1125,
          "accuracy": 0.765625
        }
      },
      "auroc": 0.9737102430555555
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4710,
          "fn": 90,
          "accuracy": 0.98125
        },
        "0.01": {
          "tp": 3995,
          "fn": 805,
          "accuracy": 0.8322916666666667
        }
      },
      "auroc": 0.9837282986111111
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9139,
          "fn": 461,
          "accuracy": 0.9519791666666667
        },
        "0.01": {
          "tp": 7670,
          "fn": 1930,
          "accuracy": 0.7989583333333333
        }
      },
      "auroc": 0.9787192708333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2389,
          "fn": 11,
          "accuracy": 0.9954166666666666
        },
        "0.01": {
          "tp": 2273,
          "fn": 127,
          "accuracy": 0.9470833333333334
        }
      },
      "auroc": 0.9887832465277777
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2367,
          "fn": 33,
          "accuracy": 0.98625
        },
        "0.01": {
          "tp": 2146,
          "fn": 254,
          "accuracy": 0.8941666666666667
        }
      },
      "auroc": 0.9866354166666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4756,
          "fn": 44,
          "accuracy": 0.9908333333333333
        },
        "0.01": {
          "tp": 4419,
          "fn": 381,
          "accuracy": 0.920625
        }
      },
      "auroc": 0.9877093315972223
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2370,
          "fn": 30,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 2209,
          "fn": 191,
          "accuracy": 0.9204166666666667
        }
      },
      "auroc": 0.9874484375
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2271,
          "fn": 129,
          "accuracy": 0.94625
        },
        "0.01": {
          "tp": 1954,
          "fn": 446,
          "accuracy": 0.8141666666666667
        }
      },
      "auroc": 0.9776147569444446
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4641,
          "fn": 159,
          "accuracy": 0.966875
        },
        "0.01": {
          "tp": 4163,
          "fn": 637,
          "accuracy": 0.8672916666666667
        }
      },
      "auroc": 0.9825315972222223
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4759,
          "fn": 41,
          "accuracy": 0.9914583333333333
        },
        "0.01": {
          "tp": 4482,
          "fn": 318,
          "accuracy": 0.93375
        }
      },
      "auroc": 0.9881158420138889
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4638,
          "fn": 162,
          "accuracy": 0.96625
        },
        "0.01": {
          "tp": 4100,
          "fn": 700,
          "accuracy": 0.8541666666666666
        }
      },
      "auroc": 0.9821250868055555
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9397,
          "fn": 203,
          "accuracy": 0.9788541666666667
        },
        "0.01": {
          "tp": 8582,
          "fn": 1018,
          "accuracy": 0.8939583333333333
        }
      },
      "auroc": 0.9851204644097222
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2398,
          "fn": 2,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 2377,
          "fn": 23,
          "accuracy": 0.9904166666666666
        }
      },
      "auroc": 0.9904790798611111
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2397,
          "fn": 3,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 2215,
          "fn": 185,
          "accuracy": 0.9229166666666667
        }
      },
      "auroc": 0.9885541666666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4795,
          "fn": 5,
          "accuracy": 0.9989583333333333
        },
        "0.01": {
          "tp": 4592,
          "fn": 208,
          "accuracy": 0.9566666666666667
        }
      },
      "auroc": 0.9895166232638889
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2219,
          "fn": 181,
          "accuracy": 0.9245833333333333
        },
        "0.01": {
          "tp": 1742,
          "fn": 658,
          "accuracy": 0.7258333333333333
        }
      },
      "auroc": 0.9762940972222222
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2231,
          "fn": 169,
          "accuracy": 0.9295833333333333
        },
        "0.01": {
          "tp": 1903,
          "fn": 497,
          "accuracy": 0.7929166666666667
        }
      },
      "auroc": 0.9780846354166667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4450,
          "fn": 350,
          "accuracy": 0.9270833333333334
        },
        "0.01": {
          "tp": 3645,
          "fn": 1155,
          "accuracy": 0.759375
        }
      },
      "auroc": 0.9771893663194444
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4617,
          "fn": 183,
          "accuracy": 0.961875
        },
        "0.01": {
          "tp": 4119,
          "fn": 681,
          "accuracy": 0.858125
        }
      },
      "auroc": 0.9833865885416667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4628,
          "fn": 172,
          "accuracy": 0.9641666666666666
        },
        "0.01": {
          "tp": 4118,
          "fn": 682,
          "accuracy": 0.8579166666666667
        }
      },
      "auroc": 0.9833194010416666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9245,
          "fn": 355,
          "accuracy": 0.9630208333333333
        },
        "0.01": {
          "tp": 8237,
          "fn": 1363,
          "accuracy": 0.8580208333333333
        }
      },
      "auroc": 0.9833529947916666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2399,
          "fn": 1,
          "accuracy": 0.9995833333333334
        },
        "0.01": {
          "tp": 2367,
          "fn": 33,
          "accuracy": 0.98625
        }
      },
      "auroc": 0.9904612847222223
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2369,
          "fn": 31,
          "accuracy": 0.9870833333333333
        },
        "0.01": {
          "tp": 2128,
          "fn": 272,
          "accuracy": 0.8866666666666667
        }
      },
      "auroc": 0.9862122395833334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4768,
          "fn": 32,
          "accuracy": 0.9933333333333333
        },
        "0.01": {
          "tp": 4495,
          "fn": 305,
          "accuracy": 0.9364583333333333
        }
      },
      "auroc": 0.9883367621527779
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1906,
          "fn": 494,
          "accuracy": 0.7941666666666667
        },
        "0.01": {
          "tp": 1132,
          "fn": 1268,
          "accuracy": 0.4716666666666667
        }
      },
      "auroc": 0.9478664930555556
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2154,
          "fn": 246,
          "accuracy": 0.8975
        },
        "0.01": {
          "tp": 1745,
          "fn": 655,
          "accuracy": 0.7270833333333333
        }
      },
      "auroc": 0.9715506944444444
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4060,
          "fn": 740,
          "accuracy": 0.8458333333333333
        },
        "0.01": {
          "tp": 2877,
          "fn": 1923,
          "accuracy": 0.599375
        }
      },
      "auroc": 0.9597085937500001
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4305,
          "fn": 495,
          "accuracy": 0.896875
        },
        "0.01": {
          "tp": 3499,
          "fn": 1301,
          "accuracy": 0.7289583333333334
        }
      },
      "auroc": 0.9691638888888889
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4523,
          "fn": 277,
          "accuracy": 0.9422916666666666
        },
        "0.01": {
          "tp": 3873,
          "fn": 927,
          "accuracy": 0.806875
        }
      },
      "auroc": 0.9788814670138889
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8828,
          "fn": 772,
          "accuracy": 0.9195833333333333
        },
        "0.01": {
          "tp": 7372,
          "fn": 2228,
          "accuracy": 0.7679166666666667
        }
      },
      "auroc": 0.9740226779513887
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2397,
          "fn": 3,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 2353,
          "fn": 47,
          "accuracy": 0.9804166666666667
        }
      },
      "auroc": 0.9901116319444444
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2349,
          "fn": 51,
          "accuracy": 0.97875
        },
        "0.01": {
          "tp": 2154,
          "fn": 246,
          "accuracy": 0.8975
        }
      },
      "auroc": 0.985926388888889
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4746,
          "fn": 54,
          "accuracy": 0.98875
        },
        "0.01": {
          "tp": 4507,
          "fn": 293,
          "accuracy": 0.9389583333333333
        }
      },
      "auroc": 0.9880190104166666
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2385,
          "fn": 15,
          "accuracy": 0.99375
        },
        "0.01": {
          "tp": 2265,
          "fn": 135,
          "accuracy": 0.94375
        }
      },
      "auroc": 0.9886383680555555
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2302,
          "fn": 98,
          "accuracy": 0.9591666666666666
        },
        "0.01": {
          "tp": 2082,
          "fn": 318,
          "accuracy": 0.8675
        }
      },
      "auroc": 0.9832776909722222
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4687,
          "fn": 113,
          "accuracy": 0.9764583333333333
        },
        "0.01": {
          "tp": 4347,
          "fn": 453,
          "accuracy": 0.905625
        }
      },
      "auroc": 0.9859580295138889
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4782,
          "fn": 18,
          "accuracy": 0.99625
        },
        "0.01": {
          "tp": 4618,
          "fn": 182,
          "accuracy": 0.9620833333333333
        }
      },
      "auroc": 0.9893749999999999
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4651,
          "fn": 149,
          "accuracy": 0.9689583333333334
        },
        "0.01": {
          "tp": 4236,
          "fn": 564,
          "accuracy": 0.8825
        }
      },
      "auroc": 0.9846020399305555
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9433,
          "fn": 167,
          "accuracy": 0.9826041666666666
        },
        "0.01": {
          "tp": 8854,
          "fn": 746,
          "accuracy": 0.9222916666666666
        }
      },
      "auroc": 0.9869885199652777
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2360,
          "fn": 40,
          "accuracy": 0.9833333333333333
        },
        "0.01": {
          "tp": 2181,
          "fn": 219,
          "accuracy": 0.90875
        }
      },
      "auroc": 0.9867440104166666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2360,
          "fn": 40,
          "accuracy": 0.9833333333333333
        },
        "0.01": {
          "tp": 2181,
          "fn": 219,
          "accuracy": 0.90875
        }
      },
      "auroc": 0.9867440104166666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2318,
          "fn": 82,
          "accuracy": 0.9658333333333333
        },
        "0.01": {
          "tp": 2110,
          "fn": 290,
          "accuracy": 0.8791666666666667
        }
      },
      "auroc": 0.9839481770833334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2318,
          "fn": 82,
          "accuracy": 0.9658333333333333
        },
        "0.01": {
          "tp": 2110,
          "fn": 290,
          "accuracy": 0.8791666666666667
        }
      },
      "auroc": 0.9839481770833334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4678,
          "fn": 122,
          "accuracy": 0.9745833333333334
        },
        "0.01": {
          "tp": 4291,
          "fn": 509,
          "accuracy": 0.8939583333333333
        }
      },
      "auroc": 0.9853460937499999
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4678,
          "fn": 122,
          "accuracy": 0.9745833333333334
        },
        "0.01": {
          "tp": 4291,
          "fn": 509,
          "accuracy": 0.8939583333333333
        }
      },
      "auroc": 0.9853460937499999
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2101,
          "fn": 299,
          "accuracy": 0.8754166666666666
        },
        "0.01": {
          "tp": 1731,
          "fn": 669,
          "accuracy": 0.72125
        }
      },
      "auroc": 0.9676653645833333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2101,
          "fn": 299,
          "accuracy": 0.8754166666666666
        },
        "0.01": {
          "tp": 1731,
          "fn": 669,
          "accuracy": 0.72125
        }
      },
      "auroc": 0.9676653645833333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1785,
          "fn": 615,
          "accuracy": 0.74375
        },
        "0.01": {
          "tp": 1290,
          "fn": 1110,
          "accuracy": 0.5375
        }
      },
      "auroc": 0.9372188368055555
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1785,
          "fn": 615,
          "accuracy": 0.74375
        },
        "0.01": {
          "tp": 1290,
          "fn": 1110,
          "accuracy": 0.5375
        }
      },
      "auroc": 0.9372188368055555
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3886,
          "fn": 914,
          "accuracy": 0.8095833333333333
        },
        "0.01": {
          "tp": 3021,
          "fn": 1779,
          "accuracy": 0.629375
        }
      },
      "auroc": 0.9524421006944445
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3886,
          "fn": 914,
          "accuracy": 0.8095833333333333
        },
        "0.01": {
          "tp": 3021,
          "fn": 1779,
          "accuracy": 0.629375
        }
      },
      "auroc": 0.9524421006944445
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2368,
          "fn": 32,
          "accuracy": 0.9866666666666667
        },
        "0.01": {
          "tp": 2192,
          "fn": 208,
          "accuracy": 0.9133333333333333
        }
      },
      "auroc": 0.9869285590277777
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2368,
          "fn": 32,
          "accuracy": 0.9866666666666667
        },
        "0.01": {
          "tp": 2192,
          "fn": 208,
          "accuracy": 0.9133333333333333
        }
      },
      "auroc": 0.9869285590277777
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2359,
          "fn": 41,
          "accuracy": 0.9829166666666667
        },
        "0.01": {
          "tp": 2141,
          "fn": 259,
          "accuracy": 0.8920833333333333
        }
      },
      "auroc": 0.9859327256944443
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2359,
          "fn": 41,
          "accuracy": 0.9829166666666667
        },
        "0.01": {
          "tp": 2141,
          "fn": 259,
          "accuracy": 0.8920833333333333
        }
      },
      "auroc": 0.9859327256944443
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4727,
          "fn": 73,
          "accuracy": 0.9847916666666666
        },
        "0.01": {
          "tp": 4333,
          "fn": 467,
          "accuracy": 0.9027083333333333
        }
      },
      "auroc": 0.9864306423611111
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4727,
          "fn": 73,
          "accuracy": 0.9847916666666666
        },
        "0.01": {
          "tp": 4333,
          "fn": 467,
          "accuracy": 0.9027083333333333
        }
      },
      "auroc": 0.9864306423611111
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2346,
          "fn": 54,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9902787326388889
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2346,
          "fn": 54,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9902787326388889
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2360,
          "fn": 40,
          "accuracy": 0.9833333333333333
        },
        "0.01": {
          "tp": 2121,
          "fn": 279,
          "accuracy": 0.88375
        }
      },
      "auroc": 0.9858706597222223
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2360,
          "fn": 40,
          "accuracy": 0.9833333333333333
        },
        "0.01": {
          "tp": 2121,
          "fn": 279,
          "accuracy": 0.88375
        }
      },
      "auroc": 0.9858706597222223
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4760,
          "fn": 40,
          "accuracy": 0.9916666666666667
        },
        "0.01": {
          "tp": 4467,
          "fn": 333,
          "accuracy": 0.930625
        }
      },
      "auroc": 0.9880746961805555
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4760,
          "fn": 40,
          "accuracy": 0.9916666666666667
        },
        "0.01": {
          "tp": 4467,
          "fn": 333,
          "accuracy": 0.930625
        }
      },
      "auroc": 0.9880746961805555
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2283,
          "fn": 117,
          "accuracy": 0.95125
        },
        "0.01": {
          "tp": 2098,
          "fn": 302,
          "accuracy": 0.8741666666666666
        }
      },
      "auroc": 0.9818823784722223
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2283,
          "fn": 117,
          "accuracy": 0.95125
        },
        "0.01": {
          "tp": 2098,
          "fn": 302,
          "accuracy": 0.8741666666666666
        }
      },
      "auroc": 0.9818823784722223
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2143,
          "fn": 257,
          "accuracy": 0.8929166666666667
        },
        "0.01": {
          "tp": 1867,
          "fn": 533,
          "accuracy": 0.7779166666666667
        }
      },
      "auroc": 0.9685460937499999
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2143,
          "fn": 257,
          "accuracy": 0.8929166666666667
        },
        "0.01": {
          "tp": 1867,
          "fn": 533,
          "accuracy": 0.7779166666666667
        }
      },
      "auroc": 0.9685460937499999
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4426,
          "fn": 374,
          "accuracy": 0.9220833333333334
        },
        "0.01": {
          "tp": 3965,
          "fn": 835,
          "accuracy": 0.8260416666666667
        }
      },
      "auroc": 0.9752142361111112
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4426,
          "fn": 374,
          "accuracy": 0.9220833333333334
        },
        "0.01": {
          "tp": 3965,
          "fn": 835,
          "accuracy": 0.8260416666666667
        }
      },
      "auroc": 0.9752142361111112
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 25872,
          "fn": 528,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 24472,
          "fn": 1928,
          "accuracy": 0.926969696969697
        }
      },
      "auroc": 0.9864601325757576
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 14192,
          "fn": 208,
          "accuracy": 0.9855555555555555
        },
        "0.01": {
          "tp": 12836,
          "fn": 1564,
          "accuracy": 0.8913888888888889
        }
      },
      "auroc": 0.9862707175925925
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 40064,
          "fn": 736,
          "accuracy": 0.9819607843137255
        },
        "0.01": {
          "tp": 37308,
          "fn": 3492,
          "accuracy": 0.9144117647058824
        }
      },
      "auroc": 0.9863932802287582
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 24238,
          "fn": 2162,
          "accuracy": 0.9181060606060606
        },
        "0.01": {
          "tp": 20407,
          "fn": 5993,
          "accuracy": 0.7729924242424242
        }
      },
      "auroc": 0.9732829703282829
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 13629,
          "fn": 771,
          "accuracy": 0.9464583333333333
        },
        "0.01": {
          "tp": 11724,
          "fn": 2676,
          "accuracy": 0.8141666666666667
        }
      },
      "auroc": 0.9794968894675925
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 37867,
          "fn": 2933,
          "accuracy": 0.9281127450980392
        },
        "0.01": {
          "tp": 32131,
          "fn": 8669,
          "accuracy": 0.7875245098039215
        }
      },
      "auroc": 0.975476118259804
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 50110,
          "fn": 2690,
          "accuracy": 0.9490530303030303
        },
        "0.01": {
          "tp": 44879,
          "fn": 7921,
          "accuracy": 0.8499810606060606
        }
      },
      "auroc": 0.9798715514520202
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 27821,
          "fn": 979,
          "accuracy": 0.9660069444444445
        },
        "0.01": {
          "tp": 24560,
          "fn": 4240,
          "accuracy": 0.8527777777777777
        }
      },
      "auroc": 0.9828838035300925
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 77931,
          "fn": 3669,
          "accuracy": 0.9550367647058824
        },
        "0.01": {
          "tp": 69439,
          "fn": 12161,
          "accuracy": 0.850968137254902
        }
      },
      "auroc": 0.980934699244281
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.99010625
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9914072916666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9914072916666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9920578125
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9924802083333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9922916666666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9923859375
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9463822916666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9862614583333332
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.966321875
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.96943125
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9892765625
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 766,
          "fn": 34,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.97935390625
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9904333333333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9915708333333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926614583333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9632145833333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9779380208333335
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926848958333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9768239583333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 785,
          "fn": 15,
          "accuracy": 0.98125
        },
        "0.01": null
      },
      "auroc": 0.9847544270833334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926614583333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926848958333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9880062500000001
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9915583333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9897822916666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9903572916666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9921098958333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.99123359375
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9923927083333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9924604166666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9924265625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.94893125
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.988515625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9687234375
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.9706619791666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9904880208333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 770,
          "fn": 30,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9805750000000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926614583333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926848958333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.99165
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9691187499999999
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9803843750000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9921557291666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9809135416666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 783,
          "fn": 17,
          "accuracy": 0.97875
        },
        "0.01": null
      },
      "auroc": 0.9865346354166666
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9909697916666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9909697916666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.982459375
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.982459375
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9867145833333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9867145833333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9892635416666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9892635416666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9763593749999999
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9763593749999999
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9828114583333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9828114583333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9909927083333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9909927083333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9918505208333332
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9918505208333332
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9904541666666666
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9904541666666666
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9912510416666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9912510416666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9908526041666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9908526041666667
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2196,
          "fn": 4,
          "accuracy": 0.9981818181818182
        },
        "0.01": null
      },
      "auroc": 0.9919785037878789
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 1,
          "accuracy": 0.9991666666666666
        },
        "0.01": null
      },
      "auroc": 0.9922105902777777
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3395,
          "fn": 5,
          "accuracy": 0.9985294117647059
        },
        "0.01": null
      },
      "auroc": 0.9920604166666667
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2121,
          "fn": 79,
          "accuracy": 0.9640909090909091
        },
        "0.01": null
      },
      "auroc": 0.9812827651515151
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1161,
          "fn": 39,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9814625000000001
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3282,
          "fn": 118,
          "accuracy": 0.9652941176470589
        },
        "0.01": null
      },
      "auroc": 0.9813462009803922
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 4317,
          "fn": 83,
          "accuracy": 0.9811363636363636
        },
        "0.01": null
      },
      "auroc": 0.986630634469697
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2360,
          "fn": 40,
          "accuracy": 0.9833333333333333
        },
        "0.01": null
      },
      "auroc": 0.986836545138889
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 6677,
          "fn": 123,
          "accuracy": 0.9819117647058824
        },
        "0.01": null
      },
      "auroc": 0.9867033088235295
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.99010625
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9914072916666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9914072916666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9920578125
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9924802083333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9922916666666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9923859375
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9463822916666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9862614583333332
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.966321875
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.96943125
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9892765625
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 766,
          "fn": 34,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.97935390625
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9904333333333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9915708333333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926614583333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9632145833333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9779380208333335
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926848958333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9768239583333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 785,
          "fn": 15,
          "accuracy": 0.98125
        },
        "0.01": null
      },
      "auroc": 0.9847544270833334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926614583333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926848958333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9880062500000001
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9915583333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9897822916666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9903572916666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9921098958333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.99123359375
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9923927083333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9924604166666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9924265625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.94893125
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.988515625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9687234375
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.9706619791666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9904880208333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 770,
          "fn": 30,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9805750000000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926614583333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926848958333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.99165
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9691187499999999
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9803843750000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9921557291666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9809135416666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 783,
          "fn": 17,
          "accuracy": 0.97875
        },
        "0.01": null
      },
      "auroc": 0.9865346354166666
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9909697916666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9909697916666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.982459375
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.982459375
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9867145833333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9867145833333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9892635416666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9892635416666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9763593749999999
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9763593749999999
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9828114583333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9828114583333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9909927083333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9909927083333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9918505208333332
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9918505208333332
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9904541666666666
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9904541666666666
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9912510416666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9912510416666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9908526041666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9908526041666667
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2196,
          "fn": 4,
          "accuracy": 0.9981818181818182
        },
        "0.01": null
      },
      "auroc": 0.9919785037878789
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 1,
          "accuracy": 0.9991666666666666
        },
        "0.01": null
      },
      "auroc": 0.9922105902777777
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3395,
          "fn": 5,
          "accuracy": 0.9985294117647059
        },
        "0.01": null
      },
      "auroc": 0.9920604166666667
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2121,
          "fn": 79,
          "accuracy": 0.9640909090909091
        },
        "0.01": null
      },
      "auroc": 0.9812827651515151
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1161,
          "fn": 39,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9814625000000001
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3282,
          "fn": 118,
          "accuracy": 0.9652941176470589
        },
        "0.01": null
      },
      "auroc": 0.9813462009803922
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 4317,
          "fn": 83,
          "accuracy": 0.9811363636363636
        },
        "0.01": null
      },
      "auroc": 0.986630634469697
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2360,
          "fn": 40,
          "accuracy": 0.9833333333333333
        },
        "0.01": null
      },
      "auroc": 0.986836545138889
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 6677,
          "fn": 123,
          "accuracy": 0.9819117647058824
        },
        "0.01": null
      },
      "auroc": 0.9867033088235295
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9904458333333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9915770833333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9915770833333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9921427083333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9925104166666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9923104166666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9924104166666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.958984375
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9861531250000001
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.97256875
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9757473958333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9892317708333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 776,
          "fn": 24,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9824895833333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.990925
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9918166666666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926614583333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9639145833333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9782880208333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926848958333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9774197916666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 785,
          "fn": 15,
          "accuracy": 0.98125
        },
        "0.01": null
      },
      "auroc": 0.9850523437500001
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9904541666666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9918104166666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9911322916666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.99158125
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.992259375
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9919203124999999
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.99256875
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9925447916666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9925567708333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9600395833333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9898916666666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.9749656250000001
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9763041666666668
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9912182291666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 781,
          "fn": 19,
          "accuracy": 0.97625
        },
        "0.01": null
      },
      "auroc": 0.9837611979166667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926614583333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926848958333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9920291666666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.973121875
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9825755208333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9923453125
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9829151041666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 784,
          "fn": 16,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9876302083333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9910010416666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9910010416666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.982209375
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.982209375
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9866052083333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9866052083333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9907614583333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9907614583333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9776291666666665
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9776291666666665
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9841953125
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9841953125
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9912427083333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9912427083333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9919755208333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9919755208333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.99056875
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.99056875
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9906739583333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9906739583333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9906213541666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9906213541666667
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2197,
          "fn": 3,
          "accuracy": 0.9986363636363637
        },
        "0.01": null
      },
      "auroc": 0.9921466856060606
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 1,
          "accuracy": 0.9991666666666666
        },
        "0.01": null
      },
      "auroc": 0.9923175347222223
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3396,
          "fn": 4,
          "accuracy": 0.9988235294117647
        },
        "0.01": null
      },
      "auroc": 0.9922069852941177
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2145,
          "fn": 55,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9837582386363637
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1162,
          "fn": 38,
          "accuracy": 0.9683333333333334
        },
        "0.01": null
      },
      "auroc": 0.98255625
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3307,
          "fn": 93,
          "accuracy": 0.9726470588235294
        },
        "0.01": null
      },
      "auroc": 0.9833340073529412
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 4342,
          "fn": 58,
          "accuracy": 0.9868181818181818
        },
        "0.01": null
      },
      "auroc": 0.9879524621212121
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2361,
          "fn": 39,
          "accuracy": 0.98375
        },
        "0.01": null
      },
      "auroc": 0.9874368923611111
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 6703,
          "fn": 97,
          "accuracy": 0.985735294117647
        },
        "0.01": null
      },
      "auroc": 0.9877704963235293
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9902510416666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9914796875
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9914796875
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9920940104166667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9924822916666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9923479166666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9924151041666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9560072916666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9858031249999999
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": null
      },
      "auroc": 0.9709052083333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9742447916666668
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9890755208333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 773,
          "fn": 27,
          "accuracy": 0.96625
        },
        "0.01": null
      },
      "auroc": 0.98166015625
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9899458333333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9913270833333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9914239583333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9685499999999999
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9799869791666668
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9920661458333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9792479166666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 783,
          "fn": 17,
          "accuracy": 0.97875
        },
        "0.01": null
      },
      "auroc": 0.98565703125
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9917552083333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9922317708333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9868125
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9913645833333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9890885416666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9897604166666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9915598958333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 795,
          "fn": 5,
          "accuracy": 0.99375
        },
        "0.01": null
      },
      "auroc": 0.99066015625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9925645833333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9907447916666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9916546875
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9534458333333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.991428125
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": null
      },
      "auroc": 0.9724369791666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9730052083333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9910864583333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 771,
          "fn": 29,
          "accuracy": 0.96375
        },
        "0.01": null
      },
      "auroc": 0.9820458333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9906177083333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9711583333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9808880208333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9916630208333332
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9819333333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 786,
          "fn": 14,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9867981770833334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9897843749999999
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9897843749999999
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9803229166666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9803229166666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9850536458333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9850536458333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9844541666666666
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9844541666666666
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9713979166666666
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9713979166666666
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9779260416666666
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9779260416666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9908177083333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9908177083333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9917630208333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9917630208333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9883354166666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9883354166666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.984415625
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.984415625
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9863755208333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9863755208333334
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2189,
          "fn": 11,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9912609848484848
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1195,
          "fn": 5,
          "accuracy": 0.9958333333333333
        },
        "0.01": null
      },
      "auroc": 0.9917017361111111
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3384,
          "fn": 16,
          "accuracy": 0.9952941176470588
        },
        "0.01": null
      },
      "auroc": 0.9914165441176471
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2117,
          "fn": 83,
          "accuracy": 0.9622727272727273
        },
        "0.01": null
      },
      "auroc": 0.9809707386363635
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1164,
          "fn": 36,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9830925347222221
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3281,
          "fn": 119,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9817196078431373
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 4306,
          "fn": 94,
          "accuracy": 0.9786363636363636
        },
        "0.01": null
      },
      "auroc": 0.9861158617424244
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2359,
          "fn": 41,
          "accuracy": 0.9829166666666667
        },
        "0.01": null
      },
      "auroc": 0.9873971354166667
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 6665,
          "fn": 135,
          "accuracy": 0.9801470588235294
        },
        "0.01": null
      },
      "auroc": 0.9865680759803921
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9900916666666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9914000000000001
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9914000000000001
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9920541666666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9924802083333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9921552083333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9923177083333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9447770833333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9860166666666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.965396875
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.9686286458333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9890859375
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 766,
          "fn": 34,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9788572916666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9903958333333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9915520833333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926614583333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.962440625
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9775510416666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926848958333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9764182291666668
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 786,
          "fn": 14,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9845515625000001
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926614583333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926848958333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.98850625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9914000000000001
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.989953125
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9906072916666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9920307291666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.9913190104166667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9924010416666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9924499999999999
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9924255208333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9495291666666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9884427083333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9689859375000001
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9709651041666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9904463541666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 770,
          "fn": 30,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9807057291666668
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926614583333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926848958333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.99165
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.968084375
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9798671875
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9921557291666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9803963541666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 783,
          "fn": 17,
          "accuracy": 0.97875
        },
        "0.01": null
      },
      "auroc": 0.9862760416666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9907677083333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9907677083333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.98154375
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.98154375
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9861557291666666
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9861557291666666
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9886114583333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9886114583333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9767510416666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9767510416666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9826812500000002
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9826812500000002
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9896479166666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9896479166666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.991178125
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.991178125
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9905020833333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9905020833333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.99123125
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.99123125
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9908666666666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9908666666666667
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2195,
          "fn": 5,
          "accuracy": 0.9977272727272727
        },
        "0.01": null
      },
      "auroc": 0.991905965909091
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 1,
          "accuracy": 0.9991666666666666
        },
        "0.01": null
      },
      "auroc": 0.9921798611111111
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3394,
          "fn": 6,
          "accuracy": 0.9982352941176471
        },
        "0.01": null
      },
      "auroc": 0.9920026348039216
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2122,
          "fn": 78,
          "accuracy": 0.9645454545454546
        },
        "0.01": null
      },
      "auroc": 0.9810649621212122
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1161,
          "fn": 39,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9810793402777778
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3283,
          "fn": 117,
          "accuracy": 0.9655882352941176
        },
        "0.01": null
      },
      "auroc": 0.9810700367647058
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 4317,
          "fn": 83,
          "accuracy": 0.9811363636363636
        },
        "0.01": null
      },
      "auroc": 0.9864854640151516
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2360,
          "fn": 40,
          "accuracy": 0.9833333333333333
        },
        "0.01": null
      },
      "auroc": 0.9866296006944446
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 6677,
          "fn": 123,
          "accuracy": 0.9819117647058824
        },
        "0.01": null
      },
      "auroc": 0.9865363357843138
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9911218749999999
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9879072916666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9895145833333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9914083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9833510416666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9873796875
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9912651041666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9856291666666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 788,
          "fn": 12,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9884471354166667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9853520833333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9542510416666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9698015625
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": null
      },
      "auroc": 0.9124583333333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9432197916666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": null
      },
      "auroc": 0.9278390625
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": null
      },
      "auroc": 0.9489052083333332
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": null
      },
      "auroc": 0.9487354166666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 684,
          "fn": 116,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9488203125000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.990984375
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9754802083333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9832322916666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9905697916666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": null
      },
      "auroc": 0.9265479166666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": null
      },
      "auroc": 0.9585588541666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9907770833333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9510140625000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 741,
          "fn": 59,
          "accuracy": 0.92625
        },
        "0.01": null
      },
      "auroc": 0.9708955729166666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9909072916666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9896072916666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9902572916666668
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.972103125
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9715958333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": null
      },
      "auroc": 0.9718494791666668
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9815052083333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9806015625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 772,
          "fn": 28,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9810533854166666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9862729166666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": null
      },
      "auroc": 0.9460635416666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": null
      },
      "auroc": 0.9661682291666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": null
      },
      "auroc": 0.8923062500000001
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": null
      },
      "auroc": 0.899490625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.8958984375000001
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.9392895833333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": null
      },
      "auroc": 0.9227770833333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 633,
          "fn": 167,
          "accuracy": 0.79125
        },
        "0.01": null
      },
      "auroc": 0.9310333333333335
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9923916666666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9885604166666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9904760416666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9862041666666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.9469218749999999
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9665630208333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9892979166666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9677411458333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 758,
          "fn": 42,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.97851953125
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9825791666666666
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9825791666666666
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9693260416666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9693260416666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9759526041666666
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9759526041666666
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9621697916666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9621697916666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9417291666666666
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9417291666666666
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": null
      },
      "auroc": 0.9519494791666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": null
      },
      "auroc": 0.9519494791666667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9923302083333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9923302083333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9912895833333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9912895833333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9918098958333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9918098958333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.991153125
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.991153125
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9816677083333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9816677083333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9864104166666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9864104166666666
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9818958333333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9818958333333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9756333333333332
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9756333333333332
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9787645833333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9787645833333334
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2154,
          "fn": 46,
          "accuracy": 0.9790909090909091
        },
        "0.01": null
      },
      "auroc": 0.9861053030303031
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1125,
          "fn": 75,
          "accuracy": 0.9375
        },
        "0.01": null
      },
      "auroc": 0.9736449652777778
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3279,
          "fn": 121,
          "accuracy": 0.9644117647058823
        },
        "0.01": null
      },
      "auroc": 0.9817075367647059
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1986,
          "fn": 214,
          "accuracy": 0.9027272727272727
        },
        "0.01": null
      },
      "auroc": 0.9640632575757577
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1012,
          "fn": 188,
          "accuracy": 0.8433333333333334
        },
        "0.01": null
      },
      "auroc": 0.9451878472222222
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2998,
          "fn": 402,
          "accuracy": 0.8817647058823529
        },
        "0.01": null
      },
      "auroc": 0.9574013480392156
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 4140,
          "fn": 260,
          "accuracy": 0.9409090909090909
        },
        "0.01": null
      },
      "auroc": 0.9750842803030303
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2137,
          "fn": 263,
          "accuracy": 0.8904166666666666
        },
        "0.01": null
      },
      "auroc": 0.95941640625
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 6277,
          "fn": 523,
          "accuracy": 0.9230882352941177
        },
        "0.01": null
      },
      "auroc": 0.9695544424019608
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9906197916666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9916640625
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9916640625
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9921861979166667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926145833333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9923270833333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9924708333333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9547125000000001
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.985934375
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9703234375
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9736635416666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9891307291666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 772,
          "fn": 28,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9813971354166667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9904708333333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9915895833333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9650010416666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9788546875000002
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9777359375
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 787,
          "fn": 13,
          "accuracy": 0.98375
        },
        "0.01": null
      },
      "auroc": 0.9852221354166668
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926614583333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926848958333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.988265625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.991678125
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9899718750000001
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9904869791666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9921697916666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9913283854166667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9924927083333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9925364583333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9925145833333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9592093749999999
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9888072916666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9740083333333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9758510416666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.990671875
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 777,
          "fn": 23,
          "accuracy": 0.97125
        },
        "0.01": null
      },
      "auroc": 0.9832614583333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926614583333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926848958333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9922427083333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9737593749999999
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9830010416666668
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9924520833333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9832338541666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 786,
          "fn": 14,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9878429687500001
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9910197916666666
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9910197916666666
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9837260416666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9837260416666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9873729166666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9873729166666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9904177083333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9904177083333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9766739583333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9766739583333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9835458333333335
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9835458333333335
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.991684375
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.991684375
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9921963541666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9921963541666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9910833333333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9910833333333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9910197916666666
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9910197916666666
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9910515625
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9910515625
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2196,
          "fn": 4,
          "accuracy": 0.9981818181818182
        },
        "0.01": null
      },
      "auroc": 0.9921664772727273
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 1,
          "accuracy": 0.9991666666666666
        },
        "0.01": null
      },
      "auroc": 0.9922354166666667
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3395,
          "fn": 5,
          "accuracy": 0.9985294117647059
        },
        "0.01": null
      },
      "auroc": 0.9921908088235295
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2138,
          "fn": 62,
          "accuracy": 0.9718181818181818
        },
        "0.01": null
      },
      "auroc": 0.9832417613636364
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1166,
          "fn": 34,
          "accuracy": 0.9716666666666667
        },
        "0.01": null
      },
      "auroc": 0.9826333333333334
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3304,
          "fn": 96,
          "accuracy": 0.971764705882353
        },
        "0.01": null
      },
      "auroc": 0.9830270220588235
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 4334,
          "fn": 66,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9877041193181819
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2365,
          "fn": 35,
          "accuracy": 0.9854166666666667
        },
        "0.01": null
      },
      "auroc": 0.9874343750000001
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 6699,
          "fn": 101,
          "accuracy": 0.9851470588235294
        },
        "0.01": null
      },
      "auroc": 0.9876089154411765
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.99010625
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9914072916666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9914072916666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9920578125
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9924802083333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9922916666666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9923859375
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.946403125
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.986346875
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.966375
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.9694416666666668
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9893192708333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 766,
          "fn": 34,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9793804687500001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9903895833333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9915489583333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926614583333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9625145833333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9775880208333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926848958333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9764520833333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 785,
          "fn": 15,
          "accuracy": 0.98125
        },
        "0.01": null
      },
      "auroc": 0.9845684895833334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926614583333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926848958333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9880062500000001
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9915583333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9897822916666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9903572916666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9921098958333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.99123359375
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9923927083333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9924604166666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9924265625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.948925
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.988515625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9687203125000001
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.9706588541666668
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9904880208333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 770,
          "fn": 30,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9805734375000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926614583333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926848958333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.99165
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9691187499999999
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9803843750000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9921557291666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9809135416666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 783,
          "fn": 17,
          "accuracy": 0.97875
        },
        "0.01": null
      },
      "auroc": 0.9865346354166666
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9909697916666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9909697916666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.982459375
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.982459375
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9867145833333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9867145833333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9892635416666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9892635416666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9763593749999999
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9763593749999999
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9828114583333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9828114583333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9909927083333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9909927083333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9918505208333332
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9918505208333332
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9904541666666666
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9904541666666666
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9912510416666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9912510416666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9908526041666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9908526041666667
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2196,
          "fn": 4,
          "accuracy": 0.9981818181818182
        },
        "0.01": null
      },
      "auroc": 0.9919785037878789
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 1,
          "accuracy": 0.9991666666666666
        },
        "0.01": null
      },
      "auroc": 0.992203298611111
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3395,
          "fn": 5,
          "accuracy": 0.9985294117647059
        },
        "0.01": null
      },
      "auroc": 0.992057843137255
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2121,
          "fn": 79,
          "accuracy": 0.9640909090909091
        },
        "0.01": null
      },
      "auroc": 0.9812840909090909
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1161,
          "fn": 39,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9813600694444444
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3282,
          "fn": 118,
          "accuracy": 0.9652941176470589
        },
        "0.01": null
      },
      "auroc": 0.981310906862745
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 4317,
          "fn": 83,
          "accuracy": 0.9811363636363636
        },
        "0.01": null
      },
      "auroc": 0.9866312973484849
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2360,
          "fn": 40,
          "accuracy": 0.9833333333333333
        },
        "0.01": null
      },
      "auroc": 0.9867816840277779
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 6677,
          "fn": 123,
          "accuracy": 0.9819117647058824
        },
        "0.01": null
      },
      "auroc": 0.9866843750000001
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9803208333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9709
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9756104166666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9748531250000001
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.96396875
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9694109375
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9775869791666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.967434375
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 761,
          "fn": 39,
          "accuracy": 0.95125
        },
        "0.01": null
      },
      "auroc": 0.9725106770833334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.979328125
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9873614583333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9833447916666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": null
      },
      "auroc": 0.9110416666666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9716125
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        },
        "0.01": null
      },
      "auroc": 0.9413270833333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": null
      },
      "auroc": 0.9451848958333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9794869791666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 703,
          "fn": 97,
          "accuracy": 0.87875
        },
        "0.01": null
      },
      "auroc": 0.9623359375
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9785843750000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": null
      },
      "auroc": 0.9540541666666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": null
      },
      "auroc": 0.9663192708333332
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9746385416666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.93414375
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9543911458333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9766114583333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.9440989583333335
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 711,
          "fn": 89,
          "accuracy": 0.88875
        },
        "0.01": null
      },
      "auroc": 0.9603552083333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9893062499999999
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9772156249999999
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9832609375000001
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": null
      },
      "auroc": 0.9229010416666665
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.9418510416666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 280,
          "fn": 120,
          "accuracy": 0.7
        },
        "0.01": null
      },
      "auroc": 0.9323760416666668
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": null
      },
      "auroc": 0.9561036458333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": null
      },
      "auroc": 0.9595333333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 674,
          "fn": 126,
          "accuracy": 0.8425
        },
        "0.01": null
      },
      "auroc": 0.9578184895833334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9821135416666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9793770833333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9807453125000001
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.9134979166666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9465604166666668
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        },
        "0.01": null
      },
      "auroc": 0.9300291666666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        },
        "0.01": null
      },
      "auroc": 0.9478057291666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        },
        "0.01": null
      },
      "auroc": 0.9629687499999999
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 655,
          "fn": 145,
          "accuracy": 0.81875
        },
        "0.01": null
      },
      "auroc": 0.9553872395833333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9703677083333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.96264375
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9665057291666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9562177083333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": null
      },
      "auroc": 0.936515625
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.9463666666666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": null
      },
      "auroc": 0.9632927083333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": null
      },
      "auroc": 0.9495796875
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 698,
          "fn": 102,
          "accuracy": 0.8725
        },
        "0.01": null
      },
      "auroc": 0.9564361979166667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9678239583333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9678239583333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": null
      },
      "auroc": 0.9435885416666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": null
      },
      "auroc": 0.9435885416666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.95570625
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.95570625
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": null
      },
      "auroc": 0.9492958333333332
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": null
      },
      "auroc": 0.9492958333333332
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.9386229166666666
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.9386229166666666
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": null
      },
      "auroc": 0.9439593749999999
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": null
      },
      "auroc": 0.9439593749999999
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.981471875
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.981471875
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9731291666666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9731291666666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9773005208333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9773005208333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9691364583333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9691364583333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.9470541666666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.9470541666666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9580953125
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9580953125
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.95985625
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.95985625
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.9576510416666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.9576510416666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.9587536458333332
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.9587536458333332
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2066,
          "fn": 134,
          "accuracy": 0.9390909090909091
        },
        "0.01": null
      },
      "auroc": 0.9734186553030303
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1122,
          "fn": 78,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9719253472222222
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3188,
          "fn": 212,
          "accuracy": 0.9376470588235294
        },
        "0.01": null
      },
      "auroc": 0.9728916053921568
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1763,
          "fn": 437,
          "accuracy": 0.8013636363636364
        },
        "0.01": null
      },
      "auroc": 0.9466541666666666
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 989,
          "fn": 211,
          "accuracy": 0.8241666666666667
        },
        "0.01": null
      },
      "auroc": 0.9491086805555556
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2752,
          "fn": 648,
          "accuracy": 0.8094117647058824
        },
        "0.01": null
      },
      "auroc": 0.9475204656862746
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3829,
          "fn": 571,
          "accuracy": 0.8702272727272727
        },
        "0.01": null
      },
      "auroc": 0.9600364109848485
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2111,
          "fn": 289,
          "accuracy": 0.8795833333333334
        },
        "0.01": null
      },
      "auroc": 0.9605170138888889
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5940,
          "fn": 860,
          "accuracy": 0.8735294117647059
        },
        "0.01": null
      },
      "auroc": 0.9602060355392157
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9905250000000001
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9916166666666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9916166666666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9921625000000001
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9925395833333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9922916666666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.992415625
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9547208333333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9862614583333332
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9704911458333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9736302083333335
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9892765625
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 772,
          "fn": 28,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9814533854166667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9903406250000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9915244791666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926614583333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9635958333333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9781286458333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926848958333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9769682291666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 784,
          "fn": 16,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9848265625000001
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.992603125
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926557291666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9892677083333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.99189375
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9905807291666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9909880208333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9922484375
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9916182291666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.992565625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9925770833333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9925713541666668
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9578822916666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.990346875
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9741145833333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9752239583333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9914619791666668
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 777,
          "fn": 23,
          "accuracy": 0.97125
        },
        "0.01": null
      },
      "auroc": 0.9833429687500002
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926614583333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926848958333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9922427083333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9738031249999999
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9830229166666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9924520833333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9832557291666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.98785390625
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9925447916666666
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9925447916666666
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9831770833333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9831770833333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9878609375
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9878609375
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9899614583333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9899614583333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9789302083333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9789302083333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9844458333333335
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9844458333333335
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926614583333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926614583333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.99195
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.99195
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9923057291666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9923057291666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9905083333333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9905083333333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911989583333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9911989583333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9908536458333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9908536458333334
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2197,
          "fn": 3,
          "accuracy": 0.9986363636363637
        },
        "0.01": null
      },
      "auroc": 0.9922069128787879
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 1,
          "accuracy": 0.9991666666666666
        },
        "0.01": null
      },
      "auroc": 0.9922048611111112
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3396,
          "fn": 4,
          "accuracy": 0.9988235294117647
        },
        "0.01": null
      },
      "auroc": 0.9922061887254902
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2144,
          "fn": 56,
          "accuracy": 0.9745454545454545
        },
        "0.01": null
      },
      "auroc": 0.983404356060606
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1167,
          "fn": 33,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9827376736111111
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3311,
          "fn": 89,
          "accuracy": 0.9738235294117648
        },
        "0.01": null
      },
      "auroc": 0.983169056372549
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 4341,
          "fn": 59,
          "accuracy": 0.9865909090909091
        },
        "0.01": null
      },
      "auroc": 0.987805634469697
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2366,
          "fn": 34,
          "accuracy": 0.9858333333333333
        },
        "0.01": null
      },
      "auroc": 0.9874712673611112
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 6707,
          "fn": 93,
          "accuracy": 0.9863235294117647
        },
        "0.01": null
      },
      "auroc": 0.9876876225490195
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926614583333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9897645833333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9912130208333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926848958333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9912364583333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9919606770833334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9925270833333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.992221875
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9923744791666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9467458333333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9861687499999999
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9664572916666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": null
      },
      "auroc": 0.9696364583333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9891953125
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 768,
          "fn": 32,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9794158854166667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9902906250000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9914994791666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9636604166666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.978184375
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9769755208333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 784,
          "fn": 16,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9848419270833334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.992640625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926744791666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9880156250000001
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9904635416666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9892395833333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9903619791666668
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9915520833333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.99095703125
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.99244375
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9924833333333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9924635416666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9499135416666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9884958333333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9692046875
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.9711786458333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9904895833333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 770,
          "fn": 30,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9808341145833334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926614583333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9926848958333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9916874999999999
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9688625
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.980275
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9921744791666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9807854166666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 783,
          "fn": 17,
          "accuracy": 0.97875
        },
        "0.01": null
      },
      "auroc": 0.9864799479166666
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9907854166666666
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9907854166666666
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.982146875
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.982146875
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9864661458333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9864661458333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9890395833333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9890395833333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.97511875
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.97511875
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9820791666666666
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9820791666666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9927083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9905791666666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9905791666666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.99164375
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.99164375
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9904541666666666
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9904541666666666
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9910427083333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9910427083333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9907484375000001
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9907484375000001
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2196,
          "fn": 4,
          "accuracy": 0.9981818181818182
        },
        "0.01": null
      },
      "auroc": 0.9919502840909091
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 1,
          "accuracy": 0.9991666666666666
        },
        "0.01": null
      },
      "auroc": 0.9921755208333333
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3395,
          "fn": 5,
          "accuracy": 0.9985294117647059
        },
        "0.01": null
      },
      "auroc": 0.9920297794117647
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2122,
          "fn": 78,
          "accuracy": 0.9645454545454546
        },
        "0.01": null
      },
      "auroc": 0.9812116477272727
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1159,
          "fn": 41,
          "accuracy": 0.9658333333333333
        },
        "0.01": null
      },
      "auroc": 0.9812359375000002
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3281,
          "fn": 119,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9812202205882354
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 4318,
          "fn": 82,
          "accuracy": 0.9813636363636363
        },
        "0.01": null
      },
      "auroc": 0.986580965909091
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2358,
          "fn": 42,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9867057291666667
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 6676,
          "fn": 124,
          "accuracy": 0.981764705882353
        },
        "0.01": null
      },
      "auroc": 0.986625
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9563031249999999
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9516520833333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9539776041666668
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.953821875
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9472666666666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9505442708333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9550624999999999
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9494593750000001
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 704,
          "fn": 96,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9522609375
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9614864583333332
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9740427083333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9677645833333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": null
      },
      "auroc": 0.9175875
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": null
      },
      "auroc": 0.9515625
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        },
        "0.01": null
      },
      "auroc": 0.934575
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9395369791666668
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": null
      },
      "auroc": 0.9628026041666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 647,
          "fn": 153,
          "accuracy": 0.80875
        },
        "0.01": null
      },
      "auroc": 0.9511697916666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9576677083333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": null
      },
      "auroc": 0.9340718749999999
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9458697916666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9495374999999999
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": null
      },
      "auroc": 0.9232354166666668
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        },
        "0.01": null
      },
      "auroc": 0.9363864583333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9536026041666668
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 274,
          "fn": 126,
          "accuracy": 0.685
        },
        "0.01": null
      },
      "auroc": 0.9286536458333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 630,
          "fn": 170,
          "accuracy": 0.7875
        },
        "0.01": null
      },
      "auroc": 0.9411281250000001
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9815458333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9556666666666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.96860625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": null
      },
      "auroc": 0.9197572916666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": null
      },
      "auroc": 0.9283458333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 242,
          "fn": 158,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.9240515625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        },
        "0.01": null
      },
      "auroc": 0.9506515625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": null
      },
      "auroc": 0.94200625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 621,
          "fn": 179,
          "accuracy": 0.77625
        },
        "0.01": null
      },
      "auroc": 0.94632890625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9663302083333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9744072916666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.97036875
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        },
        "0.01": null
      },
      "auroc": 0.9249
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        },
        "0.01": null
      },
      "auroc": 0.9414302083333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": null
      },
      "auroc": 0.9331651041666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 299,
          "fn": 101,
          "accuracy": 0.7475
        },
        "0.01": null
      },
      "auroc": 0.9456151041666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        },
        "0.01": null
      },
      "auroc": 0.95791875
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 610,
          "fn": 190,
          "accuracy": 0.7625
        },
        "0.01": null
      },
      "auroc": 0.9517669270833333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.955090625
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9434406249999999
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": null
      },
      "auroc": 0.949265625
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.936134375
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": null
      },
      "auroc": 0.9202708333333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 273,
          "fn": 127,
          "accuracy": 0.6825
        },
        "0.01": null
      },
      "auroc": 0.9282026041666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        },
        "0.01": null
      },
      "auroc": 0.9456125000000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        },
        "0.01": null
      },
      "auroc": 0.9318557291666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 612,
          "fn": 188,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.9387341145833334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.9462364583333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.9462364583333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.9335781249999999
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.9335781249999999
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.9399072916666666
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.9399072916666666
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": null
      },
      "auroc": 0.9313562499999999
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": null
      },
      "auroc": 0.9313562499999999
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": null
      },
      "auroc": 0.9225895833333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": null
      },
      "auroc": 0.9225895833333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.9269729166666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.9269729166666667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9574802083333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9574802083333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.9444802083333332
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.9444802083333332
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": null
      },
      "auroc": 0.9509802083333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": null
      },
      "auroc": 0.9509802083333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.9524885416666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.9524885416666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": null
      },
      "auroc": 0.9260468749999999
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": null
      },
      "auroc": 0.9260468749999999
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9392677083333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9392677083333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.9418437500000001
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.9418437500000001
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.941165625
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.941165625
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.9415046875
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.9415046875
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1914,
          "fn": 286,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9552571969696969
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1028,
          "fn": 172,
          "accuracy": 0.8566666666666667
        },
        "0.01": null
      },
      "auroc": 0.955546875
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2942,
          "fn": 458,
          "accuracy": 0.8652941176470588
        },
        "0.01": null
      },
      "auroc": 0.9553594362745098
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1573,
          "fn": 627,
          "accuracy": 0.715
        },
        "0.01": null
      },
      "auroc": 0.9335999053030303
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 844,
          "fn": 356,
          "accuracy": 0.7033333333333334
        },
        "0.01": null
      },
      "auroc": 0.9353519097222223
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2417,
          "fn": 983,
          "accuracy": 0.7108823529411765
        },
        "0.01": null
      },
      "auroc": 0.9342182598039215
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3487,
          "fn": 913,
          "accuracy": 0.7925
        },
        "0.01": null
      },
      "auroc": 0.9444285511363636
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1872,
          "fn": 528,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.9454493923611111
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 5359,
          "fn": 1441,
          "accuracy": 0.7880882352941176
        },
        "0.01": null
      },
      "auroc": 0.9447888480392158
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2378,
          "fn": 22,
          "accuracy": 0.9908333333333333
        },
        "0.01": null
      },
      "auroc": 0.9885100694444444
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2358,
          "fn": 42,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.98706953125
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4736,
          "fn": 64,
          "accuracy": 0.9866666666666667
        },
        "0.01": null
      },
      "auroc": 0.9877898003472223
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2369,
          "fn": 31,
          "accuracy": 0.9870833333333333
        },
        "0.01": null
      },
      "auroc": 0.9878676215277779
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2333,
          "fn": 67,
          "accuracy": 0.9720833333333333
        },
        "0.01": null
      },
      "auroc": 0.98388359375
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4702,
          "fn": 98,
          "accuracy": 0.9795833333333334
        },
        "0.01": null
      },
      "auroc": 0.9858756076388888
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4747,
          "fn": 53,
          "accuracy": 0.9889583333333334
        },
        "0.01": null
      },
      "auroc": 0.988188845486111
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4691,
          "fn": 109,
          "accuracy": 0.9772916666666667
        },
        "0.01": null
      },
      "auroc": 0.9854765624999999
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9438,
          "fn": 162,
          "accuracy": 0.983125
        },
        "0.01": null
      },
      "auroc": 0.9868327039930556
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2368,
          "fn": 32,
          "accuracy": 0.9866666666666667
        },
        "0.01": null
      },
      "auroc": 0.9882301215277778
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2359,
          "fn": 41,
          "accuracy": 0.9829166666666667
        },
        "0.01": null
      },
      "auroc": 0.98718203125
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4727,
          "fn": 73,
          "accuracy": 0.9847916666666666
        },
        "0.01": null
      },
      "auroc": 0.987706076388889
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1950,
          "fn": 450,
          "accuracy": 0.8125
        },
        "0.01": null
      },
      "auroc": 0.9413502604166667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2282,
          "fn": 118,
          "accuracy": 0.9508333333333333
        },
        "0.01": null
      },
      "auroc": 0.9784668402777779
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4232,
          "fn": 568,
          "accuracy": 0.8816666666666667
        },
        "0.01": null
      },
      "auroc": 0.9599085503472222
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4318,
          "fn": 482,
          "accuracy": 0.8995833333333333
        },
        "0.01": null
      },
      "auroc": 0.9647901909722222
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4641,
          "fn": 159,
          "accuracy": 0.966875
        },
        "0.01": null
      },
      "auroc": 0.9828244357638889
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8959,
          "fn": 641,
          "accuracy": 0.9332291666666667
        },
        "0.01": null
      },
      "auroc": 0.9738073133680554
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2374,
          "fn": 26,
          "accuracy": 0.9891666666666666
        },
        "0.01": null
      },
      "auroc": 0.9884676215277779
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2295,
          "fn": 105,
          "accuracy": 0.95625
        },
        "0.01": null
      },
      "auroc": 0.9814359374999999
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4669,
          "fn": 131,
          "accuracy": 0.9727083333333333
        },
        "0.01": null
      },
      "auroc": 0.9849517795138889
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2363,
          "fn": 37,
          "accuracy": 0.9845833333333334
        },
        "0.01": null
      },
      "auroc": 0.9872962673611111
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2114,
          "fn": 286,
          "accuracy": 0.8808333333333334
        },
        "0.01": null
      },
      "auroc": 0.9550027777777779
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4477,
          "fn": 323,
          "accuracy": 0.9327083333333334
        },
        "0.01": null
      },
      "auroc": 0.9711495225694444
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4737,
          "fn": 63,
          "accuracy": 0.986875
        },
        "0.01": null
      },
      "auroc": 0.9878819444444444
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4409,
          "fn": 391,
          "accuracy": 0.9185416666666667
        },
        "0.01": null
      },
      "auroc": 0.9682193576388889
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9146,
          "fn": 454,
          "accuracy": 0.9527083333333334
        },
        "0.01": null
      },
      "auroc": 0.9780506510416667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2396,
          "fn": 4,
          "accuracy": 0.9983333333333333
        },
        "0.01": null
      },
      "auroc": 0.99134453125
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2375,
          "fn": 25,
          "accuracy": 0.9895833333333334
        },
        "0.01": null
      },
      "auroc": 0.9879586805555555
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4771,
          "fn": 29,
          "accuracy": 0.9939583333333334
        },
        "0.01": null
      },
      "auroc": 0.9896516059027778
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2215,
          "fn": 185,
          "accuracy": 0.9229166666666667
        },
        "0.01": null
      },
      "auroc": 0.9758418402777778
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2255,
          "fn": 145,
          "accuracy": 0.9395833333333333
        },
        "0.01": null
      },
      "auroc": 0.9804231770833333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4470,
          "fn": 330,
          "accuracy": 0.93125
        },
        "0.01": null
      },
      "auroc": 0.9781325086805555
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4611,
          "fn": 189,
          "accuracy": 0.960625
        },
        "0.01": null
      },
      "auroc": 0.9835931857638889
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4630,
          "fn": 170,
          "accuracy": 0.9645833333333333
        },
        "0.01": null
      },
      "auroc": 0.9841909288194445
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9241,
          "fn": 359,
          "accuracy": 0.9626041666666667
        },
        "0.01": null
      },
      "auroc": 0.9838920572916667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2370,
          "fn": 30,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9889109375
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2333,
          "fn": 67,
          "accuracy": 0.9720833333333333
        },
        "0.01": null
      },
      "auroc": 0.98588046875
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4703,
          "fn": 97,
          "accuracy": 0.9797916666666666
        },
        "0.01": null
      },
      "auroc": 0.987395703125
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1938,
          "fn": 462,
          "accuracy": 0.8075
        },
        "0.01": null
      },
      "auroc": 0.9422926215277778
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2213,
          "fn": 187,
          "accuracy": 0.9220833333333334
        },
        "0.01": null
      },
      "auroc": 0.9742033854166667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4151,
          "fn": 649,
          "accuracy": 0.8647916666666666
        },
        "0.01": null
      },
      "auroc": 0.9582480034722223
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4308,
          "fn": 492,
          "accuracy": 0.8975
        },
        "0.01": null
      },
      "auroc": 0.9656017795138888
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4546,
          "fn": 254,
          "accuracy": 0.9470833333333334
        },
        "0.01": null
      },
      "auroc": 0.9800419270833334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8854,
          "fn": 746,
          "accuracy": 0.9222916666666666
        },
        "0.01": null
      },
      "auroc": 0.9728218532986111
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2361,
          "fn": 39,
          "accuracy": 0.98375
        },
        "0.01": null
      },
      "auroc": 0.9876541666666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2342,
          "fn": 58,
          "accuracy": 0.9758333333333333
        },
        "0.01": null
      },
      "auroc": 0.9857516493055556
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4703,
          "fn": 97,
          "accuracy": 0.9797916666666666
        },
        "0.01": null
      },
      "auroc": 0.9867029079861112
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2316,
          "fn": 84,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9836646701388888
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2110,
          "fn": 290,
          "accuracy": 0.8791666666666667
        },
        "0.01": null
      },
      "auroc": 0.961654513888889
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4426,
          "fn": 374,
          "accuracy": 0.9220833333333334
        },
        "0.01": null
      },
      "auroc": 0.972659592013889
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4677,
          "fn": 123,
          "accuracy": 0.974375
        },
        "0.01": null
      },
      "auroc": 0.9856594184027778
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4452,
          "fn": 348,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.9737030815972223
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9129,
          "fn": 471,
          "accuracy": 0.9509375
        },
        "0.01": null
      },
      "auroc": 0.9796812500000001
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2334,
          "fn": 66,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9846210069444443
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2334,
          "fn": 66,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9846210069444443
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2246,
          "fn": 154,
          "accuracy": 0.9358333333333333
        },
        "0.01": null
      },
      "auroc": 0.9739164062500001
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2246,
          "fn": 154,
          "accuracy": 0.9358333333333333
        },
        "0.01": null
      },
      "auroc": 0.9739164062500001
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4580,
          "fn": 220,
          "accuracy": 0.9541666666666667
        },
        "0.01": null
      },
      "auroc": 0.9792687065972223
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4580,
          "fn": 220,
          "accuracy": 0.9541666666666667
        },
        "0.01": null
      },
      "auroc": 0.9792687065972223
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2266,
          "fn": 134,
          "accuracy": 0.9441666666666667
        },
        "0.01": null
      },
      "auroc": 0.9786548611111111
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2266,
          "fn": 134,
          "accuracy": 0.9441666666666667
        },
        "0.01": null
      },
      "auroc": 0.9786548611111111
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2165,
          "fn": 235,
          "accuracy": 0.9020833333333333
        },
        "0.01": null
      },
      "auroc": 0.9657100694444444
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2165,
          "fn": 235,
          "accuracy": 0.9020833333333333
        },
        "0.01": null
      },
      "auroc": 0.9657100694444444
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4431,
          "fn": 369,
          "accuracy": 0.923125
        },
        "0.01": null
      },
      "auroc": 0.9721824652777777
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4431,
          "fn": 369,
          "accuracy": 0.923125
        },
        "0.01": null
      },
      "auroc": 0.9721824652777777
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2380,
          "fn": 20,
          "accuracy": 0.9916666666666667
        },
        "0.01": null
      },
      "auroc": 0.9888047743055555
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2380,
          "fn": 20,
          "accuracy": 0.9916666666666667
        },
        "0.01": null
      },
      "auroc": 0.9888047743055555
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2350,
          "fn": 50,
          "accuracy": 0.9791666666666666
        },
        "0.01": null
      },
      "auroc": 0.9869394965277778
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2350,
          "fn": 50,
          "accuracy": 0.9791666666666666
        },
        "0.01": null
      },
      "auroc": 0.9869394965277778
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4730,
          "fn": 70,
          "accuracy": 0.9854166666666667
        },
        "0.01": null
      },
      "auroc": 0.9878721354166666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4730,
          "fn": 70,
          "accuracy": 0.9854166666666667
        },
        "0.01": null
      },
      "auroc": 0.9878721354166666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2351,
          "fn": 49,
          "accuracy": 0.9795833333333334
        },
        "0.01": null
      },
      "auroc": 0.9872588541666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2351,
          "fn": 49,
          "accuracy": 0.9795833333333334
        },
        "0.01": null
      },
      "auroc": 0.9872588541666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2269,
          "fn": 131,
          "accuracy": 0.9454166666666667
        },
        "0.01": null
      },
      "auroc": 0.9811390625
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2269,
          "fn": 131,
          "accuracy": 0.9454166666666667
        },
        "0.01": null
      },
      "auroc": 0.9811390625
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4620,
          "fn": 180,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9841989583333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4620,
          "fn": 180,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9841989583333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2314,
          "fn": 86,
          "accuracy": 0.9641666666666666
        },
        "0.01": null
      },
      "auroc": 0.983034201388889
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2314,
          "fn": 86,
          "accuracy": 0.9641666666666666
        },
        "0.01": null
      },
      "auroc": 0.983034201388889
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2292,
          "fn": 108,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.982315451388889
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2292,
          "fn": 108,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.982315451388889
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4606,
          "fn": 194,
          "accuracy": 0.9595833333333333
        },
        "0.01": null
      },
      "auroc": 0.9826748263888889
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4606,
          "fn": 194,
          "accuracy": 0.9595833333333333
        },
        "0.01": null
      },
      "auroc": 0.9826748263888889
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 25892,
          "fn": 508,
          "accuracy": 0.9807575757575757
        },
        "0.01": null
      },
      "auroc": 0.9868628314393939
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 14062,
          "fn": 338,
          "accuracy": 0.9765277777777778
        },
        "0.01": null
      },
      "auroc": 0.9858797164351852
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 39954,
          "fn": 846,
          "accuracy": 0.9792647058823529
        },
        "0.01": null
      },
      "auroc": 0.9865158496732026
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 24473,
          "fn": 1927,
          "accuracy": 0.9270075757575758
        },
        "0.01": null
      },
      "auroc": 0.9734848879419191
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 13307,
          "fn": 1093,
          "accuracy": 0.9240972222222222
        },
        "0.01": null
      },
      "auroc": 0.9722723813657407
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 37780,
          "fn": 3020,
          "accuracy": 0.9259803921568628
        },
        "0.01": null
      },
      "auroc": 0.9730569444444445
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 50365,
          "fn": 2435,
          "accuracy": 0.9538825757575757
        },
        "0.01": null
      },
      "auroc": 0.9801738596906565
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 27369,
          "fn": 1431,
          "accuracy": 0.9503125
        },
        "0.01": null
      },
      "auroc": 0.979076048900463
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 77734,
          "fn": 3866,
          "accuracy": 0.9526225490196079
        },
        "0.01": null
      },
      "auroc": 0.9797863970588234
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1598,
          "fn": 2,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9937374348958333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3198,
          "fn": 2,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9938869466145834
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3198,
          "fn": 2,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9938869466145834
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 6398,
          "fn": 2,
          "accuracy": 0.9996875
        },
        "0.01": null
      },
      "auroc": 0.9939617024739584
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1598,
          "fn": 2,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9933712727864584
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1599,
          "fn": 1,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9934166341145834
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.9933939534505208
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1374,
          "fn": 226,
          "accuracy": 0.85875
        },
        "0.01": null
      },
      "auroc": 0.9574747395833334
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1584,
          "fn": 16,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9916378743489583
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2958,
          "fn": 242,
          "accuracy": 0.924375
        },
        "0.01": null
      },
      "auroc": 0.9745563069661458
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2972,
          "fn": 228,
          "accuracy": 0.92875
        },
        "0.01": null
      },
      "auroc": 0.9754230061848957
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3183,
          "fn": 17,
          "accuracy": 0.9946875
        },
        "0.01": null
      },
      "auroc": 0.9925272542317709
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 6155,
          "fn": 245,
          "accuracy": 0.96171875
        },
        "0.01": null
      },
      "auroc": 0.9839751302083335
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940237467447917
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9935439941406249
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3195,
          "fn": 5,
          "accuracy": 0.9984375
        },
        "0.01": null
      },
      "auroc": 0.9937838704427082
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9939813802083333
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1562,
          "fn": 38,
          "accuracy": 0.97625
        },
        "0.01": null
      },
      "auroc": 0.9877457682291667
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3162,
          "fn": 38,
          "accuracy": 0.988125
        },
        "0.01": null
      },
      "auroc": 0.99086357421875
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940025634765625
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3157,
          "fn": 43,
          "accuracy": 0.9865625
        },
        "0.01": null
      },
      "auroc": 0.990644881184896
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 6357,
          "fn": 43,
          "accuracy": 0.99328125
        },
        "0.01": null
      },
      "auroc": 0.9923237223307291
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1596,
          "fn": 4,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9938459798177084
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940220540364584
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3196,
          "fn": 4,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9939340169270834
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1567,
          "fn": 33,
          "accuracy": 0.979375
        },
        "0.01": null
      },
      "auroc": 0.9858031575520834
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1593,
          "fn": 7,
          "accuracy": 0.995625
        },
        "0.01": null
      },
      "auroc": 0.9927458170572916
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3160,
          "fn": 40,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9892744873046875
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3163,
          "fn": 37,
          "accuracy": 0.9884375
        },
        "0.01": null
      },
      "auroc": 0.9898245686848958
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3193,
          "fn": 7,
          "accuracy": 0.9978125
        },
        "0.01": null
      },
      "auroc": 0.9933839355468749
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 6356,
          "fn": 44,
          "accuracy": 0.993125
        },
        "0.01": null
      },
      "auroc": 0.9916042521158854
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1596,
          "fn": 4,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9931047200520833
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9931652994791668
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3191,
          "fn": 9,
          "accuracy": 0.9971875
        },
        "0.01": null
      },
      "auroc": 0.993135009765625
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1347,
          "fn": 253,
          "accuracy": 0.841875
        },
        "0.01": null
      },
      "auroc": 0.9506202962239584
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1555,
          "fn": 45,
          "accuracy": 0.971875
        },
        "0.01": null
      },
      "auroc": 0.9855817220052083
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2902,
          "fn": 298,
          "accuracy": 0.906875
        },
        "0.01": null
      },
      "auroc": 0.9681010091145832
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2943,
          "fn": 257,
          "accuracy": 0.9196875
        },
        "0.01": null
      },
      "auroc": 0.9718625081380208
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3150,
          "fn": 50,
          "accuracy": 0.984375
        },
        "0.01": null
      },
      "auroc": 0.9893735107421875
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 6093,
          "fn": 307,
          "accuracy": 0.95203125
        },
        "0.01": null
      },
      "auroc": 0.9806180094401041
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940107259114583
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940298339843749
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940202799479166
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.993577001953125
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1579,
          "fn": 21,
          "accuracy": 0.986875
        },
        "0.01": null
      },
      "auroc": 0.99034951171875
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3176,
          "fn": 24,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9919632568359374
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.9937938639322916
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3179,
          "fn": 21,
          "accuracy": 0.9934375
        },
        "0.01": null
      },
      "auroc": 0.9921896728515625
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 6376,
          "fn": 24,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.9929917683919272
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1594,
          "fn": 6,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.9929011555989583
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1594,
          "fn": 6,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.9929011555989583
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1580,
          "fn": 20,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9903494954427083
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1580,
          "fn": 20,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9903494954427083
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3174,
          "fn": 26,
          "accuracy": 0.991875
        },
        "0.01": null
      },
      "auroc": 0.9916253255208334
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3174,
          "fn": 26,
          "accuracy": 0.991875
        },
        "0.01": null
      },
      "auroc": 0.9916253255208334
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1496,
          "fn": 104,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9764663899739584
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1496,
          "fn": 104,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9764663899739584
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1379,
          "fn": 221,
          "accuracy": 0.861875
        },
        "0.01": null
      },
      "auroc": 0.9552696614583334
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1379,
          "fn": 221,
          "accuracy": 0.861875
        },
        "0.01": null
      },
      "auroc": 0.9552696614583334
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2875,
          "fn": 325,
          "accuracy": 0.8984375
        },
        "0.01": null
      },
      "auroc": 0.9658680257161459
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2875,
          "fn": 325,
          "accuracy": 0.8984375
        },
        "0.01": null
      },
      "auroc": 0.9658680257161459
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9929601236979166
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9929601236979166
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9928636718750001
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9928636718750001
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3192,
          "fn": 8,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9929118977864584
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3192,
          "fn": 8,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9929118977864584
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9936555989583333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9936555989583333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.9938460286458333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.9938460286458333
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1581,
          "fn": 19,
          "accuracy": 0.988125
        },
        "0.01": null
      },
      "auroc": 0.9907085286458335
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1581,
          "fn": 19,
          "accuracy": 0.988125
        },
        "0.01": null
      },
      "auroc": 0.9907085286458335
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1546,
          "fn": 54,
          "accuracy": 0.96625
        },
        "0.01": null
      },
      "auroc": 0.9850530436197917
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1546,
          "fn": 54,
          "accuracy": 0.96625
        },
        "0.01": null
      },
      "auroc": 0.9850530436197917
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3127,
          "fn": 73,
          "accuracy": 0.9771875
        },
        "0.01": null
      },
      "auroc": 0.9878807861328125
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3127,
          "fn": 73,
          "accuracy": 0.9771875
        },
        "0.01": null
      },
      "auroc": 0.9878807861328125
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 17458,
          "fn": 142,
          "accuracy": 0.9919318181818182
        },
        "0.01": null
      },
      "auroc": 0.9917695963541667
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 9589,
          "fn": 11,
          "accuracy": 0.9988541666666667
        },
        "0.01": null
      },
      "auroc": 0.993702379014757
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 27047,
          "fn": 153,
          "accuracy": 0.994375
        },
        "0.01": null
      },
      "auroc": 0.9924517549402574
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 16782,
          "fn": 818,
          "accuracy": 0.9535227272727272
        },
        "0.01": null
      },
      "auroc": 0.9811531368371214
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 9471,
          "fn": 129,
          "accuracy": 0.9865625
        },
        "0.01": null
      },
      "auroc": 0.9902996880425348
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 26253,
          "fn": 947,
          "accuracy": 0.9651838235294118
        },
        "0.01": null
      },
      "auroc": 0.9843813313802083
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 34240,
          "fn": 960,
          "accuracy": 0.9727272727272728
        },
        "0.01": null
      },
      "auroc": 0.986461366595644
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 19060,
          "fn": 140,
          "accuracy": 0.9927083333333333
        },
        "0.01": null
      },
      "auroc": 0.9920010335286458
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 53300,
          "fn": 1100,
          "accuracy": 0.9797794117647058
        },
        "0.01": null
      },
      "auroc": 0.9884165431602328
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1598,
          "fn": 2,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9937374348958333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3198,
          "fn": 2,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9938869466145834
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3198,
          "fn": 2,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9938869466145834
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 6398,
          "fn": 2,
          "accuracy": 0.9996875
        },
        "0.01": null
      },
      "auroc": 0.9939617024739584
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1598,
          "fn": 2,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9933712727864584
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1599,
          "fn": 1,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9934166341145834
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.9933939534505208
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1374,
          "fn": 226,
          "accuracy": 0.85875
        },
        "0.01": null
      },
      "auroc": 0.9574747395833334
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1584,
          "fn": 16,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9916378743489583
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2958,
          "fn": 242,
          "accuracy": 0.924375
        },
        "0.01": null
      },
      "auroc": 0.9745563069661458
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2972,
          "fn": 228,
          "accuracy": 0.92875
        },
        "0.01": null
      },
      "auroc": 0.9754230061848957
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3183,
          "fn": 17,
          "accuracy": 0.9946875
        },
        "0.01": null
      },
      "auroc": 0.9925272542317709
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 6155,
          "fn": 245,
          "accuracy": 0.96171875
        },
        "0.01": null
      },
      "auroc": 0.9839751302083335
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940237467447917
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9935439941406249
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3195,
          "fn": 5,
          "accuracy": 0.9984375
        },
        "0.01": null
      },
      "auroc": 0.9937838704427082
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9939813802083333
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1562,
          "fn": 38,
          "accuracy": 0.97625
        },
        "0.01": null
      },
      "auroc": 0.9877457682291667
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3162,
          "fn": 38,
          "accuracy": 0.988125
        },
        "0.01": null
      },
      "auroc": 0.99086357421875
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940025634765625
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3157,
          "fn": 43,
          "accuracy": 0.9865625
        },
        "0.01": null
      },
      "auroc": 0.990644881184896
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 6357,
          "fn": 43,
          "accuracy": 0.99328125
        },
        "0.01": null
      },
      "auroc": 0.9923237223307291
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1596,
          "fn": 4,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9938459798177084
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940220540364584
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3196,
          "fn": 4,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9939340169270834
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1567,
          "fn": 33,
          "accuracy": 0.979375
        },
        "0.01": null
      },
      "auroc": 0.98580361328125
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1593,
          "fn": 7,
          "accuracy": 0.995625
        },
        "0.01": null
      },
      "auroc": 0.9927458170572916
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3160,
          "fn": 40,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9892747151692709
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3163,
          "fn": 37,
          "accuracy": 0.9884375
        },
        "0.01": null
      },
      "auroc": 0.9898247965494791
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3193,
          "fn": 7,
          "accuracy": 0.9978125
        },
        "0.01": null
      },
      "auroc": 0.9933839355468749
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 6356,
          "fn": 44,
          "accuracy": 0.993125
        },
        "0.01": null
      },
      "auroc": 0.9916043660481771
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1596,
          "fn": 4,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9931047200520833
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9931655110677083
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3191,
          "fn": 9,
          "accuracy": 0.9971875
        },
        "0.01": null
      },
      "auroc": 0.9931351155598959
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1347,
          "fn": 253,
          "accuracy": 0.841875
        },
        "0.01": null
      },
      "auroc": 0.9506207194010416
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1555,
          "fn": 45,
          "accuracy": 0.971875
        },
        "0.01": null
      },
      "auroc": 0.9855817220052083
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2902,
          "fn": 298,
          "accuracy": 0.906875
        },
        "0.01": null
      },
      "auroc": 0.968101220703125
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2943,
          "fn": 257,
          "accuracy": 0.9196875
        },
        "0.01": null
      },
      "auroc": 0.9718627197265625
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3150,
          "fn": 50,
          "accuracy": 0.984375
        },
        "0.01": null
      },
      "auroc": 0.9893736165364584
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 6093,
          "fn": 307,
          "accuracy": 0.95203125
        },
        "0.01": null
      },
      "auroc": 0.9806181681315104
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940107259114583
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940298339843749
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940202799479166
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.993577001953125
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1579,
          "fn": 21,
          "accuracy": 0.986875
        },
        "0.01": null
      },
      "auroc": 0.99034951171875
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3176,
          "fn": 24,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9919632568359374
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.9937938639322916
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3179,
          "fn": 21,
          "accuracy": 0.9934375
        },
        "0.01": null
      },
      "auroc": 0.9921896728515625
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 6376,
          "fn": 24,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.9929917683919272
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1594,
          "fn": 6,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.9929012044270834
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1594,
          "fn": 6,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.9929012044270834
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1580,
          "fn": 20,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9903497070312499
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1580,
          "fn": 20,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9903497070312499
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3174,
          "fn": 26,
          "accuracy": 0.991875
        },
        "0.01": null
      },
      "auroc": 0.9916254557291667
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3174,
          "fn": 26,
          "accuracy": 0.991875
        },
        "0.01": null
      },
      "auroc": 0.9916254557291667
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1496,
          "fn": 104,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9764538899739583
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1496,
          "fn": 104,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9764538899739583
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1379,
          "fn": 221,
          "accuracy": 0.861875
        },
        "0.01": null
      },
      "auroc": 0.9552697916666666
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1379,
          "fn": 221,
          "accuracy": 0.861875
        },
        "0.01": null
      },
      "auroc": 0.9552697916666666
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2875,
          "fn": 325,
          "accuracy": 0.8984375
        },
        "0.01": null
      },
      "auroc": 0.9658618408203126
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2875,
          "fn": 325,
          "accuracy": 0.8984375
        },
        "0.01": null
      },
      "auroc": 0.9658618408203126
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9929601236979166
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9929601236979166
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9928636718750001
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9928636718750001
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3192,
          "fn": 8,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9929118977864584
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3192,
          "fn": 8,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9929118977864584
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9936555989583333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9936555989583333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.9938460286458333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.9938460286458333
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1581,
          "fn": 19,
          "accuracy": 0.988125
        },
        "0.01": null
      },
      "auroc": 0.9907084309895833
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1581,
          "fn": 19,
          "accuracy": 0.988125
        },
        "0.01": null
      },
      "auroc": 0.9907084309895833
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1546,
          "fn": 54,
          "accuracy": 0.96625
        },
        "0.01": null
      },
      "auroc": 0.9850528971354167
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1546,
          "fn": 54,
          "accuracy": 0.96625
        },
        "0.01": null
      },
      "auroc": 0.9850528971354167
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3127,
          "fn": 73,
          "accuracy": 0.9771875
        },
        "0.01": null
      },
      "auroc": 0.9878806640625
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3127,
          "fn": 73,
          "accuracy": 0.9771875
        },
        "0.01": null
      },
      "auroc": 0.9878806640625
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 17458,
          "fn": 142,
          "accuracy": 0.9919318181818182
        },
        "0.01": null
      },
      "auroc": 0.9917684555516099
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 9589,
          "fn": 11,
          "accuracy": 0.9988541666666667
        },
        "0.01": null
      },
      "auroc": 0.9937024142795139
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 27047,
          "fn": 153,
          "accuracy": 0.994375
        },
        "0.01": null
      },
      "auroc": 0.9924510292202818
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 16782,
          "fn": 818,
          "accuracy": 0.9535227272727272
        },
        "0.01": null
      },
      "auroc": 0.9811532344933712
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 9471,
          "fn": 129,
          "accuracy": 0.9865625
        },
        "0.01": null
      },
      "auroc": 0.9902996880425348
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 26253,
          "fn": 947,
          "accuracy": 0.9651838235294118
        },
        "0.01": null
      },
      "auroc": 0.9843813945695465
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 34240,
          "fn": 960,
          "accuracy": 0.9727272727272728
        },
        "0.01": null
      },
      "auroc": 0.9864608450224905
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 19060,
          "fn": 140,
          "accuracy": 0.9927083333333333
        },
        "0.01": null
      },
      "auroc": 0.9920010511610242
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 53300,
          "fn": 1100,
          "accuracy": 0.9797794117647058
        },
        "0.01": null
      },
      "auroc": 0.9884162118949141
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1599,
          "fn": 1,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9937738118489583
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3199,
          "fn": 1,
          "accuracy": 0.9996875
        },
        "0.01": null
      },
      "auroc": 0.9939051350911459
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3199,
          "fn": 1,
          "accuracy": 0.9996875
        },
        "0.01": null
      },
      "auroc": 0.9939051350911459
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 6399,
          "fn": 1,
          "accuracy": 0.99984375
        },
        "0.01": null
      },
      "auroc": 0.9939707967122395
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1598,
          "fn": 2,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.993389208984375
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1599,
          "fn": 1,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9934918782552082
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.9934405436197916
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1435,
          "fn": 165,
          "accuracy": 0.896875
        },
        "0.01": null
      },
      "auroc": 0.9651591145833334
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1585,
          "fn": 15,
          "accuracy": 0.990625
        },
        "0.01": null
      },
      "auroc": 0.9916305013020833
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3020,
          "fn": 180,
          "accuracy": 0.94375
        },
        "0.01": null
      },
      "auroc": 0.9783948079427083
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3033,
          "fn": 167,
          "accuracy": 0.9478125
        },
        "0.01": null
      },
      "auroc": 0.9792741617838541
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3184,
          "fn": 16,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9925611897786458
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 6217,
          "fn": 183,
          "accuracy": 0.97140625
        },
        "0.01": null
      },
      "auroc": 0.98591767578125
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940237467447917
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9936652018229166
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3195,
          "fn": 5,
          "accuracy": 0.9984375
        },
        "0.01": null
      },
      "auroc": 0.9938444742838541
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.99397783203125
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1562,
          "fn": 38,
          "accuracy": 0.97625
        },
        "0.01": null
      },
      "auroc": 0.9879128743489584
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3162,
          "fn": 38,
          "accuracy": 0.988125
        },
        "0.01": null
      },
      "auroc": 0.9909453531901042
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940007893880208
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3157,
          "fn": 43,
          "accuracy": 0.9865625
        },
        "0.01": null
      },
      "auroc": 0.9907890380859375
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 6357,
          "fn": 43,
          "accuracy": 0.99328125
        },
        "0.01": null
      },
      "auroc": 0.9923949137369791
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1598,
          "fn": 2,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9938483561197915
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940238606770834
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3198,
          "fn": 2,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9939361083984374
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1582,
          "fn": 18,
          "accuracy": 0.98875
        },
        "0.01": null
      },
      "auroc": 0.9881256184895835
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.993083935546875
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3177,
          "fn": 23,
          "accuracy": 0.9928125
        },
        "0.01": null
      },
      "auroc": 0.9906047770182291
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3180,
          "fn": 20,
          "accuracy": 0.99375
        },
        "0.01": null
      },
      "auroc": 0.9909869873046875
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3195,
          "fn": 5,
          "accuracy": 0.9984375
        },
        "0.01": null
      },
      "auroc": 0.9935538981119791
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 6375,
          "fn": 25,
          "accuracy": 0.99609375
        },
        "0.01": null
      },
      "auroc": 0.9922704427083333
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1596,
          "fn": 4,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9931620768229168
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9931920410156251
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3191,
          "fn": 9,
          "accuracy": 0.9971875
        },
        "0.01": null
      },
      "auroc": 0.9931770589192708
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1405,
          "fn": 195,
          "accuracy": 0.878125
        },
        "0.01": null
      },
      "auroc": 0.9571916829427083
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1570,
          "fn": 30,
          "accuracy": 0.98125
        },
        "0.01": null
      },
      "auroc": 0.98747822265625
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2975,
          "fn": 225,
          "accuracy": 0.9296875
        },
        "0.01": null
      },
      "auroc": 0.9723349527994791
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3001,
          "fn": 199,
          "accuracy": 0.9378125
        },
        "0.01": null
      },
      "auroc": 0.9751768798828124
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3165,
          "fn": 35,
          "accuracy": 0.9890625
        },
        "0.01": null
      },
      "auroc": 0.9903351318359375
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 6166,
          "fn": 234,
          "accuracy": 0.9634375
        },
        "0.01": null
      },
      "auroc": 0.982756005859375
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940172526041666
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.994028662109375
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940229573567708
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9936399739583333
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1579,
          "fn": 21,
          "accuracy": 0.986875
        },
        "0.01": null
      },
      "auroc": 0.9909782877604167
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3176,
          "fn": 24,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.992309130859375
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.9938286132812499
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3179,
          "fn": 21,
          "accuracy": 0.9934375
        },
        "0.01": null
      },
      "auroc": 0.9925034749348958
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 6376,
          "fn": 24,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.993166044108073
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9929770507812501
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9929770507812501
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1579,
          "fn": 21,
          "accuracy": 0.986875
        },
        "0.01": null
      },
      "auroc": 0.9905899088541666
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1579,
          "fn": 21,
          "accuracy": 0.986875
        },
        "0.01": null
      },
      "auroc": 0.9905899088541666
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3174,
          "fn": 26,
          "accuracy": 0.991875
        },
        "0.01": null
      },
      "auroc": 0.9917834798177083
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3174,
          "fn": 26,
          "accuracy": 0.991875
        },
        "0.01": null
      },
      "auroc": 0.9917834798177083
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1500,
          "fn": 100,
          "accuracy": 0.9375
        },
        "0.01": null
      },
      "auroc": 0.9778912597656251
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1500,
          "fn": 100,
          "accuracy": 0.9375
        },
        "0.01": null
      },
      "auroc": 0.9778912597656251
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1396,
          "fn": 204,
          "accuracy": 0.8725
        },
        "0.01": null
      },
      "auroc": 0.9582878255208334
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1396,
          "fn": 204,
          "accuracy": 0.8725
        },
        "0.01": null
      },
      "auroc": 0.9582878255208334
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2896,
          "fn": 304,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9680895426432292
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2896,
          "fn": 304,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9680895426432292
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9929609863281249
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9929609863281249
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9929561360677084
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9929561360677084
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3192,
          "fn": 8,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9929585611979166
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3192,
          "fn": 8,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9929585611979166
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9935452962239585
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9935452962239585
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3195,
          "fn": 5,
          "accuracy": 0.9984375
        },
        "0.01": null
      },
      "auroc": 0.9937908772786459
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3195,
          "fn": 5,
          "accuracy": 0.9984375
        },
        "0.01": null
      },
      "auroc": 0.9937908772786459
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1583,
          "fn": 17,
          "accuracy": 0.989375
        },
        "0.01": null
      },
      "auroc": 0.9910996419270833
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1583,
          "fn": 17,
          "accuracy": 0.989375
        },
        "0.01": null
      },
      "auroc": 0.9910996419270833
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1551,
          "fn": 49,
          "accuracy": 0.969375
        },
        "0.01": null
      },
      "auroc": 0.9854058919270834
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1551,
          "fn": 49,
          "accuracy": 0.969375
        },
        "0.01": null
      },
      "auroc": 0.9854058919270834
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3134,
          "fn": 66,
          "accuracy": 0.979375
        },
        "0.01": null
      },
      "auroc": 0.9882527669270833
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3134,
          "fn": 66,
          "accuracy": 0.979375
        },
        "0.01": null
      },
      "auroc": 0.9882527669270833
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 17467,
          "fn": 133,
          "accuracy": 0.9924431818181818
        },
        "0.01": null
      },
      "auroc": 0.9919493178858901
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 9589,
          "fn": 11,
          "accuracy": 0.9988541666666667
        },
        "0.01": null
      },
      "auroc": 0.993739683702257
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 27056,
          "fn": 144,
          "accuracy": 0.9947058823529412
        },
        "0.01": null
      },
      "auroc": 0.9925812117034313
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 16935,
          "fn": 665,
          "accuracy": 0.962215909090909
        },
        "0.01": null
      },
      "auroc": 0.9829923399029357
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 9490,
          "fn": 110,
          "accuracy": 0.9885416666666667
        },
        "0.01": null
      },
      "auroc": 0.990809605577257
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 26425,
          "fn": 775,
          "accuracy": 0.9715073529411765
        },
        "0.01": null
      },
      "auroc": 0.9857513748468139
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 34402,
          "fn": 798,
          "accuracy": 0.9773295454545454
        },
        "0.01": null
      },
      "auroc": 0.9874708288944128
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 19079,
          "fn": 121,
          "accuracy": 0.9936979166666666
        },
        "0.01": null
      },
      "auroc": 0.992274644639757
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 53481,
          "fn": 919,
          "accuracy": 0.9831066176470589
        },
        "0.01": null
      },
      "auroc": 0.9891662932751226
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.99401240234375
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1599,
          "fn": 1,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.993865234375
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3199,
          "fn": 1,
          "accuracy": 0.9996875
        },
        "0.01": null
      },
      "auroc": 0.993938818359375
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940316569010417
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9934259602864584
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.9937288085937499
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940220296223958
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3196,
          "fn": 4,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9936455973307292
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 6396,
          "fn": 4,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9938338134765625
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1598,
          "fn": 2,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.993191748046875
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9935359375
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3198,
          "fn": 2,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9933638427734375
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1405,
          "fn": 195,
          "accuracy": 0.878125
        },
        "0.01": null
      },
      "auroc": 0.9619304850260416
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1582,
          "fn": 18,
          "accuracy": 0.98875
        },
        "0.01": null
      },
      "auroc": 0.9916057942708334
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2987,
          "fn": 213,
          "accuracy": 0.9334375
        },
        "0.01": null
      },
      "auroc": 0.9767681396484376
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3003,
          "fn": 197,
          "accuracy": 0.9384375
        },
        "0.01": null
      },
      "auroc": 0.9775611165364584
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3182,
          "fn": 18,
          "accuracy": 0.994375
        },
        "0.01": null
      },
      "auroc": 0.9925708658854167
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 6185,
          "fn": 215,
          "accuracy": 0.96640625
        },
        "0.01": null
      },
      "auroc": 0.9850659912109374
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9939870117187499
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1590,
          "fn": 10,
          "accuracy": 0.99375
        },
        "0.01": null
      },
      "auroc": 0.9930559407552084
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3190,
          "fn": 10,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9935214762369791
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1598,
          "fn": 2,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9937511555989582
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1555,
          "fn": 45,
          "accuracy": 0.971875
        },
        "0.01": null
      },
      "auroc": 0.9876705078125001
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3153,
          "fn": 47,
          "accuracy": 0.9853125
        },
        "0.01": null
      },
      "auroc": 0.990710831705729
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3198,
          "fn": 2,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9938690836588542
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3145,
          "fn": 55,
          "accuracy": 0.9828125
        },
        "0.01": null
      },
      "auroc": 0.9903632242838541
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 6343,
          "fn": 57,
          "accuracy": 0.99109375
        },
        "0.01": null
      },
      "auroc": 0.9921161539713541
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1598,
          "fn": 2,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.993883984375
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1599,
          "fn": 1,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9938335937499999
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.9938587890625
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1556,
          "fn": 44,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9843516601562501
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1592,
          "fn": 8,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9926629231770833
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3148,
          "fn": 52,
          "accuracy": 0.98375
        },
        "0.01": null
      },
      "auroc": 0.9885072916666666
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3154,
          "fn": 46,
          "accuracy": 0.985625
        },
        "0.01": null
      },
      "auroc": 0.989117822265625
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3191,
          "fn": 9,
          "accuracy": 0.9971875
        },
        "0.01": null
      },
      "auroc": 0.9932482584635417
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 6345,
          "fn": 55,
          "accuracy": 0.99140625
        },
        "0.01": null
      },
      "auroc": 0.9911830403645833
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9931067708333333
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1586,
          "fn": 14,
          "accuracy": 0.99125
        },
        "0.01": null
      },
      "auroc": 0.9923829915364584
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3181,
          "fn": 19,
          "accuracy": 0.9940625
        },
        "0.01": null
      },
      "auroc": 0.992744881184896
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1372,
          "fn": 228,
          "accuracy": 0.8575
        },
        "0.01": null
      },
      "auroc": 0.9546818522135416
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1571,
          "fn": 29,
          "accuracy": 0.981875
        },
        "0.01": null
      },
      "auroc": 0.9888790201822917
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2943,
          "fn": 257,
          "accuracy": 0.9196875
        },
        "0.01": null
      },
      "auroc": 0.9717804361979165
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2967,
          "fn": 233,
          "accuracy": 0.9271875
        },
        "0.01": null
      },
      "auroc": 0.9738943115234374
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3157,
          "fn": 43,
          "accuracy": 0.9865625
        },
        "0.01": null
      },
      "auroc": 0.990631005859375
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 6124,
          "fn": 276,
          "accuracy": 0.956875
        },
        "0.01": null
      },
      "auroc": 0.9822626586914063
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9939463378906249
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940077799479167
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9939770589192708
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9934126302083334
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1575,
          "fn": 25,
          "accuracy": 0.984375
        },
        "0.01": null
      },
      "auroc": 0.989045068359375
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3172,
          "fn": 28,
          "accuracy": 0.99125
        },
        "0.01": null
      },
      "auroc": 0.9912288492838541
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.9936794840494791
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3175,
          "fn": 25,
          "accuracy": 0.9921875
        },
        "0.01": null
      },
      "auroc": 0.9915264241536459
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 6372,
          "fn": 28,
          "accuracy": 0.995625
        },
        "0.01": null
      },
      "auroc": 0.9926029541015625
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1585,
          "fn": 15,
          "accuracy": 0.990625
        },
        "0.01": null
      },
      "auroc": 0.9917955240885415
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1585,
          "fn": 15,
          "accuracy": 0.990625
        },
        "0.01": null
      },
      "auroc": 0.9917955240885415
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1565,
          "fn": 35,
          "accuracy": 0.978125
        },
        "0.01": null
      },
      "auroc": 0.9883115234375002
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1565,
          "fn": 35,
          "accuracy": 0.978125
        },
        "0.01": null
      },
      "auroc": 0.9883115234375002
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3150,
          "fn": 50,
          "accuracy": 0.984375
        },
        "0.01": null
      },
      "auroc": 0.9900535237630208
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3150,
          "fn": 50,
          "accuracy": 0.984375
        },
        "0.01": null
      },
      "auroc": 0.9900535237630208
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1404,
          "fn": 196,
          "accuracy": 0.8775
        },
        "0.01": null
      },
      "auroc": 0.9631184407552082
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1404,
          "fn": 196,
          "accuracy": 0.8775
        },
        "0.01": null
      },
      "auroc": 0.9631184407552082
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1274,
          "fn": 326,
          "accuracy": 0.79625
        },
        "0.01": null
      },
      "auroc": 0.9369394205729168
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1274,
          "fn": 326,
          "accuracy": 0.79625
        },
        "0.01": null
      },
      "auroc": 0.9369394205729168
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2678,
          "fn": 522,
          "accuracy": 0.836875
        },
        "0.01": null
      },
      "auroc": 0.9500289306640626
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2678,
          "fn": 522,
          "accuracy": 0.836875
        },
        "0.01": null
      },
      "auroc": 0.9500289306640626
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1596,
          "fn": 4,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9929764160156249
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1596,
          "fn": 4,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9929764160156249
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9927654459635417
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9927654459635417
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3191,
          "fn": 9,
          "accuracy": 0.9971875
        },
        "0.01": null
      },
      "auroc": 0.9928709309895833
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3191,
          "fn": 9,
          "accuracy": 0.9971875
        },
        "0.01": null
      },
      "auroc": 0.9928709309895833
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.994008984375
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.994008984375
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1591,
          "fn": 9,
          "accuracy": 0.994375
        },
        "0.01": null
      },
      "auroc": 0.9925256510416667
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1591,
          "fn": 9,
          "accuracy": 0.994375
        },
        "0.01": null
      },
      "auroc": 0.9925256510416667
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3191,
          "fn": 9,
          "accuracy": 0.9971875
        },
        "0.01": null
      },
      "auroc": 0.9932673177083333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3191,
          "fn": 9,
          "accuracy": 0.9971875
        },
        "0.01": null
      },
      "auroc": 0.9932673177083333
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1552,
          "fn": 48,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9865369466145832
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1552,
          "fn": 48,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9865369466145832
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1499,
          "fn": 101,
          "accuracy": 0.936875
        },
        "0.01": null
      },
      "auroc": 0.9765792480468751
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1499,
          "fn": 101,
          "accuracy": 0.936875
        },
        "0.01": null
      },
      "auroc": 0.9765792480468751
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3051,
          "fn": 149,
          "accuracy": 0.9534375
        },
        "0.01": null
      },
      "auroc": 0.9815580973307292
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3051,
          "fn": 149,
          "accuracy": 0.9534375
        },
        "0.01": null
      },
      "auroc": 0.9815580973307292
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 17328,
          "fn": 272,
          "accuracy": 0.9845454545454545
        },
        "0.01": null
      },
      "auroc": 0.9900513242779356
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 9574,
          "fn": 26,
          "accuracy": 0.9972916666666667
        },
        "0.01": null
      },
      "auroc": 0.9934469129774306
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 26902,
          "fn": 298,
          "accuracy": 0.9890441176470588
        },
        "0.01": null
      },
      "auroc": 0.9912497673483456
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 16652,
          "fn": 948,
          "accuracy": 0.9461363636363637
        },
        "0.01": null
      },
      "auroc": 0.9790255208333334
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 9472,
          "fn": 128,
          "accuracy": 0.9866666666666667
        },
        "0.01": null
      },
      "auroc": 0.9905482123480903
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 26124,
          "fn": 1076,
          "accuracy": 0.9604411764705882
        },
        "0.01": null
      },
      "auroc": 0.9830923531326594
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 33980,
          "fn": 1220,
          "accuracy": 0.9653409090909091
        },
        "0.01": null
      },
      "auroc": 0.9845384225556344
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 19046,
          "fn": 154,
          "accuracy": 0.9919791666666666
        },
        "0.01": null
      },
      "auroc": 0.9919975626627606
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 53026,
          "fn": 1374,
          "accuracy": 0.9747426470588235
        },
        "0.01": null
      },
      "auroc": 0.9871710602405024
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1598,
          "fn": 2,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.993735546875
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3198,
          "fn": 2,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9938860026041667
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3198,
          "fn": 2,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9938860026041667
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 6398,
          "fn": 2,
          "accuracy": 0.9996875
        },
        "0.01": null
      },
      "auroc": 0.99396123046875
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1598,
          "fn": 2,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.99333193359375
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1599,
          "fn": 1,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.993422265625
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.9933770996093749
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1360,
          "fn": 240,
          "accuracy": 0.85
        },
        "0.01": null
      },
      "auroc": 0.9548010091145833
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1582,
          "fn": 18,
          "accuracy": 0.98875
        },
        "0.01": null
      },
      "auroc": 0.9915602864583333
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2942,
          "fn": 258,
          "accuracy": 0.919375
        },
        "0.01": null
      },
      "auroc": 0.9731806477864583
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2958,
          "fn": 242,
          "accuracy": 0.924375
        },
        "0.01": null
      },
      "auroc": 0.9740664713541667
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3181,
          "fn": 19,
          "accuracy": 0.9940625
        },
        "0.01": null
      },
      "auroc": 0.9924912760416666
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 6139,
          "fn": 261,
          "accuracy": 0.95921875
        },
        "0.01": null
      },
      "auroc": 0.9832788736979167
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940237467447917
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9935370442708333
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3195,
          "fn": 5,
          "accuracy": 0.9984375
        },
        "0.01": null
      },
      "auroc": 0.9937803955078126
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9939834635416667
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1564,
          "fn": 36,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9874034342447917
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3164,
          "fn": 36,
          "accuracy": 0.98875
        },
        "0.01": null
      },
      "auroc": 0.9906934488932291
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.994003605143229
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3159,
          "fn": 41,
          "accuracy": 0.9871875
        },
        "0.01": null
      },
      "auroc": 0.9904702392578124
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 6359,
          "fn": 41,
          "accuracy": 0.99359375
        },
        "0.01": null
      },
      "auroc": 0.9922369222005207
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1596,
          "fn": 4,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.993832275390625
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940172526041666
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3196,
          "fn": 4,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9939247639973958
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1563,
          "fn": 37,
          "accuracy": 0.976875
        },
        "0.01": null
      },
      "auroc": 0.9854303548177082
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1593,
          "fn": 7,
          "accuracy": 0.995625
        },
        "0.01": null
      },
      "auroc": 0.9927002929687498
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3156,
          "fn": 44,
          "accuracy": 0.98625
        },
        "0.01": null
      },
      "auroc": 0.9890653238932292
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3159,
          "fn": 41,
          "accuracy": 0.9871875
        },
        "0.01": null
      },
      "auroc": 0.9896313151041666
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3193,
          "fn": 7,
          "accuracy": 0.9978125
        },
        "0.01": null
      },
      "auroc": 0.9933587727864582
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 6352,
          "fn": 48,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9914950439453124
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1596,
          "fn": 4,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.993044189453125
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1596,
          "fn": 4,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9931237630208334
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3192,
          "fn": 8,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9930839762369793
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1346,
          "fn": 254,
          "accuracy": 0.84125
        },
        "0.01": null
      },
      "auroc": 0.94881904296875
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1555,
          "fn": 45,
          "accuracy": 0.971875
        },
        "0.01": null
      },
      "auroc": 0.9854281575520834
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2901,
          "fn": 299,
          "accuracy": 0.9065625
        },
        "0.01": null
      },
      "auroc": 0.9671236002604168
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2942,
          "fn": 258,
          "accuracy": 0.919375
        },
        "0.01": null
      },
      "auroc": 0.9709316162109376
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3151,
          "fn": 49,
          "accuracy": 0.9846875
        },
        "0.01": null
      },
      "auroc": 0.9892759602864585
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 6093,
          "fn": 307,
          "accuracy": 0.95203125
        },
        "0.01": null
      },
      "auroc": 0.9801037882486979
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940107259114583
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940298339843749
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940202799479166
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.993586865234375
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1578,
          "fn": 22,
          "accuracy": 0.98625
        },
        "0.01": null
      },
      "auroc": 0.9900852050781249
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3175,
          "fn": 25,
          "accuracy": 0.9921875
        },
        "0.01": null
      },
      "auroc": 0.9918360351562501
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.9937987955729166
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3178,
          "fn": 22,
          "accuracy": 0.993125
        },
        "0.01": null
      },
      "auroc": 0.99205751953125
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 6375,
          "fn": 25,
          "accuracy": 0.99609375
        },
        "0.01": null
      },
      "auroc": 0.9929281575520833
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1594,
          "fn": 6,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.992849658203125
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1594,
          "fn": 6,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.992849658203125
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1579,
          "fn": 21,
          "accuracy": 0.986875
        },
        "0.01": null
      },
      "auroc": 0.9900598470052083
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1579,
          "fn": 21,
          "accuracy": 0.986875
        },
        "0.01": null
      },
      "auroc": 0.9900598470052083
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3173,
          "fn": 27,
          "accuracy": 0.9915625
        },
        "0.01": null
      },
      "auroc": 0.9914547526041667
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3173,
          "fn": 27,
          "accuracy": 0.9915625
        },
        "0.01": null
      },
      "auroc": 0.9914547526041667
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1494,
          "fn": 106,
          "accuracy": 0.93375
        },
        "0.01": null
      },
      "auroc": 0.9759261881510417
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1494,
          "fn": 106,
          "accuracy": 0.93375
        },
        "0.01": null
      },
      "auroc": 0.9759261881510417
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1380,
          "fn": 220,
          "accuracy": 0.8625
        },
        "0.01": null
      },
      "auroc": 0.9544595052083334
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1380,
          "fn": 220,
          "accuracy": 0.8625
        },
        "0.01": null
      },
      "auroc": 0.9544595052083334
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2874,
          "fn": 326,
          "accuracy": 0.898125
        },
        "0.01": null
      },
      "auroc": 0.9651928466796875
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2874,
          "fn": 326,
          "accuracy": 0.898125
        },
        "0.01": null
      },
      "auroc": 0.9651928466796875
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9929159016927083
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9929159016927083
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9928631998697917
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9928631998697917
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3192,
          "fn": 8,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9928895507812501
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3192,
          "fn": 8,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9928895507812501
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9935038736979167
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9935038736979167
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.993770166015625
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.993770166015625
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1580,
          "fn": 20,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9907068847656251
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1580,
          "fn": 20,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9907068847656251
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1548,
          "fn": 52,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9849154296875
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1548,
          "fn": 52,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9849154296875
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3128,
          "fn": 72,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9878111572265624
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3128,
          "fn": 72,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9878111572265624
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 17455,
          "fn": 145,
          "accuracy": 0.9917613636363637
        },
        "0.01": null
      },
      "auroc": 0.9917013109611743
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 9590,
          "fn": 10,
          "accuracy": 0.9989583333333333
        },
        "0.01": null
      },
      "auroc": 0.9936944363064235
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 27045,
          "fn": 155,
          "accuracy": 0.9943014705882353
        },
        "0.01": null
      },
      "auroc": 0.9924047669653799
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 16765,
          "fn": 835,
          "accuracy": 0.9525568181818181
        },
        "0.01": null
      },
      "auroc": 0.9805871863162878
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 9470,
          "fn": 130,
          "accuracy": 0.9864583333333333
        },
        "0.01": null
      },
      "auroc": 0.9901521538628473
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 26235,
          "fn": 965,
          "accuracy": 0.9645220588235294
        },
        "0.01": null
      },
      "auroc": 0.9839630572150737
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 34220,
          "fn": 980,
          "accuracy": 0.9721590909090909
        },
        "0.01": null
      },
      "auroc": 0.9861442486387312
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 19060,
          "fn": 140,
          "accuracy": 0.9927083333333333
        },
        "0.01": null
      },
      "auroc": 0.9919232950846354
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 53280,
          "fn": 1120,
          "accuracy": 0.9794117647058823
        },
        "0.01": null
      },
      "auroc": 0.9881839120902267
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1596,
          "fn": 4,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9926859375
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1575,
          "fn": 25,
          "accuracy": 0.984375
        },
        "0.01": null
      },
      "auroc": 0.9901856282552084
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3171,
          "fn": 29,
          "accuracy": 0.9909375
        },
        "0.01": null
      },
      "auroc": 0.9914357828776043
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1589,
          "fn": 11,
          "accuracy": 0.993125
        },
        "0.01": null
      },
      "auroc": 0.9922458333333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1556,
          "fn": 44,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9872301757812499
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3145,
          "fn": 55,
          "accuracy": 0.9828125
        },
        "0.01": null
      },
      "auroc": 0.9897380045572918
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3185,
          "fn": 15,
          "accuracy": 0.9953125
        },
        "0.01": null
      },
      "auroc": 0.9924658854166666
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3131,
          "fn": 69,
          "accuracy": 0.9784375
        },
        "0.01": null
      },
      "auroc": 0.9887079020182291
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 6316,
          "fn": 84,
          "accuracy": 0.986875
        },
        "0.01": null
      },
      "auroc": 0.990586893717448
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1572,
          "fn": 28,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9891916015625001
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1494,
          "fn": 106,
          "accuracy": 0.93375
        },
        "0.01": null
      },
      "auroc": 0.9748978352864583
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3066,
          "fn": 134,
          "accuracy": 0.958125
        },
        "0.01": null
      },
      "auroc": 0.9820447184244792
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1176,
          "fn": 424,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9271468912760417
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1437,
          "fn": 163,
          "accuracy": 0.898125
        },
        "0.01": null
      },
      "auroc": 0.9655763509114583
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2613,
          "fn": 587,
          "accuracy": 0.8165625
        },
        "0.01": null
      },
      "auroc": 0.94636162109375
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2748,
          "fn": 452,
          "accuracy": 0.85875
        },
        "0.01": null
      },
      "auroc": 0.9581692464192708
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2931,
          "fn": 269,
          "accuracy": 0.9159375
        },
        "0.01": null
      },
      "auroc": 0.9702370930989583
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 5679,
          "fn": 721,
          "accuracy": 0.88734375
        },
        "0.01": null
      },
      "auroc": 0.9642031697591147
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1587,
          "fn": 13,
          "accuracy": 0.991875
        },
        "0.01": null
      },
      "auroc": 0.9913500162760418
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1533,
          "fn": 67,
          "accuracy": 0.958125
        },
        "0.01": null
      },
      "auroc": 0.9810099772135418
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3120,
          "fn": 80,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9861799967447917
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1573,
          "fn": 27,
          "accuracy": 0.983125
        },
        "0.01": null
      },
      "auroc": 0.9896061686197917
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1387,
          "fn": 213,
          "accuracy": 0.866875
        },
        "0.01": null
      },
      "auroc": 0.9625864420572916
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2960,
          "fn": 240,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9760963053385417
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3160,
          "fn": 40,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9904780924479166
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2920,
          "fn": 280,
          "accuracy": 0.9125
        },
        "0.01": null
      },
      "auroc": 0.9717982096354167
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 6080,
          "fn": 320,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9811381510416667
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1570,
          "fn": 30,
          "accuracy": 0.98125
        },
        "0.01": null
      },
      "auroc": 0.9901476888020834
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1590,
          "fn": 10,
          "accuracy": 0.99375
        },
        "0.01": null
      },
      "auroc": 0.9902557128906251
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3160,
          "fn": 40,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9902017008463542
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1462,
          "fn": 138,
          "accuracy": 0.91375
        },
        "0.01": null
      },
      "auroc": 0.9697624348958334
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1474,
          "fn": 126,
          "accuracy": 0.92125
        },
        "0.01": null
      },
      "auroc": 0.9691738606770833
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2936,
          "fn": 264,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9694681477864584
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3032,
          "fn": 168,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9799550618489583
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3064,
          "fn": 136,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9797147867838543
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 6096,
          "fn": 304,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.9798349243164063
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1565,
          "fn": 35,
          "accuracy": 0.978125
        },
        "0.01": null
      },
      "auroc": 0.9885763997395833
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1476,
          "fn": 124,
          "accuracy": 0.9225
        },
        "0.01": null
      },
      "auroc": 0.9714621744791667
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3041,
          "fn": 159,
          "accuracy": 0.9503125
        },
        "0.01": null
      },
      "auroc": 0.980019287109375
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1090,
          "fn": 510,
          "accuracy": 0.68125
        },
        "0.01": null
      },
      "auroc": 0.9126616210937499
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1183,
          "fn": 417,
          "accuracy": 0.739375
        },
        "0.01": null
      },
      "auroc": 0.9211723958333333
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2273,
          "fn": 927,
          "accuracy": 0.7103125
        },
        "0.01": null
      },
      "auroc": 0.9169170084635416
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2655,
          "fn": 545,
          "accuracy": 0.8296875
        },
        "0.01": null
      },
      "auroc": 0.9506190104166666
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2659,
          "fn": 541,
          "accuracy": 0.8309375
        },
        "0.01": null
      },
      "auroc": 0.94631728515625
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 5314,
          "fn": 1086,
          "accuracy": 0.8303125
        },
        "0.01": null
      },
      "auroc": 0.9484681477864584
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1598,
          "fn": 2,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.99281513671875
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1581,
          "fn": 19,
          "accuracy": 0.988125
        },
        "0.01": null
      },
      "auroc": 0.990604052734375
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3179,
          "fn": 21,
          "accuracy": 0.9934375
        },
        "0.01": null
      },
      "auroc": 0.9917095947265625
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1580,
          "fn": 20,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9898663899739585
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1501,
          "fn": 99,
          "accuracy": 0.938125
        },
        "0.01": null
      },
      "auroc": 0.97743115234375
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3081,
          "fn": 119,
          "accuracy": 0.9628125
        },
        "0.01": null
      },
      "auroc": 0.9836487711588542
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3178,
          "fn": 22,
          "accuracy": 0.993125
        },
        "0.01": null
      },
      "auroc": 0.9913407633463541
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3082,
          "fn": 118,
          "accuracy": 0.963125
        },
        "0.01": null
      },
      "auroc": 0.9840176025390626
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 6260,
          "fn": 140,
          "accuracy": 0.978125
        },
        "0.01": null
      },
      "auroc": 0.9876791829427083
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1563,
          "fn": 37,
          "accuracy": 0.976875
        },
        "0.01": null
      },
      "auroc": 0.9868365071614583
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1563,
          "fn": 37,
          "accuracy": 0.976875
        },
        "0.01": null
      },
      "auroc": 0.9868365071614583
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1534,
          "fn": 66,
          "accuracy": 0.95875
        },
        "0.01": null
      },
      "auroc": 0.9818425944010418
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1534,
          "fn": 66,
          "accuracy": 0.95875
        },
        "0.01": null
      },
      "auroc": 0.9818425944010418
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3097,
          "fn": 103,
          "accuracy": 0.9678125
        },
        "0.01": null
      },
      "auroc": 0.98433955078125
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3097,
          "fn": 103,
          "accuracy": 0.9678125
        },
        "0.01": null
      },
      "auroc": 0.98433955078125
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1274,
          "fn": 326,
          "accuracy": 0.79625
        },
        "0.01": null
      },
      "auroc": 0.9420913248697917
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1274,
          "fn": 326,
          "accuracy": 0.79625
        },
        "0.01": null
      },
      "auroc": 0.9420913248697917
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1132,
          "fn": 468,
          "accuracy": 0.7075
        },
        "0.01": null
      },
      "auroc": 0.9148390950520833
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1132,
          "fn": 468,
          "accuracy": 0.7075
        },
        "0.01": null
      },
      "auroc": 0.9148390950520833
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2406,
          "fn": 794,
          "accuracy": 0.751875
        },
        "0.01": null
      },
      "auroc": 0.9284652099609374
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2406,
          "fn": 794,
          "accuracy": 0.751875
        },
        "0.01": null
      },
      "auroc": 0.9284652099609374
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1588,
          "fn": 12,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.989954443359375
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1588,
          "fn": 12,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.989954443359375
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1582,
          "fn": 18,
          "accuracy": 0.98875
        },
        "0.01": null
      },
      "auroc": 0.9906189453124999
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1582,
          "fn": 18,
          "accuracy": 0.98875
        },
        "0.01": null
      },
      "auroc": 0.9906189453124999
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3170,
          "fn": 30,
          "accuracy": 0.990625
        },
        "0.01": null
      },
      "auroc": 0.9902866943359375
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3170,
          "fn": 30,
          "accuracy": 0.990625
        },
        "0.01": null
      },
      "auroc": 0.9902866943359375
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1592,
          "fn": 8,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9896166666666667
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1592,
          "fn": 8,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9896166666666667
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1542,
          "fn": 58,
          "accuracy": 0.96375
        },
        "0.01": null
      },
      "auroc": 0.981846142578125
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1542,
          "fn": 58,
          "accuracy": 0.96375
        },
        "0.01": null
      },
      "auroc": 0.981846142578125
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3134,
          "fn": 66,
          "accuracy": 0.979375
        },
        "0.01": null
      },
      "auroc": 0.985731404622396
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3134,
          "fn": 66,
          "accuracy": 0.979375
        },
        "0.01": null
      },
      "auroc": 0.985731404622396
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1508,
          "fn": 92,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.977616748046875
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1508,
          "fn": 92,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.977616748046875
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1423,
          "fn": 177,
          "accuracy": 0.889375
        },
        "0.01": null
      },
      "auroc": 0.9637697265624999
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1423,
          "fn": 177,
          "accuracy": 0.889375
        },
        "0.01": null
      },
      "auroc": 0.9637697265624999
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2931,
          "fn": 269,
          "accuracy": 0.9159375
        },
        "0.01": null
      },
      "auroc": 0.9706932373046875
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2931,
          "fn": 269,
          "accuracy": 0.9159375
        },
        "0.01": null
      },
      "auroc": 0.9706932373046875
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 17013,
          "fn": 587,
          "accuracy": 0.9666477272727273
        },
        "0.01": null
      },
      "auroc": 0.9846256791548297
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 9249,
          "fn": 351,
          "accuracy": 0.9634375
        },
        "0.01": null
      },
      "auroc": 0.9830692301432291
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 26262,
          "fn": 938,
          "accuracy": 0.9655147058823529
        },
        "0.01": null
      },
      "auroc": 0.9840763442095588
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 15683,
          "fn": 1917,
          "accuracy": 0.8910795454545455
        },
        "0.01": null
      },
      "auroc": 0.9649278039180871
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 8538,
          "fn": 1062,
          "accuracy": 0.889375
        },
        "0.01": null
      },
      "auroc": 0.9638617296006944
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 24221,
          "fn": 2979,
          "accuracy": 0.8904779411764706
        },
        "0.01": null
      },
      "auroc": 0.9645515423943014
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 32696,
          "fn": 2504,
          "accuracy": 0.9288636363636363
        },
        "0.01": null
      },
      "auroc": 0.9747767415364582
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 17787,
          "fn": 1413,
          "accuracy": 0.92640625
        },
        "0.01": null
      },
      "auroc": 0.9734654798719617
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 50483,
          "fn": 3917,
          "accuracy": 0.9279963235294117
        },
        "0.01": null
      },
      "auroc": 0.9743139433019301
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1599,
          "fn": 1,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9937882161458333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3199,
          "fn": 1,
          "accuracy": 0.9996875
        },
        "0.01": null
      },
      "auroc": 0.9939123372395833
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3199,
          "fn": 1,
          "accuracy": 0.9996875
        },
        "0.01": null
      },
      "auroc": 0.9939123372395833
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 6399,
          "fn": 1,
          "accuracy": 0.99984375
        },
        "0.01": null
      },
      "auroc": 0.9939743977864584
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1598,
          "fn": 2,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9932662923177085
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1599,
          "fn": 1,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9933397460937501
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.9933030192057292
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1380,
          "fn": 220,
          "accuracy": 0.8625
        },
        "0.01": null
      },
      "auroc": 0.959441259765625
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1585,
          "fn": 15,
          "accuracy": 0.990625
        },
        "0.01": null
      },
      "auroc": 0.991579638671875
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2965,
          "fn": 235,
          "accuracy": 0.9265625
        },
        "0.01": null
      },
      "auroc": 0.9755104492187501
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2978,
          "fn": 222,
          "accuracy": 0.930625
        },
        "0.01": null
      },
      "auroc": 0.9763537760416667
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3184,
          "fn": 16,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9924596923828126
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 6162,
          "fn": 238,
          "accuracy": 0.9628125
        },
        "0.01": null
      },
      "auroc": 0.9844067342122396
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940237467447917
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9935659016927083
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3195,
          "fn": 5,
          "accuracy": 0.9984375
        },
        "0.01": null
      },
      "auroc": 0.99379482421875
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9939861816406249
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1565,
          "fn": 35,
          "accuracy": 0.978125
        },
        "0.01": null
      },
      "auroc": 0.987943359375
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3165,
          "fn": 35,
          "accuracy": 0.9890625
        },
        "0.01": null
      },
      "auroc": 0.9909647705078126
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940049641927082
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3160,
          "fn": 40,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9907546305338542
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 6360,
          "fn": 40,
          "accuracy": 0.99375
        },
        "0.01": null
      },
      "auroc": 0.9923797973632811
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1596,
          "fn": 4,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9938059733072917
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940136067708334
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3196,
          "fn": 4,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9939097900390625
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1574,
          "fn": 26,
          "accuracy": 0.98375
        },
        "0.01": null
      },
      "auroc": 0.9863191243489583
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1593,
          "fn": 7,
          "accuracy": 0.995625
        },
        "0.01": null
      },
      "auroc": 0.9926582356770832
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3167,
          "fn": 33,
          "accuracy": 0.9896875
        },
        "0.01": null
      },
      "auroc": 0.9894886800130209
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3170,
          "fn": 30,
          "accuracy": 0.990625
        },
        "0.01": null
      },
      "auroc": 0.9900625488281249
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3193,
          "fn": 7,
          "accuracy": 0.9978125
        },
        "0.01": null
      },
      "auroc": 0.9933359212239583
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 6363,
          "fn": 37,
          "accuracy": 0.99421875
        },
        "0.01": null
      },
      "auroc": 0.9916992350260418
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1596,
          "fn": 4,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9931217610677083
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9931886555989584
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3191,
          "fn": 9,
          "accuracy": 0.9971875
        },
        "0.01": null
      },
      "auroc": 0.9931552083333333
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1361,
          "fn": 239,
          "accuracy": 0.850625
        },
        "0.01": null
      },
      "auroc": 0.9526409505208333
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1559,
          "fn": 41,
          "accuracy": 0.974375
        },
        "0.01": null
      },
      "auroc": 0.9856595214843751
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2920,
          "fn": 280,
          "accuracy": 0.9125
        },
        "0.01": null
      },
      "auroc": 0.9691502360026042
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2957,
          "fn": 243,
          "accuracy": 0.9240625
        },
        "0.01": null
      },
      "auroc": 0.9728813557942708
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3154,
          "fn": 46,
          "accuracy": 0.985625
        },
        "0.01": null
      },
      "auroc": 0.9894240885416666
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 6111,
          "fn": 289,
          "accuracy": 0.95484375
        },
        "0.01": null
      },
      "auroc": 0.9811527221679687
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940107259114583
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940298339843749
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940202799479166
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1598,
          "fn": 2,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.993636767578125
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1581,
          "fn": 19,
          "accuracy": 0.988125
        },
        "0.01": null
      },
      "auroc": 0.990861181640625
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3179,
          "fn": 21,
          "accuracy": 0.9934375
        },
        "0.01": null
      },
      "auroc": 0.992248974609375
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3198,
          "fn": 2,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9938237467447917
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3181,
          "fn": 19,
          "accuracy": 0.9940625
        },
        "0.01": null
      },
      "auroc": 0.9924455078125
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 6379,
          "fn": 21,
          "accuracy": 0.99671875
        },
        "0.01": null
      },
      "auroc": 0.9931346272786459
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9929207356770834
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9929207356770834
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1580,
          "fn": 20,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9905366861979166
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1580,
          "fn": 20,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9905366861979166
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3175,
          "fn": 25,
          "accuracy": 0.9921875
        },
        "0.01": null
      },
      "auroc": 0.9917287109375
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3175,
          "fn": 25,
          "accuracy": 0.9921875
        },
        "0.01": null
      },
      "auroc": 0.9917287109375
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1498,
          "fn": 102,
          "accuracy": 0.93625
        },
        "0.01": null
      },
      "auroc": 0.97700390625
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1498,
          "fn": 102,
          "accuracy": 0.93625
        },
        "0.01": null
      },
      "auroc": 0.97700390625
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1380,
          "fn": 220,
          "accuracy": 0.8625
        },
        "0.01": null
      },
      "auroc": 0.9558011881510416
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1380,
          "fn": 220,
          "accuracy": 0.8625
        },
        "0.01": null
      },
      "auroc": 0.9558011881510416
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2878,
          "fn": 322,
          "accuracy": 0.899375
        },
        "0.01": null
      },
      "auroc": 0.9664025472005207
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2878,
          "fn": 322,
          "accuracy": 0.899375
        },
        "0.01": null
      },
      "auroc": 0.9664025472005207
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9929601236979166
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9929601236979166
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9928636718750001
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9928636718750001
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3192,
          "fn": 8,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9929118977864584
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3192,
          "fn": 8,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9929118977864584
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1598,
          "fn": 2,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.993727978515625
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1598,
          "fn": 2,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.993727978515625
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3198,
          "fn": 2,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9938822184244791
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3198,
          "fn": 2,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9938822184244791
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1582,
          "fn": 18,
          "accuracy": 0.98875
        },
        "0.01": null
      },
      "auroc": 0.9905697102864585
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1582,
          "fn": 18,
          "accuracy": 0.98875
        },
        "0.01": null
      },
      "auroc": 0.9905697102864585
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1549,
          "fn": 51,
          "accuracy": 0.968125
        },
        "0.01": null
      },
      "auroc": 0.985280078125
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1549,
          "fn": 51,
          "accuracy": 0.968125
        },
        "0.01": null
      },
      "auroc": 0.985280078125
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3131,
          "fn": 69,
          "accuracy": 0.9784375
        },
        "0.01": null
      },
      "auroc": 0.9879248942057293
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3131,
          "fn": 69,
          "accuracy": 0.9784375
        },
        "0.01": null
      },
      "auroc": 0.9879248942057293
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 17462,
          "fn": 138,
          "accuracy": 0.9921590909090909
        },
        "0.01": null
      },
      "auroc": 0.9917959901751894
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 9589,
          "fn": 11,
          "accuracy": 0.9988541666666667
        },
        "0.01": null
      },
      "auroc": 0.9936957004123264
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 27051,
          "fn": 149,
          "accuracy": 0.9945220588235294
        },
        "0.01": null
      },
      "auroc": 0.9924664761412377
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 16815,
          "fn": 785,
          "accuracy": 0.9553977272727273
        },
        "0.01": null
      },
      "auroc": 0.9816609404592802
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 9482,
          "fn": 118,
          "accuracy": 0.9877083333333333
        },
        "0.01": null
      },
      "auroc": 0.9904150254991319
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 26297,
          "fn": 903,
          "accuracy": 0.9668014705882353
        },
        "0.01": null
      },
      "auroc": 0.984750617532169
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 34277,
          "fn": 923,
          "accuracy": 0.9737784090909091
        },
        "0.01": null
      },
      "auroc": 0.9867284653172348
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 19071,
          "fn": 129,
          "accuracy": 0.99328125
        },
        "0.01": null
      },
      "auroc": 0.992055362955729
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 53348,
          "fn": 1052,
          "accuracy": 0.9806617647058824
        },
        "0.01": null
      },
      "auroc": 0.9886085468367034
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1598,
          "fn": 2,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9937374348958333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3198,
          "fn": 2,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9938869466145834
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3198,
          "fn": 2,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9938869466145834
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 6398,
          "fn": 2,
          "accuracy": 0.9996875
        },
        "0.01": null
      },
      "auroc": 0.9939617024739584
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1598,
          "fn": 2,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9933709960937501
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1599,
          "fn": 1,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9934166341145834
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.9933938151041667
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1373,
          "fn": 227,
          "accuracy": 0.858125
        },
        "0.01": null
      },
      "auroc": 0.9575076985677083
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1584,
          "fn": 16,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9916464192708333
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2957,
          "fn": 243,
          "accuracy": 0.9240625
        },
        "0.01": null
      },
      "auroc": 0.9745770589192708
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2971,
          "fn": 229,
          "accuracy": 0.9284375
        },
        "0.01": null
      },
      "auroc": 0.9754393473307291
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3183,
          "fn": 17,
          "accuracy": 0.9946875
        },
        "0.01": null
      },
      "auroc": 0.9925315266927083
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 6154,
          "fn": 246,
          "accuracy": 0.9615625
        },
        "0.01": null
      },
      "auroc": 0.9839854370117186
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940237467447917
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9935367187500002
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3195,
          "fn": 5,
          "accuracy": 0.9984375
        },
        "0.01": null
      },
      "auroc": 0.9937802327473959
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9939813802083333
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1561,
          "fn": 39,
          "accuracy": 0.975625
        },
        "0.01": null
      },
      "auroc": 0.9876256835937499
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3161,
          "fn": 39,
          "accuracy": 0.9878125
        },
        "0.01": null
      },
      "auroc": 0.9908035319010416
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940025634765625
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3156,
          "fn": 44,
          "accuracy": 0.98625
        },
        "0.01": null
      },
      "auroc": 0.9905812011718749
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 6356,
          "fn": 44,
          "accuracy": 0.993125
        },
        "0.01": null
      },
      "auroc": 0.9922918823242187
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1596,
          "fn": 4,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9938463541666667
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940220540364584
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3196,
          "fn": 4,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9939342041015625
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1567,
          "fn": 33,
          "accuracy": 0.979375
        },
        "0.01": null
      },
      "auroc": 0.98580361328125
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1593,
          "fn": 7,
          "accuracy": 0.995625
        },
        "0.01": null
      },
      "auroc": 0.9927476399739583
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3160,
          "fn": 40,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9892756266276043
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3163,
          "fn": 37,
          "accuracy": 0.9884375
        },
        "0.01": null
      },
      "auroc": 0.9898249837239583
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3193,
          "fn": 7,
          "accuracy": 0.9978125
        },
        "0.01": null
      },
      "auroc": 0.9933848470052082
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 6356,
          "fn": 44,
          "accuracy": 0.993125
        },
        "0.01": null
      },
      "auroc": 0.9916049153645834
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1596,
          "fn": 4,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.99310390625
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9931655110677083
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3191,
          "fn": 9,
          "accuracy": 0.9971875
        },
        "0.01": null
      },
      "auroc": 0.9931347086588541
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1346,
          "fn": 254,
          "accuracy": 0.84125
        },
        "0.01": null
      },
      "auroc": 0.9506151529947916
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1555,
          "fn": 45,
          "accuracy": 0.971875
        },
        "0.01": null
      },
      "auroc": 0.985575732421875
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2901,
          "fn": 299,
          "accuracy": 0.9065625
        },
        "0.01": null
      },
      "auroc": 0.9680954427083333
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2942,
          "fn": 258,
          "accuracy": 0.919375
        },
        "0.01": null
      },
      "auroc": 0.971859529622396
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3150,
          "fn": 50,
          "accuracy": 0.984375
        },
        "0.01": null
      },
      "auroc": 0.9893706217447917
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 6092,
          "fn": 308,
          "accuracy": 0.951875
        },
        "0.01": null
      },
      "auroc": 0.9806150756835937
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940107259114583
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940298339843749
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940202799479166
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.993577001953125
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1579,
          "fn": 21,
          "accuracy": 0.986875
        },
        "0.01": null
      },
      "auroc": 0.99034951171875
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3176,
          "fn": 24,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9919632568359374
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.9937938639322916
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3179,
          "fn": 21,
          "accuracy": 0.9934375
        },
        "0.01": null
      },
      "auroc": 0.9921896728515625
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 6376,
          "fn": 24,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.9929917683919272
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1594,
          "fn": 6,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.9929012044270834
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1594,
          "fn": 6,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.9929012044270834
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1580,
          "fn": 20,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9903497070312499
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1580,
          "fn": 20,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9903497070312499
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3174,
          "fn": 26,
          "accuracy": 0.991875
        },
        "0.01": null
      },
      "auroc": 0.9916254557291667
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3174,
          "fn": 26,
          "accuracy": 0.991875
        },
        "0.01": null
      },
      "auroc": 0.9916254557291667
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1496,
          "fn": 104,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.976461083984375
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1496,
          "fn": 104,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.976461083984375
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1379,
          "fn": 221,
          "accuracy": 0.861875
        },
        "0.01": null
      },
      "auroc": 0.9552697916666666
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1379,
          "fn": 221,
          "accuracy": 0.861875
        },
        "0.01": null
      },
      "auroc": 0.9552697916666666
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2875,
          "fn": 325,
          "accuracy": 0.8984375
        },
        "0.01": null
      },
      "auroc": 0.9658654378255208
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2875,
          "fn": 325,
          "accuracy": 0.8984375
        },
        "0.01": null
      },
      "auroc": 0.9658654378255208
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9929601236979166
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9929601236979166
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9928636718750001
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9928636718750001
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3192,
          "fn": 8,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9929118977864584
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3192,
          "fn": 8,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9929118977864584
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9936555989583333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9936555989583333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.9938460286458333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.9938460286458333
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1581,
          "fn": 19,
          "accuracy": 0.988125
        },
        "0.01": null
      },
      "auroc": 0.9907086751302083
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1581,
          "fn": 19,
          "accuracy": 0.988125
        },
        "0.01": null
      },
      "auroc": 0.9907086751302083
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1546,
          "fn": 54,
          "accuracy": 0.96625
        },
        "0.01": null
      },
      "auroc": 0.9850169759114584
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1546,
          "fn": 54,
          "accuracy": 0.96625
        },
        "0.01": null
      },
      "auroc": 0.9850169759114584
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3127,
          "fn": 73,
          "accuracy": 0.9771875
        },
        "0.01": null
      },
      "auroc": 0.9878628255208333
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3127,
          "fn": 73,
          "accuracy": 0.9771875
        },
        "0.01": null
      },
      "auroc": 0.9878628255208333
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 17458,
          "fn": 142,
          "accuracy": 0.9919318181818182
        },
        "0.01": null
      },
      "auroc": 0.9917690666429925
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 9589,
          "fn": 11,
          "accuracy": 0.9988541666666667
        },
        "0.01": null
      },
      "auroc": 0.9937012017144098
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 27047,
          "fn": 153,
          "accuracy": 0.994375
        },
        "0.01": null
      },
      "auroc": 0.9924509966681985
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 16780,
          "fn": 820,
          "accuracy": 0.9534090909090909
        },
        "0.01": null
      },
      "auroc": 0.9811524591619318
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 9470,
          "fn": 130,
          "accuracy": 0.9864583333333333
        },
        "0.01": null
      },
      "auroc": 0.9902804036458333
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 26250,
          "fn": 950,
          "accuracy": 0.9650735294117647
        },
        "0.01": null
      },
      "auroc": 0.9843740866268382
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 34238,
          "fn": 962,
          "accuracy": 0.9726704545454545
        },
        "0.01": null
      },
      "auroc": 0.9864607629024622
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 19059,
          "fn": 141,
          "accuracy": 0.99265625
        },
        "0.01": null
      },
      "auroc": 0.9919908026801215
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 53297,
          "fn": 1103,
          "accuracy": 0.9797242647058824
        },
        "0.01": null
      },
      "auroc": 0.9884125416475185
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1561,
          "fn": 39,
          "accuracy": 0.975625
        },
        "0.01": null
      },
      "auroc": 0.9879661946614582
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1518,
          "fn": 82,
          "accuracy": 0.94875
        },
        "0.01": null
      },
      "auroc": 0.9841178059895833
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3079,
          "fn": 121,
          "accuracy": 0.9621875
        },
        "0.01": null
      },
      "auroc": 0.9860420003255208
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1550,
          "fn": 50,
          "accuracy": 0.96875
        },
        "0.01": null
      },
      "auroc": 0.986106396484375
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1475,
          "fn": 125,
          "accuracy": 0.921875
        },
        "0.01": null
      },
      "auroc": 0.980313232421875
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3025,
          "fn": 175,
          "accuracy": 0.9453125
        },
        "0.01": null
      },
      "auroc": 0.9832098144531249
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3111,
          "fn": 89,
          "accuracy": 0.9721875
        },
        "0.01": null
      },
      "auroc": 0.9870362955729167
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2993,
          "fn": 207,
          "accuracy": 0.9353125
        },
        "0.01": null
      },
      "auroc": 0.9822155192057291
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 6104,
          "fn": 296,
          "accuracy": 0.95375
        },
        "0.01": null
      },
      "auroc": 0.9846259073893229
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1544,
          "fn": 56,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9857301595052084
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1559,
          "fn": 41,
          "accuracy": 0.974375
        },
        "0.01": null
      },
      "auroc": 0.984905859375
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3103,
          "fn": 97,
          "accuracy": 0.9696875
        },
        "0.01": null
      },
      "auroc": 0.9853180094401042
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 989,
          "fn": 611,
          "accuracy": 0.618125
        },
        "0.01": null
      },
      "auroc": 0.927464111328125
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1507,
          "fn": 93,
          "accuracy": 0.941875
        },
        "0.01": null
      },
      "auroc": 0.9776541666666666
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2496,
          "fn": 704,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.9525591389973959
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2533,
          "fn": 667,
          "accuracy": 0.7915625
        },
        "0.01": null
      },
      "auroc": 0.9565971354166666
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3066,
          "fn": 134,
          "accuracy": 0.958125
        },
        "0.01": null
      },
      "auroc": 0.9812800130208333
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5599,
          "fn": 801,
          "accuracy": 0.87484375
        },
        "0.01": null
      },
      "auroc": 0.9689385742187502
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1548,
          "fn": 52,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9877858561197917
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1339,
          "fn": 261,
          "accuracy": 0.836875
        },
        "0.01": null
      },
      "auroc": 0.97188642578125
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2887,
          "fn": 313,
          "accuracy": 0.9021875
        },
        "0.01": null
      },
      "auroc": 0.9798361409505209
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1513,
          "fn": 87,
          "accuracy": 0.945625
        },
        "0.01": null
      },
      "auroc": 0.9845409993489583
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 401,
          "accuracy": 0.749375
        },
        "0.01": null
      },
      "auroc": 0.9573302408854166
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2712,
          "fn": 488,
          "accuracy": 0.8475
        },
        "0.01": null
      },
      "auroc": 0.9709356201171875
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3061,
          "fn": 139,
          "accuracy": 0.9565625
        },
        "0.01": null
      },
      "auroc": 0.9861634277343749
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2538,
          "fn": 662,
          "accuracy": 0.793125
        },
        "0.01": null
      },
      "auroc": 0.9646083333333333
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5599,
          "fn": 801,
          "accuracy": 0.87484375
        },
        "0.01": null
      },
      "auroc": 0.9753858805338541
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1585,
          "fn": 15,
          "accuracy": 0.990625
        },
        "0.01": null
      },
      "auroc": 0.9918130696614583
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1533,
          "fn": 67,
          "accuracy": 0.958125
        },
        "0.01": null
      },
      "auroc": 0.9836317057291667
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3118,
          "fn": 82,
          "accuracy": 0.974375
        },
        "0.01": null
      },
      "auroc": 0.9877223876953124
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1024,
          "fn": 576,
          "accuracy": 0.64
        },
        "0.01": null
      },
      "auroc": 0.934522802734375
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1174,
          "fn": 426,
          "accuracy": 0.73375
        },
        "0.01": null
      },
      "auroc": 0.9518346842447917
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2198,
          "fn": 1002,
          "accuracy": 0.686875
        },
        "0.01": null
      },
      "auroc": 0.9431787434895833
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2609,
          "fn": 591,
          "accuracy": 0.8153125
        },
        "0.01": null
      },
      "auroc": 0.9631679361979167
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2707,
          "fn": 493,
          "accuracy": 0.8459375
        },
        "0.01": null
      },
      "auroc": 0.9677331949869792
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5316,
          "fn": 1084,
          "accuracy": 0.830625
        },
        "0.01": null
      },
      "auroc": 0.9654505655924479
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1538,
          "fn": 62,
          "accuracy": 0.96125
        },
        "0.01": null
      },
      "auroc": 0.9885930013020835
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1461,
          "fn": 139,
          "accuracy": 0.913125
        },
        "0.01": null
      },
      "auroc": 0.9824217122395833
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2999,
          "fn": 201,
          "accuracy": 0.9371875
        },
        "0.01": null
      },
      "auroc": 0.9855073567708332
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1003,
          "fn": 597,
          "accuracy": 0.626875
        },
        "0.01": null
      },
      "auroc": 0.9326392415364584
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1218,
          "fn": 382,
          "accuracy": 0.76125
        },
        "0.01": null
      },
      "auroc": 0.9565587239583333
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2221,
          "fn": 979,
          "accuracy": 0.6940625
        },
        "0.01": null
      },
      "auroc": 0.9445989827473957
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2541,
          "fn": 659,
          "accuracy": 0.7940625
        },
        "0.01": null
      },
      "auroc": 0.9606161214192709
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2679,
          "fn": 521,
          "accuracy": 0.8371875
        },
        "0.01": null
      },
      "auroc": 0.9694902180989582
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5220,
          "fn": 1180,
          "accuracy": 0.815625
        },
        "0.01": null
      },
      "auroc": 0.9650531697591146
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1576,
          "fn": 24,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9889887369791668
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1517,
          "fn": 83,
          "accuracy": 0.948125
        },
        "0.01": null
      },
      "auroc": 0.982137353515625
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3093,
          "fn": 107,
          "accuracy": 0.9665625
        },
        "0.01": null
      },
      "auroc": 0.9855630452473959
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1500,
          "fn": 100,
          "accuracy": 0.9375
        },
        "0.01": null
      },
      "auroc": 0.9816634277343751
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1350,
          "fn": 250,
          "accuracy": 0.84375
        },
        "0.01": null
      },
      "auroc": 0.9692114420572917
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2850,
          "fn": 350,
          "accuracy": 0.890625
        },
        "0.01": null
      },
      "auroc": 0.9754374348958332
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3076,
          "fn": 124,
          "accuracy": 0.96125
        },
        "0.01": null
      },
      "auroc": 0.9853260823567709
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2867,
          "fn": 333,
          "accuracy": 0.8959375
        },
        "0.01": null
      },
      "auroc": 0.9756743977864584
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5943,
          "fn": 457,
          "accuracy": 0.92859375
        },
        "0.01": null
      },
      "auroc": 0.9805002400716145
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1447,
          "fn": 153,
          "accuracy": 0.904375
        },
        "0.01": null
      },
      "auroc": 0.9778081868489584
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1447,
          "fn": 153,
          "accuracy": 0.904375
        },
        "0.01": null
      },
      "auroc": 0.9778081868489584
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1312,
          "fn": 288,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9637701822916667
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1312,
          "fn": 288,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9637701822916667
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2759,
          "fn": 441,
          "accuracy": 0.8621875
        },
        "0.01": null
      },
      "auroc": 0.9707891845703125
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2759,
          "fn": 441,
          "accuracy": 0.8621875
        },
        "0.01": null
      },
      "auroc": 0.9707891845703125
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1049,
          "fn": 551,
          "accuracy": 0.655625
        },
        "0.01": null
      },
      "auroc": 0.9375459635416667
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1049,
          "fn": 551,
          "accuracy": 0.655625
        },
        "0.01": null
      },
      "auroc": 0.9375459635416667
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 931,
          "fn": 669,
          "accuracy": 0.581875
        },
        "0.01": null
      },
      "auroc": 0.9267944986979169
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 931,
          "fn": 669,
          "accuracy": 0.581875
        },
        "0.01": null
      },
      "auroc": 0.9267944986979169
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1980,
          "fn": 1220,
          "accuracy": 0.61875
        },
        "0.01": null
      },
      "auroc": 0.9321702311197917
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1980,
          "fn": 1220,
          "accuracy": 0.61875
        },
        "0.01": null
      },
      "auroc": 0.9321702311197917
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1565,
          "fn": 35,
          "accuracy": 0.978125
        },
        "0.01": null
      },
      "auroc": 0.9879355305989583
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1565,
          "fn": 35,
          "accuracy": 0.978125
        },
        "0.01": null
      },
      "auroc": 0.9879355305989583
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1513,
          "fn": 87,
          "accuracy": 0.945625
        },
        "0.01": null
      },
      "auroc": 0.9818767415364583
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1513,
          "fn": 87,
          "accuracy": 0.945625
        },
        "0.01": null
      },
      "auroc": 0.9818767415364583
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3078,
          "fn": 122,
          "accuracy": 0.961875
        },
        "0.01": null
      },
      "auroc": 0.9849061360677083
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3078,
          "fn": 122,
          "accuracy": 0.961875
        },
        "0.01": null
      },
      "auroc": 0.9849061360677083
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1559,
          "fn": 41,
          "accuracy": 0.974375
        },
        "0.01": null
      },
      "auroc": 0.9881043294270833
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1559,
          "fn": 41,
          "accuracy": 0.974375
        },
        "0.01": null
      },
      "auroc": 0.9881043294270833
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1391,
          "fn": 209,
          "accuracy": 0.869375
        },
        "0.01": null
      },
      "auroc": 0.9699085286458334
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1391,
          "fn": 209,
          "accuracy": 0.869375
        },
        "0.01": null
      },
      "auroc": 0.9699085286458334
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2950,
          "fn": 250,
          "accuracy": 0.921875
        },
        "0.01": null
      },
      "auroc": 0.9790064290364583
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2950,
          "fn": 250,
          "accuracy": 0.921875
        },
        "0.01": null
      },
      "auroc": 0.9790064290364583
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1330,
          "fn": 270,
          "accuracy": 0.83125
        },
        "0.01": null
      },
      "auroc": 0.9663318033854166
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1330,
          "fn": 270,
          "accuracy": 0.83125
        },
        "0.01": null
      },
      "auroc": 0.9663318033854166
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1227,
          "fn": 373,
          "accuracy": 0.766875
        },
        "0.01": null
      },
      "auroc": 0.9576810546875001
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1227,
          "fn": 373,
          "accuracy": 0.766875
        },
        "0.01": null
      },
      "auroc": 0.9576810546875001
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2557,
          "fn": 643,
          "accuracy": 0.7990625
        },
        "0.01": null
      },
      "auroc": 0.9620064290364584
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2557,
          "fn": 643,
          "accuracy": 0.7990625
        },
        "0.01": null
      },
      "auroc": 0.9620064290364584
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 16302,
          "fn": 1298,
          "accuracy": 0.92625
        },
        "0.01": null
      },
      "auroc": 0.9807820756392045
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 8927,
          "fn": 673,
          "accuracy": 0.9298958333333334
        },
        "0.01": null
      },
      "auroc": 0.981516810438368
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 25229,
          "fn": 1971,
          "accuracy": 0.9275367647058823
        },
        "0.01": null
      },
      "auroc": 0.9810413938036153
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 13953,
          "fn": 3647,
          "accuracy": 0.7927840909090909
        },
        "0.01": null
      },
      "auroc": 0.9588152713660039
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 7923,
          "fn": 1677,
          "accuracy": 0.8253125
        },
        "0.01": null
      },
      "auroc": 0.9654837483723957
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 21876,
          "fn": 5324,
          "accuracy": 0.804264705882353
        },
        "0.01": null
      },
      "auroc": 0.9611688514859067
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 30255,
          "fn": 4945,
          "accuracy": 0.8595170454545454
        },
        "0.01": null
      },
      "auroc": 0.9697986735026041
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 16850,
          "fn": 2350,
          "accuracy": 0.8776041666666666
        },
        "0.01": null
      },
      "auroc": 0.973500279405382
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 47105,
          "fn": 7295,
          "accuracy": 0.8659007352941176
        },
        "0.01": null
      },
      "auroc": 0.971105122644761
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1599,
          "fn": 1,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9937547037760417
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3199,
          "fn": 1,
          "accuracy": 0.9996875
        },
        "0.01": null
      },
      "auroc": 0.9938955810546874
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3199,
          "fn": 1,
          "accuracy": 0.9996875
        },
        "0.01": null
      },
      "auroc": 0.9938955810546874
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 6399,
          "fn": 1,
          "accuracy": 0.99984375
        },
        "0.01": null
      },
      "auroc": 0.9939660196940104
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1598,
          "fn": 2,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9933230468750001
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1599,
          "fn": 1,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9934166341145834
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.9933698404947917
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1416,
          "fn": 184,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9636838216145833
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1584,
          "fn": 16,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9916257649739584
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3000,
          "fn": 200,
          "accuracy": 0.9375
        },
        "0.01": null
      },
      "auroc": 0.977654793294271
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3014,
          "fn": 186,
          "accuracy": 0.941875
        },
        "0.01": null
      },
      "auroc": 0.9785034342447916
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3183,
          "fn": 17,
          "accuracy": 0.9946875
        },
        "0.01": null
      },
      "auroc": 0.9925211995442708
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 6197,
          "fn": 203,
          "accuracy": 0.96828125
        },
        "0.01": null
      },
      "auroc": 0.9855123168945311
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.994021337890625
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9935638183593751
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3195,
          "fn": 5,
          "accuracy": 0.9984375
        },
        "0.01": null
      },
      "auroc": 0.993792578125
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9939813802083333
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1563,
          "fn": 37,
          "accuracy": 0.976875
        },
        "0.01": null
      },
      "auroc": 0.9878081868489583
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3163,
          "fn": 37,
          "accuracy": 0.9884375
        },
        "0.01": null
      },
      "auroc": 0.9908947835286458
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940013590494791
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3158,
          "fn": 42,
          "accuracy": 0.986875
        },
        "0.01": null
      },
      "auroc": 0.9906860026041666
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 6358,
          "fn": 42,
          "accuracy": 0.9934375
        },
        "0.01": null
      },
      "auroc": 0.9923436808268229
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9938776529947917
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940142415364583
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.993945947265625
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1575,
          "fn": 25,
          "accuracy": 0.984375
        },
        "0.01": null
      },
      "auroc": 0.9869339029947918
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9932683919270834
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3170,
          "fn": 30,
          "accuracy": 0.990625
        },
        "0.01": null
      },
      "auroc": 0.9901011474609376
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3172,
          "fn": 28,
          "accuracy": 0.99125
        },
        "0.01": null
      },
      "auroc": 0.9904057779947917
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3195,
          "fn": 5,
          "accuracy": 0.9984375
        },
        "0.01": null
      },
      "auroc": 0.9936413167317708
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 6367,
          "fn": 33,
          "accuracy": 0.99484375
        },
        "0.01": null
      },
      "auroc": 0.9920235473632812
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1596,
          "fn": 4,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9931773111979166
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9932998372395834
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3193,
          "fn": 7,
          "accuracy": 0.9978125
        },
        "0.01": null
      },
      "auroc": 0.99323857421875
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1395,
          "fn": 205,
          "accuracy": 0.871875
        },
        "0.01": null
      },
      "auroc": 0.9573001139322919
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1570,
          "fn": 30,
          "accuracy": 0.98125
        },
        "0.01": null
      },
      "auroc": 0.9876921223958334
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2965,
          "fn": 235,
          "accuracy": 0.9265625
        },
        "0.01": null
      },
      "auroc": 0.9724961181640626
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2991,
          "fn": 209,
          "accuracy": 0.9346875
        },
        "0.01": null
      },
      "auroc": 0.9752387125651043
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3167,
          "fn": 33,
          "accuracy": 0.9896875
        },
        "0.01": null
      },
      "auroc": 0.9904959798177083
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 6158,
          "fn": 242,
          "accuracy": 0.9621875
        },
        "0.01": null
      },
      "auroc": 0.9828673461914063
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940112630208333
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940298339843749
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940205485026041
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1598,
          "fn": 2,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9936014811197917
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1585,
          "fn": 15,
          "accuracy": 0.990625
        },
        "0.01": null
      },
      "auroc": 0.991020556640625
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3183,
          "fn": 17,
          "accuracy": 0.9946875
        },
        "0.01": null
      },
      "auroc": 0.9923110188802085
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3198,
          "fn": 2,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9938063720703125
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3185,
          "fn": 15,
          "accuracy": 0.9953125
        },
        "0.01": null
      },
      "auroc": 0.9925251953124999
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 6383,
          "fn": 17,
          "accuracy": 0.99734375
        },
        "0.01": null
      },
      "auroc": 0.9931657836914062
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1594,
          "fn": 6,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.9929846354166666
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1594,
          "fn": 6,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.9929846354166666
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1579,
          "fn": 21,
          "accuracy": 0.986875
        },
        "0.01": null
      },
      "auroc": 0.9900994954427083
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1579,
          "fn": 21,
          "accuracy": 0.986875
        },
        "0.01": null
      },
      "auroc": 0.9900994954427083
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3173,
          "fn": 27,
          "accuracy": 0.9915625
        },
        "0.01": null
      },
      "auroc": 0.9915420654296876
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3173,
          "fn": 27,
          "accuracy": 0.9915625
        },
        "0.01": null
      },
      "auroc": 0.9915420654296876
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1492,
          "fn": 108,
          "accuracy": 0.9325
        },
        "0.01": null
      },
      "auroc": 0.9757849446614584
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1492,
          "fn": 108,
          "accuracy": 0.9325
        },
        "0.01": null
      },
      "auroc": 0.9757849446614584
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1386,
          "fn": 214,
          "accuracy": 0.86625
        },
        "0.01": null
      },
      "auroc": 0.9547717936197917
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1386,
          "fn": 214,
          "accuracy": 0.86625
        },
        "0.01": null
      },
      "auroc": 0.9547717936197917
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2878,
          "fn": 322,
          "accuracy": 0.899375
        },
        "0.01": null
      },
      "auroc": 0.9652783691406249
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2878,
          "fn": 322,
          "accuracy": 0.899375
        },
        "0.01": null
      },
      "auroc": 0.9652783691406249
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.993020703125
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.993020703125
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9929062337239585
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9929062337239585
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3192,
          "fn": 8,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9929634684244791
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3192,
          "fn": 8,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9929634684244791
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940316569010417
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940316569010417
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1599,
          "fn": 1,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9937161621093751
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1599,
          "fn": 1,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9937161621093751
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3199,
          "fn": 1,
          "accuracy": 0.9996875
        },
        "0.01": null
      },
      "auroc": 0.9938739095052083
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3199,
          "fn": 1,
          "accuracy": 0.9996875
        },
        "0.01": null
      },
      "auroc": 0.9938739095052083
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1583,
          "fn": 17,
          "accuracy": 0.989375
        },
        "0.01": null
      },
      "auroc": 0.9906155598958335
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1583,
          "fn": 17,
          "accuracy": 0.989375
        },
        "0.01": null
      },
      "auroc": 0.9906155598958335
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1547,
          "fn": 53,
          "accuracy": 0.966875
        },
        "0.01": null
      },
      "auroc": 0.9842228352864584
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1547,
          "fn": 53,
          "accuracy": 0.966875
        },
        "0.01": null
      },
      "auroc": 0.9842228352864584
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3130,
          "fn": 70,
          "accuracy": 0.978125
        },
        "0.01": null
      },
      "auroc": 0.9874191975911457
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3130,
          "fn": 70,
          "accuracy": 0.978125
        },
        "0.01": null
      },
      "auroc": 0.9874191975911457
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 17457,
          "fn": 143,
          "accuracy": 0.991875
        },
        "0.01": null
      },
      "auroc": 0.9917167791193182
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 9591,
          "fn": 9,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.9937268039279514
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 27048,
          "fn": 152,
          "accuracy": 0.9944117647058823
        },
        "0.01": null
      },
      "auroc": 0.9924261996400122
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 16890,
          "fn": 710,
          "accuracy": 0.959659090909091
        },
        "0.01": null
      },
      "auroc": 0.9822957889441287
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 9496,
          "fn": 104,
          "accuracy": 0.9891666666666666
        },
        "0.01": null
      },
      "auroc": 0.99086162109375
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 26386,
          "fn": 814,
          "accuracy": 0.9700735294117647
        },
        "0.01": null
      },
      "auroc": 0.9853190238204657
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 34347,
          "fn": 853,
          "accuracy": 0.9757670454545454
        },
        "0.01": null
      },
      "auroc": 0.9870062840317235
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 19087,
          "fn": 113,
          "accuracy": 0.9941145833333334
        },
        "0.01": null
      },
      "auroc": 0.9922942125108507
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 53434,
          "fn": 966,
          "accuracy": 0.9822426470588236
        },
        "0.01": null
      },
      "auroc": 0.9888726117302389
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940316569010417
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1598,
          "fn": 2,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9937062662760416
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3198,
          "fn": 2,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9938689615885417
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940340576171875
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3198,
          "fn": 2,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9938713623046875
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 6398,
          "fn": 2,
          "accuracy": 0.9996875
        },
        "0.01": null
      },
      "auroc": 0.9939527099609375
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1598,
          "fn": 2,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9933598632812501
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1599,
          "fn": 1,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9934018229166666
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.9933808430989585
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1367,
          "fn": 233,
          "accuracy": 0.854375
        },
        "0.01": null
      },
      "auroc": 0.9563447753906249
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1586,
          "fn": 14,
          "accuracy": 0.99125
        },
        "0.01": null
      },
      "auroc": 0.9916477213541667
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2953,
          "fn": 247,
          "accuracy": 0.9228125
        },
        "0.01": null
      },
      "auroc": 0.9739962483723958
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2965,
          "fn": 235,
          "accuracy": 0.9265625
        },
        "0.01": null
      },
      "auroc": 0.9748523193359375
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3185,
          "fn": 15,
          "accuracy": 0.9953125
        },
        "0.01": null
      },
      "auroc": 0.9925247721354167
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 6150,
          "fn": 250,
          "accuracy": 0.9609375
        },
        "0.01": null
      },
      "auroc": 0.9836885457356771
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940237467447917
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9935336263020832
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3195,
          "fn": 5,
          "accuracy": 0.9984375
        },
        "0.01": null
      },
      "auroc": 0.9937786865234374
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.993989404296875
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1560,
          "fn": 40,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9876056966145834
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3160,
          "fn": 40,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9907975504557291
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940065755208334
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3155,
          "fn": 45,
          "accuracy": 0.9859375
        },
        "0.01": null
      },
      "auroc": 0.9905696614583335
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 6355,
          "fn": 45,
          "accuracy": 0.99296875
        },
        "0.01": null
      },
      "auroc": 0.9922881184895834
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1596,
          "fn": 4,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9938463541666667
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940202311197917
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3196,
          "fn": 4,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9939332926432292
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1568,
          "fn": 32,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9855759440104167
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1592,
          "fn": 8,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9924755859375001
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3160,
          "fn": 40,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9890257649739583
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3164,
          "fn": 36,
          "accuracy": 0.98875
        },
        "0.01": null
      },
      "auroc": 0.9897111490885417
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3192,
          "fn": 8,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9932479085286458
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 6356,
          "fn": 44,
          "accuracy": 0.993125
        },
        "0.01": null
      },
      "auroc": 0.9914795288085938
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1596,
          "fn": 4,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9930819173177083
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9931356445312499
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3191,
          "fn": 9,
          "accuracy": 0.9971875
        },
        "0.01": null
      },
      "auroc": 0.9931087809244792
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1339,
          "fn": 261,
          "accuracy": 0.836875
        },
        "0.01": null
      },
      "auroc": 0.9500531901041667
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1553,
          "fn": 47,
          "accuracy": 0.970625
        },
        "0.01": null
      },
      "auroc": 0.9854151692708333
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2892,
          "fn": 308,
          "accuracy": 0.90375
        },
        "0.01": null
      },
      "auroc": 0.9677341796874999
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2935,
          "fn": 265,
          "accuracy": 0.9171875
        },
        "0.01": null
      },
      "auroc": 0.9715675537109375
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3148,
          "fn": 52,
          "accuracy": 0.98375
        },
        "0.01": null
      },
      "auroc": 0.9892754069010417
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 6083,
          "fn": 317,
          "accuracy": 0.95046875
        },
        "0.01": null
      },
      "auroc": 0.9804214803059894
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940107259114583
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940298339843749
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940202799479166
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9936361002604166
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1578,
          "fn": 22,
          "accuracy": 0.98625
        },
        "0.01": null
      },
      "auroc": 0.990110009765625
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3175,
          "fn": 25,
          "accuracy": 0.9921875
        },
        "0.01": null
      },
      "auroc": 0.9918730550130208
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.9938234130859374
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3178,
          "fn": 22,
          "accuracy": 0.993125
        },
        "0.01": null
      },
      "auroc": 0.992069921875
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 6375,
          "fn": 25,
          "accuracy": 0.99609375
        },
        "0.01": null
      },
      "auroc": 0.9929466674804688
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1594,
          "fn": 6,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.992854345703125
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1594,
          "fn": 6,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.992854345703125
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1580,
          "fn": 20,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9901673828125
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1580,
          "fn": 20,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9901673828125
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3174,
          "fn": 26,
          "accuracy": 0.991875
        },
        "0.01": null
      },
      "auroc": 0.9915108642578125
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3174,
          "fn": 26,
          "accuracy": 0.991875
        },
        "0.01": null
      },
      "auroc": 0.9915108642578125
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1494,
          "fn": 106,
          "accuracy": 0.93375
        },
        "0.01": null
      },
      "auroc": 0.9756790364583333
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1494,
          "fn": 106,
          "accuracy": 0.93375
        },
        "0.01": null
      },
      "auroc": 0.9756790364583333
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1374,
          "fn": 226,
          "accuracy": 0.85875
        },
        "0.01": null
      },
      "auroc": 0.9544021972656249
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1374,
          "fn": 226,
          "accuracy": 0.85875
        },
        "0.01": null
      },
      "auroc": 0.9544021972656249
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2868,
          "fn": 332,
          "accuracy": 0.89625
        },
        "0.01": null
      },
      "auroc": 0.9650406168619792
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2868,
          "fn": 332,
          "accuracy": 0.89625
        },
        "0.01": null
      },
      "auroc": 0.9650406168619792
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9929307942708334
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9929307942708334
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9928424153645834
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9928424153645834
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3192,
          "fn": 8,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9928866048177084
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3192,
          "fn": 8,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9928866048177084
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9940364583333333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9935876302083333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9935876302083333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.9938120442708334
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.9938120442708334
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1581,
          "fn": 19,
          "accuracy": 0.988125
        },
        "0.01": null
      },
      "auroc": 0.990631201171875
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1581,
          "fn": 19,
          "accuracy": 0.988125
        },
        "0.01": null
      },
      "auroc": 0.990631201171875
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1542,
          "fn": 58,
          "accuracy": 0.96375
        },
        "0.01": null
      },
      "auroc": 0.9846374999999999
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1542,
          "fn": 58,
          "accuracy": 0.96375
        },
        "0.01": null
      },
      "auroc": 0.9846374999999999
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3123,
          "fn": 77,
          "accuracy": 0.9759375
        },
        "0.01": null
      },
      "auroc": 0.9876343505859374
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3123,
          "fn": 77,
          "accuracy": 0.9759375
        },
        "0.01": null
      },
      "auroc": 0.9876343505859374
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 17456,
          "fn": 144,
          "accuracy": 0.9918181818181818
        },
        "0.01": null
      },
      "auroc": 0.9916809910629736
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 9589,
          "fn": 11,
          "accuracy": 0.9988541666666667
        },
        "0.01": null
      },
      "auroc": 0.9936929361979167
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 27045,
          "fn": 155,
          "accuracy": 0.9943014705882353
        },
        "0.01": null
      },
      "auroc": 0.9923910893458947
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 16759,
          "fn": 841,
          "accuracy": 0.9522159090909091
        },
        "0.01": null
      },
      "auroc": 0.9808425633285984
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 9467,
          "fn": 133,
          "accuracy": 0.9861458333333334
        },
        "0.01": null
      },
      "auroc": 0.9901600748697916
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 26226,
          "fn": 974,
          "accuracy": 0.9641911764705883
        },
        "0.01": null
      },
      "auroc": 0.9841310968137255
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 34215,
          "fn": 985,
          "accuracy": 0.9720170454545455
        },
        "0.01": null
      },
      "auroc": 0.986261777195786
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 19056,
          "fn": 144,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9919265055338541
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 53271,
          "fn": 1129,
          "accuracy": 0.9792463235294118
        },
        "0.01": null
      },
      "auroc": 0.9882610930798101
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1434,
          "fn": 166,
          "accuracy": 0.89625
        },
        "0.01": null
      },
      "auroc": 0.9742240234375
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1345,
          "fn": 255,
          "accuracy": 0.840625
        },
        "0.01": null
      },
      "auroc": 0.9690442871093751
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2779,
          "fn": 421,
          "accuracy": 0.8684375
        },
        "0.01": null
      },
      "auroc": 0.9716341552734375
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1396,
          "fn": 204,
          "accuracy": 0.8725
        },
        "0.01": null
      },
      "auroc": 0.9723387369791667
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1326,
          "fn": 274,
          "accuracy": 0.82875
        },
        "0.01": null
      },
      "auroc": 0.9668061848958334
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2722,
          "fn": 478,
          "accuracy": 0.850625
        },
        "0.01": null
      },
      "auroc": 0.9695724609375
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2830,
          "fn": 370,
          "accuracy": 0.884375
        },
        "0.01": null
      },
      "auroc": 0.9732813802083333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2671,
          "fn": 529,
          "accuracy": 0.8346875
        },
        "0.01": null
      },
      "auroc": 0.967925236002604
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 5501,
          "fn": 899,
          "accuracy": 0.85953125
        },
        "0.01": null
      },
      "auroc": 0.9706033081054688
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1449,
          "fn": 151,
          "accuracy": 0.905625
        },
        "0.01": null
      },
      "auroc": 0.9773968098958334
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1402,
          "fn": 198,
          "accuracy": 0.87625
        },
        "0.01": null
      },
      "auroc": 0.9719199869791667
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2851,
          "fn": 349,
          "accuracy": 0.8909375
        },
        "0.01": null
      },
      "auroc": 0.9746583984375
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 936,
          "fn": 664,
          "accuracy": 0.585
        },
        "0.01": null
      },
      "auroc": 0.933532568359375
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1257,
          "fn": 343,
          "accuracy": 0.785625
        },
        "0.01": null
      },
      "auroc": 0.9601315104166668
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2193,
          "fn": 1007,
          "accuracy": 0.6853125
        },
        "0.01": null
      },
      "auroc": 0.9468320393880209
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2385,
          "fn": 815,
          "accuracy": 0.7453125
        },
        "0.01": null
      },
      "auroc": 0.9554646891276042
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2659,
          "fn": 541,
          "accuracy": 0.8309375
        },
        "0.01": null
      },
      "auroc": 0.9660257486979166
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 5044,
          "fn": 1356,
          "accuracy": 0.788125
        },
        "0.01": null
      },
      "auroc": 0.9607452189127603
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1429,
          "fn": 171,
          "accuracy": 0.893125
        },
        "0.01": null
      },
      "auroc": 0.9752150390625
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1154,
          "fn": 446,
          "accuracy": 0.72125
        },
        "0.01": null
      },
      "auroc": 0.960252294921875
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2583,
          "fn": 617,
          "accuracy": 0.8071875
        },
        "0.01": null
      },
      "auroc": 0.9677336669921877
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1352,
          "fn": 248,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9700534016927084
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1024,
          "fn": 576,
          "accuracy": 0.64
        },
        "0.01": null
      },
      "auroc": 0.9498327962239583
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2376,
          "fn": 824,
          "accuracy": 0.7425
        },
        "0.01": null
      },
      "auroc": 0.9599430989583333
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2781,
          "fn": 419,
          "accuracy": 0.8690625
        },
        "0.01": null
      },
      "auroc": 0.9726342203776042
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2178,
          "fn": 1022,
          "accuracy": 0.680625
        },
        "0.01": null
      },
      "auroc": 0.9550425455729168
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4959,
          "fn": 1441,
          "accuracy": 0.77484375
        },
        "0.01": null
      },
      "auroc": 0.9638383829752604
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1545,
          "fn": 55,
          "accuracy": 0.965625
        },
        "0.01": null
      },
      "auroc": 0.9869897786458334
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1434,
          "fn": 166,
          "accuracy": 0.89625
        },
        "0.01": null
      },
      "auroc": 0.9742126302083332
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2979,
          "fn": 221,
          "accuracy": 0.9309375
        },
        "0.01": null
      },
      "auroc": 0.9806012044270832
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 957,
          "fn": 643,
          "accuracy": 0.598125
        },
        "0.01": null
      },
      "auroc": 0.9361551432291666
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1001,
          "fn": 599,
          "accuracy": 0.625625
        },
        "0.01": null
      },
      "auroc": 0.9454459309895833
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1958,
          "fn": 1242,
          "accuracy": 0.611875
        },
        "0.01": null
      },
      "auroc": 0.9408005371093751
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2502,
          "fn": 698,
          "accuracy": 0.781875
        },
        "0.01": null
      },
      "auroc": 0.9615724609375
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2435,
          "fn": 765,
          "accuracy": 0.7609375
        },
        "0.01": null
      },
      "auroc": 0.9598292805989583
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4937,
          "fn": 1463,
          "accuracy": 0.77140625
        },
        "0.01": null
      },
      "auroc": 0.9607008707682292
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1456,
          "fn": 144,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9835965006510418
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1380,
          "fn": 220,
          "accuracy": 0.8625
        },
        "0.01": null
      },
      "auroc": 0.9793053710937499
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2836,
          "fn": 364,
          "accuracy": 0.88625
        },
        "0.01": null
      },
      "auroc": 0.9814509358723957
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 982,
          "fn": 618,
          "accuracy": 0.61375
        },
        "0.01": null
      },
      "auroc": 0.9413982584635415
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1049,
          "fn": 551,
          "accuracy": 0.655625
        },
        "0.01": null
      },
      "auroc": 0.9517322591145834
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2031,
          "fn": 1169,
          "accuracy": 0.6346875
        },
        "0.01": null
      },
      "auroc": 0.9465652587890625
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2438,
          "fn": 762,
          "accuracy": 0.761875
        },
        "0.01": null
      },
      "auroc": 0.9624973795572916
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2429,
          "fn": 771,
          "accuracy": 0.7590625
        },
        "0.01": null
      },
      "auroc": 0.9655188151041667
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4867,
          "fn": 1533,
          "accuracy": 0.76046875
        },
        "0.01": null
      },
      "auroc": 0.9640080973307292
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1471,
          "fn": 129,
          "accuracy": 0.919375
        },
        "0.01": null
      },
      "auroc": 0.97733388671875
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1304,
          "fn": 296,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.9638423665364583
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2775,
          "fn": 425,
          "accuracy": 0.8671875
        },
        "0.01": null
      },
      "auroc": 0.9705881266276041
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1339,
          "fn": 261,
          "accuracy": 0.836875
        },
        "0.01": null
      },
      "auroc": 0.967085009765625
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1078,
          "fn": 522,
          "accuracy": 0.67375
        },
        "0.01": null
      },
      "auroc": 0.9537857584635416
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2417,
          "fn": 783,
          "accuracy": 0.7553125
        },
        "0.01": null
      },
      "auroc": 0.9604353841145834
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2810,
          "fn": 390,
          "accuracy": 0.878125
        },
        "0.01": null
      },
      "auroc": 0.9722094482421875
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2382,
          "fn": 818,
          "accuracy": 0.744375
        },
        "0.01": null
      },
      "auroc": 0.9588140625000001
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 5192,
          "fn": 1208,
          "accuracy": 0.81125
        },
        "0.01": null
      },
      "auroc": 0.9655117553710937
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1277,
          "fn": 323,
          "accuracy": 0.798125
        },
        "0.01": null
      },
      "auroc": 0.9655017089843749
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1277,
          "fn": 323,
          "accuracy": 0.798125
        },
        "0.01": null
      },
      "auroc": 0.9655017089843749
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1168,
          "fn": 432,
          "accuracy": 0.73
        },
        "0.01": null
      },
      "auroc": 0.9546713541666666
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1168,
          "fn": 432,
          "accuracy": 0.73
        },
        "0.01": null
      },
      "auroc": 0.9546713541666666
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2445,
          "fn": 755,
          "accuracy": 0.7640625
        },
        "0.01": null
      },
      "auroc": 0.9600865315755207
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2445,
          "fn": 755,
          "accuracy": 0.7640625
        },
        "0.01": null
      },
      "auroc": 0.9600865315755207
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 914,
          "fn": 686,
          "accuracy": 0.57125
        },
        "0.01": null
      },
      "auroc": 0.9359872558593751
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 914,
          "fn": 686,
          "accuracy": 0.57125
        },
        "0.01": null
      },
      "auroc": 0.9359872558593751
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 815,
          "fn": 785,
          "accuracy": 0.509375
        },
        "0.01": null
      },
      "auroc": 0.930000927734375
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 815,
          "fn": 785,
          "accuracy": 0.509375
        },
        "0.01": null
      },
      "auroc": 0.930000927734375
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1729,
          "fn": 1471,
          "accuracy": 0.5403125
        },
        "0.01": null
      },
      "auroc": 0.9329940917968749
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1729,
          "fn": 1471,
          "accuracy": 0.5403125
        },
        "0.01": null
      },
      "auroc": 0.9329940917968749
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1430,
          "fn": 170,
          "accuracy": 0.89375
        },
        "0.01": null
      },
      "auroc": 0.9730893717447916
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1430,
          "fn": 170,
          "accuracy": 0.89375
        },
        "0.01": null
      },
      "auroc": 0.9730893717447916
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1315,
          "fn": 285,
          "accuracy": 0.821875
        },
        "0.01": null
      },
      "auroc": 0.9653224446614582
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1315,
          "fn": 285,
          "accuracy": 0.821875
        },
        "0.01": null
      },
      "auroc": 0.9653224446614582
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2745,
          "fn": 455,
          "accuracy": 0.8578125
        },
        "0.01": null
      },
      "auroc": 0.969205908203125
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2745,
          "fn": 455,
          "accuracy": 0.8578125
        },
        "0.01": null
      },
      "auroc": 0.969205908203125
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1485,
          "fn": 115,
          "accuracy": 0.928125
        },
        "0.01": null
      },
      "auroc": 0.9796643717447917
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1485,
          "fn": 115,
          "accuracy": 0.928125
        },
        "0.01": null
      },
      "auroc": 0.9796643717447917
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1238,
          "fn": 362,
          "accuracy": 0.77375
        },
        "0.01": null
      },
      "auroc": 0.9600001302083334
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1238,
          "fn": 362,
          "accuracy": 0.77375
        },
        "0.01": null
      },
      "auroc": 0.9600001302083334
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2723,
          "fn": 477,
          "accuracy": 0.8509375
        },
        "0.01": null
      },
      "auroc": 0.9698322509765626
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2723,
          "fn": 477,
          "accuracy": 0.8509375
        },
        "0.01": null
      },
      "auroc": 0.9698322509765626
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1186,
          "fn": 414,
          "accuracy": 0.74125
        },
        "0.01": null
      },
      "auroc": 0.958485107421875
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1186,
          "fn": 414,
          "accuracy": 0.74125
        },
        "0.01": null
      },
      "auroc": 0.958485107421875
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1107,
          "fn": 493,
          "accuracy": 0.691875
        },
        "0.01": null
      },
      "auroc": 0.9527678222656251
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1107,
          "fn": 493,
          "accuracy": 0.691875
        },
        "0.01": null
      },
      "auroc": 0.9527678222656251
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2293,
          "fn": 907,
          "accuracy": 0.7165625
        },
        "0.01": null
      },
      "auroc": 0.95562646484375
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2293,
          "fn": 907,
          "accuracy": 0.7165625
        },
        "0.01": null
      },
      "auroc": 0.95562646484375
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 15076,
          "fn": 2524,
          "accuracy": 0.8565909090909091
        },
        "0.01": null
      },
      "auroc": 0.9715894412878787
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 8019,
          "fn": 1581,
          "accuracy": 0.8353125
        },
        "0.01": null
      },
      "auroc": 0.9697628228081598
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 23095,
          "fn": 4105,
          "accuracy": 0.8490808823529412
        },
        "0.01": null
      },
      "auroc": 0.9709447524126839
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 12605,
          "fn": 4995,
          "accuracy": 0.7161931818181818
        },
        "0.01": null
      },
      "auroc": 0.9530296179569129
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 6735,
          "fn": 2865,
          "accuracy": 0.7015625
        },
        "0.01": null
      },
      "auroc": 0.9546224066840279
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 19340,
          "fn": 7860,
          "accuracy": 0.7110294117647059
        },
        "0.01": null
      },
      "auroc": 0.9535917786841299
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 27681,
          "fn": 7519,
          "accuracy": 0.7863920454545454
        },
        "0.01": null
      },
      "auroc": 0.9623095296223958
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 14754,
          "fn": 4446,
          "accuracy": 0.7684375
        },
        "0.01": null
      },
      "auroc": 0.9621926147460937
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 42435,
          "fn": 11965,
          "accuracy": 0.7800551470588235
        },
        "0.01": null
      },
      "auroc": 0.9622682655484068
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18991,
          "fn": 209,
          "accuracy": 0.9891145833333334
        },
        "0.01": null
      },
      "auroc": 0.9917650187174478
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18837,
          "fn": 363,
          "accuracy": 0.98109375
        },
        "0.01": null
      },
      "auroc": 0.9907920518663195
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 37828,
          "fn": 572,
          "accuracy": 0.9851041666666667
        },
        "0.01": null
      },
      "auroc": 0.9912785352918837
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18935,
          "fn": 265,
          "accuracy": 0.9861979166666667
        },
        "0.01": null
      },
      "auroc": 0.9914174574110242
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18741,
          "fn": 459,
          "accuracy": 0.97609375
        },
        "0.01": null
      },
      "auroc": 0.989812200249566
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 37676,
          "fn": 724,
          "accuracy": 0.9811458333333334
        },
        "0.01": null
      },
      "auroc": 0.9906148288302951
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 37926,
          "fn": 474,
          "accuracy": 0.98765625
        },
        "0.01": null
      },
      "auroc": 0.991591238064236
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 37578,
          "fn": 822,
          "accuracy": 0.97859375
        },
        "0.01": null
      },
      "auroc": 0.9903021260579427
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 75504,
          "fn": 1296,
          "accuracy": 0.983125
        },
        "0.01": null
      },
      "auroc": 0.9909466820610894
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18947,
          "fn": 253,
          "accuracy": 0.9868229166666667
        },
        "0.01": null
      },
      "auroc": 0.9910245171440971
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18847,
          "fn": 353,
          "accuracy": 0.9816145833333333
        },
        "0.01": null
      },
      "auroc": 0.989381822374132
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 37794,
          "fn": 606,
          "accuracy": 0.98421875
        },
        "0.01": null
      },
      "auroc": 0.9902031697591146
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 15585,
          "fn": 3615,
          "accuracy": 0.81171875
        },
        "0.01": null
      },
      "auroc": 0.9518301011827258
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18457,
          "fn": 743,
          "accuracy": 0.9613020833333333
        },
        "0.01": null
      },
      "auroc": 0.9856611585828993
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 34042,
          "fn": 4358,
          "accuracy": 0.8865104166666666
        },
        "0.01": null
      },
      "auroc": 0.9687456298828125
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 34532,
          "fn": 3868,
          "accuracy": 0.8992708333333334
        },
        "0.01": null
      },
      "auroc": 0.9714273091634114
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 37304,
          "fn": 1096,
          "accuracy": 0.9714583333333333
        },
        "0.01": null
      },
      "auroc": 0.9875214904785155
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 71836,
          "fn": 4964,
          "accuracy": 0.9353645833333334
        },
        "0.01": null
      },
      "auroc": 0.9794743998209636
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18964,
          "fn": 236,
          "accuracy": 0.9877083333333333
        },
        "0.01": null
      },
      "auroc": 0.9917104573567709
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18376,
          "fn": 824,
          "accuracy": 0.9570833333333333
        },
        "0.01": null
      },
      "auroc": 0.9878912448459202
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 37340,
          "fn": 1060,
          "accuracy": 0.9723958333333333
        },
        "0.01": null
      },
      "auroc": 0.9898008511013454
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18836,
          "fn": 364,
          "accuracy": 0.9810416666666667
        },
        "0.01": null
      },
      "auroc": 0.9908178439670139
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 17664,
          "fn": 1536,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9799342298719618
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 36500,
          "fn": 1900,
          "accuracy": 0.9505208333333334
        },
        "0.01": null
      },
      "auroc": 0.9853760369194879
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 37800,
          "fn": 600,
          "accuracy": 0.984375
        },
        "0.01": null
      },
      "auroc": 0.9912641506618923
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 36040,
          "fn": 2360,
          "accuracy": 0.9385416666666667
        },
        "0.01": null
      },
      "auroc": 0.9839127373589409
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 73840,
          "fn": 2960,
          "accuracy": 0.9614583333333333
        },
        "0.01": null
      },
      "auroc": 0.9875884440104167
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 19069,
          "fn": 131,
          "accuracy": 0.9931770833333333
        },
        "0.01": null
      },
      "auroc": 0.9927986206054689
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18956,
          "fn": 244,
          "accuracy": 0.9872916666666667
        },
        "0.01": null
      },
      "auroc": 0.9911740831163194
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 38025,
          "fn": 375,
          "accuracy": 0.990234375
        },
        "0.01": null
      },
      "auroc": 0.991986351860894
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 17562,
          "fn": 1638,
          "accuracy": 0.9146875
        },
        "0.01": null
      },
      "auroc": 0.9762156141493057
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 17988,
          "fn": 1212,
          "accuracy": 0.936875
        },
        "0.01": null
      },
      "auroc": 0.9834619262695314
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 35550,
          "fn": 2850,
          "accuracy": 0.92578125
        },
        "0.01": null
      },
      "auroc": 0.9798387702094185
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 36631,
          "fn": 1769,
          "accuracy": 0.9539322916666667
        },
        "0.01": null
      },
      "auroc": 0.9845071173773872
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 36944,
          "fn": 1456,
          "accuracy": 0.9620833333333333
        },
        "0.01": null
      },
      "auroc": 0.9873180046929253
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 73575,
          "fn": 3225,
          "accuracy": 0.9580078125
        },
        "0.01": null
      },
      "auroc": 0.9859125610351562
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18922,
          "fn": 278,
          "accuracy": 0.9855208333333333
        },
        "0.01": null
      },
      "auroc": 0.9915644395616319
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18666,
          "fn": 534,
          "accuracy": 0.9721875
        },
        "0.01": null
      },
      "auroc": 0.9892507093641492
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 37588,
          "fn": 812,
          "accuracy": 0.9788541666666667
        },
        "0.01": null
      },
      "auroc": 0.9904075744628907
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 15333,
          "fn": 3867,
          "accuracy": 0.79859375
        },
        "0.01": null
      },
      "auroc": 0.9466035101996527
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 17493,
          "fn": 1707,
          "accuracy": 0.91109375
        },
        "0.01": null
      },
      "auroc": 0.9755628974066839
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 32826,
          "fn": 5574,
          "accuracy": 0.85484375
        },
        "0.01": null
      },
      "auroc": 0.9610832038031685
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 34255,
          "fn": 4145,
          "accuracy": 0.8920572916666667
        },
        "0.01": null
      },
      "auroc": 0.9690839748806424
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 36159,
          "fn": 2241,
          "accuracy": 0.941640625
        },
        "0.01": null
      },
      "auroc": 0.9824068033854166
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 70414,
          "fn": 6386,
          "accuracy": 0.9168489583333334
        },
        "0.01": null
      },
      "auroc": 0.9757453891330296
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 19045,
          "fn": 155,
          "accuracy": 0.9919270833333333
        },
        "0.01": null
      },
      "auroc": 0.9920980807834201
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18802,
          "fn": 398,
          "accuracy": 0.9792708333333333
        },
        "0.01": null
      },
      "auroc": 0.9902357543945312
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 37847,
          "fn": 553,
          "accuracy": 0.9855989583333333
        },
        "0.01": null
      },
      "auroc": 0.9911669175889757
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18794,
          "fn": 406,
          "accuracy": 0.9788541666666667
        },
        "0.01": null
      },
      "auroc": 0.9900716376410591
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18142,
          "fn": 1058,
          "accuracy": 0.9448958333333334
        },
        "0.01": null
      },
      "auroc": 0.9844647664388022
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 36936,
          "fn": 1464,
          "accuracy": 0.961875
        },
        "0.01": null
      },
      "auroc": 0.9872682020399304
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 37839,
          "fn": 561,
          "accuracy": 0.985390625
        },
        "0.01": null
      },
      "auroc": 0.9910848592122397
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 36944,
          "fn": 1456,
          "accuracy": 0.9620833333333333
        },
        "0.01": null
      },
      "auroc": 0.9873502604166667
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 74783,
          "fn": 2017,
          "accuracy": 0.9737369791666667
        },
        "0.01": null
      },
      "auroc": 0.9892175598144531
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18626,
          "fn": 574,
          "accuracy": 0.9701041666666667
        },
        "0.01": null
      },
      "auroc": 0.9887693264431424
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18626,
          "fn": 574,
          "accuracy": 0.9701041666666667
        },
        "0.01": null
      },
      "auroc": 0.9887693264431424
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18216,
          "fn": 984,
          "accuracy": 0.94875
        },
        "0.01": null
      },
      "auroc": 0.9842581570095488
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18216,
          "fn": 984,
          "accuracy": 0.94875
        },
        "0.01": null
      },
      "auroc": 0.9842581570095488
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 36842,
          "fn": 1558,
          "accuracy": 0.9594270833333334
        },
        "0.01": null
      },
      "auroc": 0.9865137417263454
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 36842,
          "fn": 1558,
          "accuracy": 0.9594270833333334
        },
        "0.01": null
      },
      "auroc": 0.9865137417263454
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 16607,
          "fn": 2593,
          "accuracy": 0.8649479166666667
        },
        "0.01": null
      },
      "auroc": 0.965867473687066
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 16607,
          "fn": 2593,
          "accuracy": 0.8649479166666667
        },
        "0.01": null
      },
      "auroc": 0.965867473687066
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 15205,
          "fn": 3995,
          "accuracy": 0.7919270833333333
        },
        "0.01": null
      },
      "auroc": 0.9460088080512152
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 15205,
          "fn": 3995,
          "accuracy": 0.7919270833333333
        },
        "0.01": null
      },
      "auroc": 0.9460088080512152
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 31812,
          "fn": 6588,
          "accuracy": 0.8284375
        },
        "0.01": null
      },
      "auroc": 0.9559381408691405
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 31812,
          "fn": 6588,
          "accuracy": 0.8284375
        },
        "0.01": null
      },
      "auroc": 0.9559381408691405
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18955,
          "fn": 245,
          "accuracy": 0.9872395833333333
        },
        "0.01": null
      },
      "auroc": 0.990635386827257
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18955,
          "fn": 245,
          "accuracy": 0.9872395833333333
        },
        "0.01": null
      },
      "auroc": 0.990635386827257
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18765,
          "fn": 435,
          "accuracy": 0.97734375
        },
        "0.01": null
      },
      "auroc": 0.9894671875000001
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18765,
          "fn": 435,
          "accuracy": 0.97734375
        },
        "0.01": null
      },
      "auroc": 0.9894671875000001
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 37720,
          "fn": 680,
          "accuracy": 0.9822916666666667
        },
        "0.01": null
      },
      "auroc": 0.9900512871636286
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 37720,
          "fn": 680,
          "accuracy": 0.9822916666666667
        },
        "0.01": null
      },
      "auroc": 0.9900512871636286
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 19036,
          "fn": 164,
          "accuracy": 0.9914583333333333
        },
        "0.01": null
      },
      "auroc": 0.9919734347873264
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 19036,
          "fn": 164,
          "accuracy": 0.9914583333333333
        },
        "0.01": null
      },
      "auroc": 0.9919734347873264
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18539,
          "fn": 661,
          "accuracy": 0.9655729166666667
        },
        "0.01": null
      },
      "auroc": 0.9877773491753472
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18539,
          "fn": 661,
          "accuracy": 0.9655729166666667
        },
        "0.01": null
      },
      "auroc": 0.9877773491753472
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 37575,
          "fn": 825,
          "accuracy": 0.978515625
        },
        "0.01": null
      },
      "auroc": 0.9898753919813368
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 37575,
          "fn": 825,
          "accuracy": 0.978515625
        },
        "0.01": null
      },
      "auroc": 0.9898753919813368
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18228,
          "fn": 972,
          "accuracy": 0.949375
        },
        "0.01": null
      },
      "auroc": 0.9845599365234375
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18228,
          "fn": 972,
          "accuracy": 0.949375
        },
        "0.01": null
      },
      "auroc": 0.9845599365234375
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 17631,
          "fn": 1569,
          "accuracy": 0.91828125
        },
        "0.01": null
      },
      "auroc": 0.9775318752712674
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 17631,
          "fn": 1569,
          "accuracy": 0.91828125
        },
        "0.01": null
      },
      "auroc": 0.9775318752712674
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 35859,
          "fn": 2541,
          "accuracy": 0.933828125
        },
        "0.01": null
      },
      "auroc": 0.9810459058973525
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 35859,
          "fn": 2541,
          "accuracy": 0.933828125
        },
        "0.01": null
      },
      "auroc": 0.9810459058973525
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 205390,
          "fn": 5810,
          "accuracy": 0.9724905303030303
        },
        "0.01": null
      },
      "auroc": 0.9884333356760969
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 112484,
          "fn": 2716,
          "accuracy": 0.9764236111111111
        },
        "0.01": null
      },
      "auroc": 0.9897876109935619
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 317874,
          "fn": 8526,
          "accuracy": 0.9738786764705882
        },
        "0.01": null
      },
      "auroc": 0.988911315199908
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 193401,
          "fn": 17799,
          "accuracy": 0.9157244318181819
        },
        "0.01": null
      },
      "auroc": 0.9756363219598326
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 108485,
          "fn": 6715,
          "accuracy": 0.9417100694444445
        },
        "0.01": null
      },
      "auroc": 0.9831495298032407
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 301886,
          "fn": 24514,
          "accuracy": 0.9248958333333334
        },
        "0.01": null
      },
      "auroc": 0.9782880423751532
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 398791,
          "fn": 23609,
          "accuracy": 0.944107481060606
        },
        "0.01": null
      },
      "auroc": 0.9820348288179648
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 220969,
          "fn": 9431,
          "accuracy": 0.9590668402777778
        },
        "0.01": null
      },
      "auroc": 0.9864685703984013
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 619760,
          "fn": 33040,
          "accuracy": 0.9493872549019607
        },
        "0.01": null
      },
      "auroc": 0.9835996787875306
    }
  ],
  "thresholds": {
    "0.05": {
      "abstracts": 0.07922010242938993,
      "books": 0.9531128948926926,
      "news": 0.9026758736371994,
      "poetry": 0.9716509193181992,
      "recipes": 0.00029220789670958425,
      "reddit": 0.10061889171600347,
      "reviews": 0.5534828373789787,
      "wiki": 0.2940902897715568
    },
    "0.01": {
      "abstracts": 0.9776576024293899,
      "books": 0.9999999999906868,
      "news": 0.9998438423871994,
      "poetry": 0.9999999999068678,
      "recipes": 0.510134126842022,
      "reddit": 0.9995446729660036,
      "reviews": 0.9997718998789789,
      "wiki": 0.9999999999161808
    }
  },
  "fpr": {
    "0.05": {
      "abstracts": 0.050000000000000044,
      "books": 0.050000000000000044,
      "news": 0.050000000000000044,
      "poetry": 0.050000000000000044,
      "recipes": 0.050000000000000044,
      "reddit": 0.050000000000000044,
      "reviews": 0.050000000000000044,
      "wiki": 0.050000000000000044
    },
    "0.01": {
      "abstracts": 0.010000000000000009,
      "books": 0.020000000000000018,
      "news": 0.010000000000000009,
      "poetry": 0.025000000000000022,
      "recipes": 0.010000000000000009,
      "reddit": 0.010000000000000009,
      "reviews": 0.010000000000000009,
      "wiki": 0.015000000000000013
    }
  }
}