import functools
import hashlib
import json
import os
import traceback
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any

import pandas as pd

from raid.evaluate import run_evaluation
from raid.utils import load_data

# prevent manipulation of results - the results must be generated by this script or else the hash will not match
LEADERBOARD_SALT = os.getenv("LEADERBOARD_SALT", "supersecret").encode()
SUBMISSIONS_ROOT = Path(__file__).parent / "submissions"

# hydrate.py changelog
HYDRATE_SCRIPT_SALT = b"1"


# ==== types ====
@dataclass
class SubmissionMetadata:
    date_released: str
    detector_name: str
    contact_info: str
    website: str | None = None
    paper_link: str | None = None
    huggingface_link: str | None = None
    github_link: str | None = None


@dataclass
class LeaderboardSubmission:
    metadata: SubmissionMetadata
    needs_eval: bool
    submission_hash: Any  # (hash)
    metadata_fp: Path
    predictions_fp: Path
    results_fp: Path


# ==== main ====
def hydrate_all():
    """Main entrypoint - ensure all metadata submissions have valid associated results files"""
    exit_code = 0
    written_files = []
    # for each submission directory,
    for submission_dir in SUBMISSIONS_ROOT.iterdir():
        if not submission_dir.is_dir():
            continue
        # check if it is valid and needs eval
        try:
            submission = check_submission(submission_dir)
        except Exception as e:
            # if invalid, log a check annotation and mark job failure
            print(f"::error title=Invalid submission ({submission_dir.name})::{e}")
            exit_code = 1
            continue

        # if valid and needs eval, run the eval
        if not submission.needs_eval:
            continue
        print(
            f"Found submission for {submission.metadata.detector_name} (metadata:"
            f" {submission.metadata_fp.relative_to(SUBMISSIONS_ROOT)}, predictions:"
            f" {submission.predictions_fp.relative_to(SUBMISSIONS_ROOT)}) that requires eval!"
        )
        try:
            result_fp = eval_submission(submission)
            written_files.append(result_fp)
        except Exception as e:
            # if invalid, log a check annotation and mark job failure
            print(f"::error file={submission.metadata_fp},title=Could not eval submission::{e}")
            traceback.print_exc()
            exit_code = 1

    print(f"Done. Wrote {len(written_files)} results files.")

    # write the written results files to GH outputs
    if "GITHUB_OUTPUT" in os.environ:
        written_files_list = " ".join(f'"{fp.absolute()}"' for fp in written_files)
        with open(os.environ["GITHUB_OUTPUT"], "a") as f:
            f.write(f"changed={len(written_files)}\n")
            f.write(f"written-results={written_files_list}\n")
    return exit_code


def check_submission(submission_dir: Path) -> LeaderboardSubmission:
    """Given a path to a submission directory, check if its corresponding results file needs to be regened.

    If exception raised, the input file(s) is/are somehow invalid; log it and continue but don't run evals
    """
    submission_name = submission_dir.name
    metadata_fp = submission_dir / "metadata.json"
    predictions_fp = submission_dir / "predictions.json"
    results_fp = submission_dir / "results.json"

    # hash the submission
    the_hash = hashlib.sha256()
    the_hash.update(submission_name.encode())
    the_hash.update(metadata_fp.read_bytes())

    # ensure the metadata is readable and has all the required properties
    with open(metadata_fp) as f:
        metadata_data = json.load(f)
        metadata_data = SubmissionMetadata(**metadata_data)

    # hash the prediction file
    the_hash.update(predictions_fp.read_bytes())

    # salt the hash
    the_hash.update(LEADERBOARD_SALT)
    the_hash.update(HYDRATE_SCRIPT_SALT)

    # check if a results file exists with that hash
    needs_eval = True
    if results_fp.exists():
        with open(results_fp) as f:
            try:
                result_data = json.load(f)
                submission_hash = result_data["_submission_hash"]
                results_hash = result_data["_results_hash"]

                calced_results_hash = hashlib.sha256()
                calced_results_hash.update(submission_hash.encode())
                calced_results_hash.update(json.dumps(result_data["scores"]).encode())
                calced_results_hash.update(LEADERBOARD_SALT)
                # result file exists and hashes match
                if submission_hash == the_hash.hexdigest() and results_hash == calced_results_hash.hexdigest():
                    # if so, no eval needed!
                    needs_eval = False
            except (ValueError, KeyError):
                pass
    return LeaderboardSubmission(
        metadata=metadata_data,
        needs_eval=needs_eval,
        submission_hash=the_hash,
        metadata_fp=metadata_fp,
        predictions_fp=predictions_fp,
        results_fp=results_fp,
    )


def eval_submission(submission: LeaderboardSubmission):
    """Read in the gold labels and predictions and eval them all, then write the results file."""

    # load labels and predictions
    df = load_test_data()
    with open(submission.predictions_fp) as f:
        d = json.load(f)

    # evaluate
    eval_results = run_evaluation(d, df)
    scores = eval_results["scores"]

    # get the aggregates
    scores_agg = next(
        (
            s
            for s in reversed(scores)
            if s["domain"] == s["model"] == s["decoding"] == s["repetition_penalty"] == s["attack"] == "all"
        ),
        None,
    )
    if scores_agg is None:
        scores_agg = {
            "_note": (
                "No aggregate score across all settings is reported here as some domains/generator models/decoding"
                " strategies/repetition penalties/adversarial attacks were not included in the submission. This"
                " submission will not appear in the main leaderboard; it will only be visible within the splits in"
                " which all samples were evaluated."
            )
        }

    scores_agg_no_adversarial = next(
        (
            s
            for s in reversed(scores)
            if (s["domain"] == s["model"] == s["decoding"] == s["repetition_penalty"] == "all")
            and (s["attack"] == "none")
        ),
        None,
    )
    if scores_agg_no_adversarial is None:
        scores_agg_no_adversarial = {
            "_note": (
                "No aggregate score across all non-adversarial settings is reported here as some domains/generator"
                " models/decoding strategies/repetition penalties were not included in the submission."
            )
        }

    # hash the results to prevent score manipulation
    results_hash = hashlib.sha256()
    results_hash.update(submission.submission_hash.hexdigest().encode())
    results_hash.update(json.dumps(scores).encode())
    results_hash.update(LEADERBOARD_SALT)

    result = {
        "_autogen_note": (
            "This file is automatically generated by the RAID leaderboard submission script. Do not edit this file"
            " manually or include it in new submissions' PRs."
        ),
        # SHA256(dirname, metadata, predictions, salt)
        "_submission_hash": submission.submission_hash.hexdigest(),
        # SHA256(_submission_hash, scores, salt)
        "_results_hash": results_hash.hexdigest(),
        # metadata
        **asdict(submission.metadata),
        # results
        "score_agg": {"all": scores_agg, "no_adversarial": scores_agg_no_adversarial},
        **eval_results,
    }

    with open(submission.results_fp, "w") as f:
        json.dump(result, f, indent=2)
    return submission.results_fp


@functools.cache
def load_test_data():
    """Load and cache the test data + labels."""
    df = load_data(split="test")

    with open("test_labels.json") as f:
        labels = json.load(f)

    # Load the dataframe and read in the scores
    labels_df = pd.DataFrame.from_records(labels)

    # Merge dataframes based on the id and validate that ids are unique
    return df.join(labels_df.set_index("id"), on="id", validate="one_to_one")


if __name__ == "__main__":
    ec = hydrate_all()
    exit(ec)
